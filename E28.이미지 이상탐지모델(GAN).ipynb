{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from PIL import Image \n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32, 3), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   # Frog:6\n",
    "            new_t_labels.append([0])  # Frog을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 구성 및 label검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Skip-GANomaly 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Generator (U-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. 전체 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator가 32X32X3 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 29.72638511657715, \t Total Dis Loss : 0.0021412153728306293\n",
      "Steps : 200, \t Total Gen Loss : 20.11224937438965, \t Total Dis Loss : 0.006104249972850084\n",
      "Steps : 300, \t Total Gen Loss : 26.488229751586914, \t Total Dis Loss : 0.0004037196340505034\n",
      "Steps : 400, \t Total Gen Loss : 26.631183624267578, \t Total Dis Loss : 0.0013612407492473722\n",
      "Steps : 500, \t Total Gen Loss : 28.59820556640625, \t Total Dis Loss : 0.0009037409909069538\n",
      "Steps : 600, \t Total Gen Loss : 21.597644805908203, \t Total Dis Loss : 0.0012046239571645856\n",
      "Steps : 700, \t Total Gen Loss : 27.047513961791992, \t Total Dis Loss : 0.0005425274139270186\n",
      "Steps : 800, \t Total Gen Loss : 24.1607666015625, \t Total Dis Loss : 0.0003686897689476609\n",
      "Steps : 900, \t Total Gen Loss : 25.225303649902344, \t Total Dis Loss : 0.0019039470935240388\n",
      "Steps : 1000, \t Total Gen Loss : 25.10439682006836, \t Total Dis Loss : 0.00027016637613996863\n",
      "Steps : 1100, \t Total Gen Loss : 26.712074279785156, \t Total Dis Loss : 0.0005899559473618865\n",
      "Steps : 1200, \t Total Gen Loss : 26.364023208618164, \t Total Dis Loss : 0.0008467428851872683\n",
      "Steps : 1300, \t Total Gen Loss : 21.095691680908203, \t Total Dis Loss : 0.0004829761164728552\n",
      "Steps : 1400, \t Total Gen Loss : 22.84945297241211, \t Total Dis Loss : 0.004238820169121027\n",
      "Steps : 1500, \t Total Gen Loss : 22.817378997802734, \t Total Dis Loss : 0.003180284518748522\n",
      "Steps : 1600, \t Total Gen Loss : 27.54513931274414, \t Total Dis Loss : 0.00018847297178581357\n",
      "Steps : 1700, \t Total Gen Loss : 24.912452697753906, \t Total Dis Loss : 0.0005489122704602778\n",
      "Steps : 1800, \t Total Gen Loss : 25.154499053955078, \t Total Dis Loss : 0.0007225658046081662\n",
      "Steps : 1900, \t Total Gen Loss : 24.821945190429688, \t Total Dis Loss : 0.0010125305270776153\n",
      "Steps : 2000, \t Total Gen Loss : 24.276477813720703, \t Total Dis Loss : 0.00047344370977953076\n",
      "Steps : 2100, \t Total Gen Loss : 25.71893882751465, \t Total Dis Loss : 0.0004934035241603851\n",
      "Steps : 2200, \t Total Gen Loss : 26.536056518554688, \t Total Dis Loss : 0.0006543388590216637\n",
      "Steps : 2300, \t Total Gen Loss : 27.500995635986328, \t Total Dis Loss : 0.0010856535518541932\n",
      "Steps : 2400, \t Total Gen Loss : 26.604459762573242, \t Total Dis Loss : 0.00028956402093172073\n",
      "Steps : 2500, \t Total Gen Loss : 24.51300048828125, \t Total Dis Loss : 0.00015625385276507586\n",
      "Steps : 2600, \t Total Gen Loss : 25.223384857177734, \t Total Dis Loss : 0.0002605449117254466\n",
      "Steps : 2700, \t Total Gen Loss : 27.02874755859375, \t Total Dis Loss : 0.0005854276823811233\n",
      "Steps : 2800, \t Total Gen Loss : 23.1865234375, \t Total Dis Loss : 0.0011423614341765642\n",
      "Steps : 2900, \t Total Gen Loss : 25.692249298095703, \t Total Dis Loss : 0.0013710658531636\n",
      "Steps : 3000, \t Total Gen Loss : 26.621902465820312, \t Total Dis Loss : 0.003058577422052622\n",
      "Steps : 3100, \t Total Gen Loss : 26.45510482788086, \t Total Dis Loss : 0.002457543509081006\n",
      "Steps : 3200, \t Total Gen Loss : 28.56842803955078, \t Total Dis Loss : 0.0005284292274154723\n",
      "Steps : 3300, \t Total Gen Loss : 23.814979553222656, \t Total Dis Loss : 0.017269957810640335\n",
      "Steps : 3400, \t Total Gen Loss : 26.129310607910156, \t Total Dis Loss : 0.009436356835067272\n",
      "Steps : 3500, \t Total Gen Loss : 26.842905044555664, \t Total Dis Loss : 0.014905893243849277\n",
      "Steps : 3600, \t Total Gen Loss : 28.663982391357422, \t Total Dis Loss : 0.0015765827847644687\n",
      "Steps : 3700, \t Total Gen Loss : 27.16236114501953, \t Total Dis Loss : 0.0024862592108547688\n",
      "Steps : 3800, \t Total Gen Loss : 23.68640899658203, \t Total Dis Loss : 0.0050298250280320644\n",
      "Steps : 3900, \t Total Gen Loss : 26.877788543701172, \t Total Dis Loss : 0.00046424195170402527\n",
      "Steps : 4000, \t Total Gen Loss : 27.66205596923828, \t Total Dis Loss : 0.0013422330375760794\n",
      "Steps : 4100, \t Total Gen Loss : 25.508392333984375, \t Total Dis Loss : 0.00026029598666355014\n",
      "Steps : 4200, \t Total Gen Loss : 27.682064056396484, \t Total Dis Loss : 0.045466188341379166\n",
      "Steps : 4300, \t Total Gen Loss : 25.510913848876953, \t Total Dis Loss : 0.0009965610224753618\n",
      "Steps : 4400, \t Total Gen Loss : 27.337942123413086, \t Total Dis Loss : 0.0029894188046455383\n",
      "Steps : 4500, \t Total Gen Loss : 25.990859985351562, \t Total Dis Loss : 0.0005343083175830543\n",
      "Steps : 4600, \t Total Gen Loss : 24.820693969726562, \t Total Dis Loss : 0.00032941755489446223\n",
      "Steps : 4700, \t Total Gen Loss : 28.645488739013672, \t Total Dis Loss : 0.00029739251476712525\n",
      "Steps : 4800, \t Total Gen Loss : 25.933500289916992, \t Total Dis Loss : 0.00010349795047659427\n",
      "Steps : 4900, \t Total Gen Loss : 26.61272430419922, \t Total Dis Loss : 0.0006949749076738954\n",
      "Steps : 5000, \t Total Gen Loss : 25.844167709350586, \t Total Dis Loss : 0.000513708044309169\n",
      "Steps : 5100, \t Total Gen Loss : 25.476978302001953, \t Total Dis Loss : 0.003620823146775365\n",
      "Steps : 5200, \t Total Gen Loss : 32.32455825805664, \t Total Dis Loss : 0.00028130057035014033\n",
      "Steps : 5300, \t Total Gen Loss : 24.419523239135742, \t Total Dis Loss : 0.05102540925145149\n",
      "Steps : 5400, \t Total Gen Loss : 24.92374610900879, \t Total Dis Loss : 0.0008197992574423552\n",
      "Steps : 5500, \t Total Gen Loss : 28.37689208984375, \t Total Dis Loss : 0.00017890208982862532\n",
      "Steps : 5600, \t Total Gen Loss : 25.584300994873047, \t Total Dis Loss : 0.0011762403883039951\n",
      "Time for epoch 1 is 71.45133399963379 sec\n",
      "Steps : 5700, \t Total Gen Loss : 24.237396240234375, \t Total Dis Loss : 0.0011812965385615826\n",
      "Steps : 5800, \t Total Gen Loss : 22.80231475830078, \t Total Dis Loss : 0.0013408726081252098\n",
      "Steps : 5900, \t Total Gen Loss : 24.283153533935547, \t Total Dis Loss : 0.0009465673356316984\n",
      "Steps : 6000, \t Total Gen Loss : 23.808364868164062, \t Total Dis Loss : 0.0006638771737925708\n",
      "Steps : 6100, \t Total Gen Loss : 23.743953704833984, \t Total Dis Loss : 0.0010844176867976785\n",
      "Steps : 6200, \t Total Gen Loss : 23.276309967041016, \t Total Dis Loss : 0.0006354335928335786\n",
      "Steps : 6300, \t Total Gen Loss : 23.34039306640625, \t Total Dis Loss : 0.005258611403405666\n",
      "Steps : 6400, \t Total Gen Loss : 23.63665771484375, \t Total Dis Loss : 0.0021353703923523426\n",
      "Steps : 6500, \t Total Gen Loss : 22.192869186401367, \t Total Dis Loss : 0.000833963742479682\n",
      "Steps : 6600, \t Total Gen Loss : 24.599620819091797, \t Total Dis Loss : 0.0011037292424589396\n",
      "Steps : 6700, \t Total Gen Loss : 26.617183685302734, \t Total Dis Loss : 0.000527525378856808\n",
      "Steps : 6800, \t Total Gen Loss : 24.094030380249023, \t Total Dis Loss : 0.00039932201616466045\n",
      "Steps : 6900, \t Total Gen Loss : 25.690093994140625, \t Total Dis Loss : 0.0011203853646293283\n",
      "Steps : 7000, \t Total Gen Loss : 27.231121063232422, \t Total Dis Loss : 0.0002928287722170353\n",
      "Steps : 7100, \t Total Gen Loss : 25.921802520751953, \t Total Dis Loss : 0.0004351474344730377\n",
      "Steps : 7200, \t Total Gen Loss : 24.400070190429688, \t Total Dis Loss : 0.003617688314989209\n",
      "Steps : 7300, \t Total Gen Loss : 24.040218353271484, \t Total Dis Loss : 0.000769170350395143\n",
      "Steps : 7400, \t Total Gen Loss : 22.079734802246094, \t Total Dis Loss : 0.0005863244878128171\n",
      "Steps : 7500, \t Total Gen Loss : 23.789081573486328, \t Total Dis Loss : 0.00043188277049921453\n",
      "Steps : 7600, \t Total Gen Loss : 25.357982635498047, \t Total Dis Loss : 0.0007043930818326771\n",
      "Steps : 7700, \t Total Gen Loss : 25.988500595092773, \t Total Dis Loss : 0.0004295816470403224\n",
      "Steps : 7800, \t Total Gen Loss : 25.136444091796875, \t Total Dis Loss : 0.0005424008704721928\n",
      "Steps : 7900, \t Total Gen Loss : 23.1820068359375, \t Total Dis Loss : 0.0004968951689079404\n",
      "Steps : 8000, \t Total Gen Loss : 23.53852653503418, \t Total Dis Loss : 0.0001010293053695932\n",
      "Steps : 8100, \t Total Gen Loss : 24.503215789794922, \t Total Dis Loss : 0.00018783823179546744\n",
      "Steps : 8200, \t Total Gen Loss : 26.175039291381836, \t Total Dis Loss : 0.00015809008618816733\n",
      "Steps : 8300, \t Total Gen Loss : 26.449039459228516, \t Total Dis Loss : 0.00013219742686487734\n",
      "Steps : 8400, \t Total Gen Loss : 25.194591522216797, \t Total Dis Loss : 0.0005078212707303464\n",
      "Steps : 8500, \t Total Gen Loss : 22.355262756347656, \t Total Dis Loss : 0.00026392852305434644\n",
      "Steps : 8600, \t Total Gen Loss : 22.93282699584961, \t Total Dis Loss : 0.00019228678138460964\n",
      "Steps : 8700, \t Total Gen Loss : 29.592714309692383, \t Total Dis Loss : 0.00013828248484060168\n",
      "Steps : 8800, \t Total Gen Loss : 27.10407066345215, \t Total Dis Loss : 7.358842412941158e-05\n",
      "Steps : 8900, \t Total Gen Loss : 23.085926055908203, \t Total Dis Loss : 0.011210732161998749\n",
      "Steps : 9000, \t Total Gen Loss : 23.056053161621094, \t Total Dis Loss : 0.0011976142413914204\n",
      "Steps : 9100, \t Total Gen Loss : 24.180404663085938, \t Total Dis Loss : 0.0004648097965400666\n",
      "Steps : 9200, \t Total Gen Loss : 24.56817626953125, \t Total Dis Loss : 0.0002618458238430321\n",
      "Steps : 9300, \t Total Gen Loss : 23.716522216796875, \t Total Dis Loss : 0.0013076954055577517\n",
      "Steps : 9400, \t Total Gen Loss : 23.47202491760254, \t Total Dis Loss : 0.0017935150535777211\n",
      "Steps : 9500, \t Total Gen Loss : 24.321273803710938, \t Total Dis Loss : 0.0003012217639479786\n",
      "Steps : 9600, \t Total Gen Loss : 26.18093490600586, \t Total Dis Loss : 0.003354893997311592\n",
      "Steps : 9700, \t Total Gen Loss : 23.045862197875977, \t Total Dis Loss : 0.0009501017048023641\n",
      "Steps : 9800, \t Total Gen Loss : 23.316007614135742, \t Total Dis Loss : 0.0013004024513065815\n",
      "Steps : 9900, \t Total Gen Loss : 24.96717071533203, \t Total Dis Loss : 0.0003162392822559923\n",
      "Steps : 10000, \t Total Gen Loss : 26.507015228271484, \t Total Dis Loss : 0.0004696485120803118\n",
      "Steps : 10100, \t Total Gen Loss : 26.573009490966797, \t Total Dis Loss : 0.0002089411427732557\n",
      "Steps : 10200, \t Total Gen Loss : 22.18901252746582, \t Total Dis Loss : 0.0005497948732227087\n",
      "Steps : 10300, \t Total Gen Loss : 22.366769790649414, \t Total Dis Loss : 0.0009943324839696288\n",
      "Steps : 10400, \t Total Gen Loss : 24.54051399230957, \t Total Dis Loss : 0.0002453486085869372\n",
      "Steps : 10500, \t Total Gen Loss : 26.225601196289062, \t Total Dis Loss : 0.00011792611621785909\n",
      "Steps : 10600, \t Total Gen Loss : 23.757095336914062, \t Total Dis Loss : 0.0002786660916171968\n",
      "Steps : 10700, \t Total Gen Loss : 26.85549545288086, \t Total Dis Loss : 0.0002755140303634107\n",
      "Steps : 10800, \t Total Gen Loss : 23.991912841796875, \t Total Dis Loss : 0.0008352779550477862\n",
      "Steps : 10900, \t Total Gen Loss : 27.50634002685547, \t Total Dis Loss : 0.00011582774459384382\n",
      "Steps : 11000, \t Total Gen Loss : 26.235658645629883, \t Total Dis Loss : 0.0003292113833595067\n",
      "Steps : 11100, \t Total Gen Loss : 24.73129653930664, \t Total Dis Loss : 0.0003238124481867999\n",
      "Steps : 11200, \t Total Gen Loss : 24.840309143066406, \t Total Dis Loss : 0.0003140228509437293\n",
      "Time for epoch 2 is 72.76555609703064 sec\n",
      "Steps : 11300, \t Total Gen Loss : 22.934696197509766, \t Total Dis Loss : 0.0001463393564336002\n",
      "Steps : 11400, \t Total Gen Loss : 22.631752014160156, \t Total Dis Loss : 0.0018278968054801226\n",
      "Steps : 11500, \t Total Gen Loss : 24.436742782592773, \t Total Dis Loss : 0.00024906540056690574\n",
      "Steps : 11600, \t Total Gen Loss : 20.25583267211914, \t Total Dis Loss : 0.0017225705087184906\n",
      "Steps : 11700, \t Total Gen Loss : 25.443801879882812, \t Total Dis Loss : 0.0016818707808852196\n",
      "Steps : 11800, \t Total Gen Loss : 24.100006103515625, \t Total Dis Loss : 0.0010537162888795137\n",
      "Steps : 11900, \t Total Gen Loss : 27.89765739440918, \t Total Dis Loss : 0.00043257029028609395\n",
      "Steps : 12000, \t Total Gen Loss : 27.589275360107422, \t Total Dis Loss : 4.905249807052314e-05\n",
      "Steps : 12100, \t Total Gen Loss : 24.182044982910156, \t Total Dis Loss : 0.004117225296795368\n",
      "Steps : 12200, \t Total Gen Loss : 24.89503288269043, \t Total Dis Loss : 0.0008740723133087158\n",
      "Steps : 12300, \t Total Gen Loss : 29.41835594177246, \t Total Dis Loss : 0.0001031278952723369\n",
      "Steps : 12400, \t Total Gen Loss : 26.812925338745117, \t Total Dis Loss : 0.00013330986257642508\n",
      "Steps : 12500, \t Total Gen Loss : 21.13763427734375, \t Total Dis Loss : 0.0004760886949952692\n",
      "Steps : 12600, \t Total Gen Loss : 27.01957130432129, \t Total Dis Loss : 2.7757005227613263e-05\n",
      "Steps : 12700, \t Total Gen Loss : 25.53359603881836, \t Total Dis Loss : 0.00010022368223872036\n",
      "Steps : 12800, \t Total Gen Loss : 23.674219131469727, \t Total Dis Loss : 0.0004983675316907465\n",
      "Steps : 12900, \t Total Gen Loss : 27.512664794921875, \t Total Dis Loss : 0.0002401235979050398\n",
      "Steps : 13000, \t Total Gen Loss : 23.3341121673584, \t Total Dis Loss : 0.0003693236503750086\n",
      "Steps : 13100, \t Total Gen Loss : 29.959321975708008, \t Total Dis Loss : 0.012622619979083538\n",
      "Steps : 13200, \t Total Gen Loss : 31.637935638427734, \t Total Dis Loss : 0.0023886975832283497\n",
      "Steps : 13300, \t Total Gen Loss : 23.760852813720703, \t Total Dis Loss : 0.0012713986216112971\n",
      "Steps : 13400, \t Total Gen Loss : 29.452014923095703, \t Total Dis Loss : 0.0007514846511185169\n",
      "Steps : 13500, \t Total Gen Loss : 23.729801177978516, \t Total Dis Loss : 0.003041106043383479\n",
      "Steps : 13600, \t Total Gen Loss : 29.282554626464844, \t Total Dis Loss : 0.006600494496524334\n",
      "Steps : 13700, \t Total Gen Loss : 28.53759002685547, \t Total Dis Loss : 0.0003711830941028893\n",
      "Steps : 13800, \t Total Gen Loss : 30.95861053466797, \t Total Dis Loss : 0.001288745435886085\n",
      "Steps : 13900, \t Total Gen Loss : 30.5413818359375, \t Total Dis Loss : 0.002656958531588316\n",
      "Steps : 14000, \t Total Gen Loss : 28.182373046875, \t Total Dis Loss : 0.0014630411751568317\n",
      "Steps : 14100, \t Total Gen Loss : 26.200010299682617, \t Total Dis Loss : 0.0020637614652514458\n",
      "Steps : 14200, \t Total Gen Loss : 23.3328857421875, \t Total Dis Loss : 0.000639727630186826\n",
      "Steps : 14300, \t Total Gen Loss : 21.631855010986328, \t Total Dis Loss : 0.0011860381346195936\n",
      "Steps : 14400, \t Total Gen Loss : 26.64362335205078, \t Total Dis Loss : 0.000525299459695816\n",
      "Steps : 14500, \t Total Gen Loss : 21.86507225036621, \t Total Dis Loss : 0.0009374840883538127\n",
      "Steps : 14600, \t Total Gen Loss : 22.02545738220215, \t Total Dis Loss : 0.00039674321305938065\n",
      "Steps : 14700, \t Total Gen Loss : 25.863201141357422, \t Total Dis Loss : 0.00042163344915024936\n",
      "Steps : 14800, \t Total Gen Loss : 27.481534957885742, \t Total Dis Loss : 0.0003876847040373832\n",
      "Steps : 14900, \t Total Gen Loss : 23.891828536987305, \t Total Dis Loss : 0.0002318823098903522\n",
      "Steps : 15000, \t Total Gen Loss : 26.360244750976562, \t Total Dis Loss : 0.00024551598471589386\n",
      "Steps : 15100, \t Total Gen Loss : 23.931577682495117, \t Total Dis Loss : 0.0005005706334486604\n",
      "Steps : 15200, \t Total Gen Loss : 26.8072452545166, \t Total Dis Loss : 0.00015684326353948563\n",
      "Steps : 15300, \t Total Gen Loss : 22.393966674804688, \t Total Dis Loss : 0.0002478526730556041\n",
      "Steps : 15400, \t Total Gen Loss : 22.35873794555664, \t Total Dis Loss : 0.00025237814406864345\n",
      "Steps : 15500, \t Total Gen Loss : 25.24636459350586, \t Total Dis Loss : 0.00022257283853832632\n",
      "Steps : 15600, \t Total Gen Loss : 21.663280487060547, \t Total Dis Loss : 0.0029605745803564787\n",
      "Steps : 15700, \t Total Gen Loss : 21.869976043701172, \t Total Dis Loss : 0.00045301811769604683\n",
      "Steps : 15800, \t Total Gen Loss : 25.125164031982422, \t Total Dis Loss : 0.00015781598631292582\n",
      "Steps : 15900, \t Total Gen Loss : 27.60373306274414, \t Total Dis Loss : 3.714439662871882e-05\n",
      "Steps : 16000, \t Total Gen Loss : 23.62300682067871, \t Total Dis Loss : 0.00019720745331142098\n",
      "Steps : 16100, \t Total Gen Loss : 25.692501068115234, \t Total Dis Loss : 0.0030028135515749454\n",
      "Steps : 16200, \t Total Gen Loss : 28.735851287841797, \t Total Dis Loss : 0.00017205497715622187\n",
      "Steps : 16300, \t Total Gen Loss : 24.60219383239746, \t Total Dis Loss : 7.30712417862378e-05\n",
      "Steps : 16400, \t Total Gen Loss : 24.933849334716797, \t Total Dis Loss : 0.00032974572968669236\n",
      "Steps : 16500, \t Total Gen Loss : 26.14109230041504, \t Total Dis Loss : 0.000507343269418925\n",
      "Steps : 16600, \t Total Gen Loss : 26.192276000976562, \t Total Dis Loss : 0.03564204275608063\n",
      "Steps : 16700, \t Total Gen Loss : 25.52509307861328, \t Total Dis Loss : 0.00047705817269161344\n",
      "Steps : 16800, \t Total Gen Loss : 26.707340240478516, \t Total Dis Loss : 0.00010472271969774738\n",
      "Time for epoch 3 is 71.78056645393372 sec\n",
      "Steps : 16900, \t Total Gen Loss : 29.984111785888672, \t Total Dis Loss : 0.012751196511089802\n",
      "Steps : 17000, \t Total Gen Loss : 23.988109588623047, \t Total Dis Loss : 0.000420020311139524\n",
      "Steps : 17100, \t Total Gen Loss : 27.89365005493164, \t Total Dis Loss : 0.0001225680607603863\n",
      "Steps : 17200, \t Total Gen Loss : 27.426551818847656, \t Total Dis Loss : 0.00013009950635023415\n",
      "Steps : 17300, \t Total Gen Loss : 24.723541259765625, \t Total Dis Loss : 0.00027972349198535085\n",
      "Steps : 17400, \t Total Gen Loss : 27.355018615722656, \t Total Dis Loss : 0.0002999585121870041\n",
      "Steps : 17500, \t Total Gen Loss : 25.754024505615234, \t Total Dis Loss : 0.00015403331781271845\n",
      "Steps : 17600, \t Total Gen Loss : 26.673980712890625, \t Total Dis Loss : 0.000132792629301548\n",
      "Steps : 17700, \t Total Gen Loss : 26.4158935546875, \t Total Dis Loss : 0.0002148339117411524\n",
      "Steps : 17800, \t Total Gen Loss : 28.996231079101562, \t Total Dis Loss : 0.0003612746950238943\n",
      "Steps : 17900, \t Total Gen Loss : 25.32735252380371, \t Total Dis Loss : 0.0006147698732092977\n",
      "Steps : 18000, \t Total Gen Loss : 24.535314559936523, \t Total Dis Loss : 0.002316859317943454\n",
      "Steps : 18100, \t Total Gen Loss : 25.616683959960938, \t Total Dis Loss : 0.00012435056851245463\n",
      "Steps : 18200, \t Total Gen Loss : 30.26650047302246, \t Total Dis Loss : 6.722278340021148e-05\n",
      "Steps : 18300, \t Total Gen Loss : 28.007633209228516, \t Total Dis Loss : 8.329531556228176e-05\n",
      "Steps : 18400, \t Total Gen Loss : 26.14260482788086, \t Total Dis Loss : 0.0006089267553761601\n",
      "Steps : 18500, \t Total Gen Loss : 28.87976837158203, \t Total Dis Loss : 0.00022864763741381466\n",
      "Steps : 18600, \t Total Gen Loss : 26.293315887451172, \t Total Dis Loss : 0.00018812029156833887\n",
      "Steps : 18700, \t Total Gen Loss : 25.222782135009766, \t Total Dis Loss : 2.1088171706651337e-05\n",
      "Steps : 18800, \t Total Gen Loss : 26.62521743774414, \t Total Dis Loss : 5.356362817110494e-05\n",
      "Steps : 18900, \t Total Gen Loss : 25.77301025390625, \t Total Dis Loss : 4.2649335227906704e-05\n",
      "Steps : 19000, \t Total Gen Loss : 29.09522247314453, \t Total Dis Loss : 0.00016223973943851888\n",
      "Steps : 19100, \t Total Gen Loss : 25.0296573638916, \t Total Dis Loss : 0.00044953214819543064\n",
      "Steps : 19200, \t Total Gen Loss : 27.051822662353516, \t Total Dis Loss : 4.136314964853227e-05\n",
      "Steps : 19300, \t Total Gen Loss : 25.54579734802246, \t Total Dis Loss : 2.3090980903361924e-05\n",
      "Steps : 19400, \t Total Gen Loss : 23.807273864746094, \t Total Dis Loss : 0.0004306133487261832\n",
      "Steps : 19500, \t Total Gen Loss : 29.736677169799805, \t Total Dis Loss : 6.790442421333864e-05\n",
      "Steps : 19600, \t Total Gen Loss : 26.27179527282715, \t Total Dis Loss : 6.867919000796974e-05\n",
      "Steps : 19700, \t Total Gen Loss : 24.439115524291992, \t Total Dis Loss : 0.0008784161182120442\n",
      "Steps : 19800, \t Total Gen Loss : 26.010986328125, \t Total Dis Loss : 0.0004041224892716855\n",
      "Steps : 19900, \t Total Gen Loss : 22.863203048706055, \t Total Dis Loss : 0.0029170666821300983\n",
      "Steps : 20000, \t Total Gen Loss : 28.210338592529297, \t Total Dis Loss : 0.00014837288472335786\n",
      "Steps : 20100, \t Total Gen Loss : 26.049335479736328, \t Total Dis Loss : 7.721859583398327e-05\n",
      "Steps : 20200, \t Total Gen Loss : 30.090068817138672, \t Total Dis Loss : 6.632463919231668e-05\n",
      "Steps : 20300, \t Total Gen Loss : 27.4731388092041, \t Total Dis Loss : 8.842549868859351e-05\n",
      "Steps : 20400, \t Total Gen Loss : 25.98971939086914, \t Total Dis Loss : 5.529257032321766e-05\n",
      "Steps : 20500, \t Total Gen Loss : 27.49102020263672, \t Total Dis Loss : 3.5025957913603634e-05\n",
      "Steps : 20600, \t Total Gen Loss : 30.474987030029297, \t Total Dis Loss : 4.241673741489649e-05\n",
      "Steps : 20700, \t Total Gen Loss : 28.687345504760742, \t Total Dis Loss : 5.302755016600713e-05\n",
      "Steps : 20800, \t Total Gen Loss : 22.367794036865234, \t Total Dis Loss : 0.00033743662061169744\n",
      "Steps : 20900, \t Total Gen Loss : 26.116748809814453, \t Total Dis Loss : 2.5697916498756967e-05\n",
      "Steps : 21000, \t Total Gen Loss : 29.45267105102539, \t Total Dis Loss : 0.002704532351344824\n",
      "Steps : 21100, \t Total Gen Loss : 27.99637222290039, \t Total Dis Loss : 0.0025639920495450497\n",
      "Steps : 21200, \t Total Gen Loss : 28.07370376586914, \t Total Dis Loss : 4.6401059080380946e-05\n",
      "Steps : 21300, \t Total Gen Loss : 23.500699996948242, \t Total Dis Loss : 0.0074903774075210094\n",
      "Steps : 21400, \t Total Gen Loss : 26.294740676879883, \t Total Dis Loss : 0.0003880815929733217\n",
      "Steps : 21500, \t Total Gen Loss : 27.647239685058594, \t Total Dis Loss : 0.00014753127470612526\n",
      "Steps : 21600, \t Total Gen Loss : 27.61872673034668, \t Total Dis Loss : 0.0003442632732912898\n",
      "Steps : 21700, \t Total Gen Loss : 26.223979949951172, \t Total Dis Loss : 0.00012869993224740028\n",
      "Steps : 21800, \t Total Gen Loss : 24.163658142089844, \t Total Dis Loss : 0.0028723303694278\n",
      "Steps : 21900, \t Total Gen Loss : 23.839336395263672, \t Total Dis Loss : 0.00020905761630274355\n",
      "Steps : 22000, \t Total Gen Loss : 24.064266204833984, \t Total Dis Loss : 0.0003184579545632005\n",
      "Steps : 22100, \t Total Gen Loss : 23.232757568359375, \t Total Dis Loss : 0.0002308050898136571\n",
      "Steps : 22200, \t Total Gen Loss : 27.275310516357422, \t Total Dis Loss : 9.624195809010416e-05\n",
      "Steps : 22300, \t Total Gen Loss : 25.532512664794922, \t Total Dis Loss : 0.000667750311549753\n",
      "Steps : 22400, \t Total Gen Loss : 27.472591400146484, \t Total Dis Loss : 7.21635515219532e-05\n",
      "Steps : 22500, \t Total Gen Loss : 28.552766799926758, \t Total Dis Loss : 0.00014979607658460736\n",
      "Time for epoch 4 is 71.81005263328552 sec\n",
      "Steps : 22600, \t Total Gen Loss : 25.092370986938477, \t Total Dis Loss : 6.848147313576192e-05\n",
      "Steps : 22700, \t Total Gen Loss : 25.29905891418457, \t Total Dis Loss : 0.043662525713443756\n",
      "Steps : 22800, \t Total Gen Loss : 25.915035247802734, \t Total Dis Loss : 5.2001330914208665e-05\n",
      "Steps : 22900, \t Total Gen Loss : 25.544286727905273, \t Total Dis Loss : 0.00011496427759993821\n",
      "Steps : 23000, \t Total Gen Loss : 27.901824951171875, \t Total Dis Loss : 0.00013914683950133622\n",
      "Steps : 23100, \t Total Gen Loss : 23.481061935424805, \t Total Dis Loss : 0.0005626882193610072\n",
      "Steps : 23200, \t Total Gen Loss : 23.84030532836914, \t Total Dis Loss : 0.0004768396611325443\n",
      "Steps : 23300, \t Total Gen Loss : 22.765525817871094, \t Total Dis Loss : 0.00011835551413241774\n",
      "Steps : 23400, \t Total Gen Loss : 25.41611099243164, \t Total Dis Loss : 0.00010534730972722173\n",
      "Steps : 23500, \t Total Gen Loss : 28.021621704101562, \t Total Dis Loss : 0.00011613534297794104\n",
      "Steps : 23600, \t Total Gen Loss : 25.16977882385254, \t Total Dis Loss : 9.577296441420913e-05\n",
      "Steps : 23700, \t Total Gen Loss : 29.677539825439453, \t Total Dis Loss : 0.00020899878290947527\n",
      "Steps : 23800, \t Total Gen Loss : 30.97168731689453, \t Total Dis Loss : 1.820087527448777e-05\n",
      "Steps : 23900, \t Total Gen Loss : 27.160011291503906, \t Total Dis Loss : 5.990355566609651e-05\n",
      "Steps : 24000, \t Total Gen Loss : 26.714035034179688, \t Total Dis Loss : 0.0003092743281740695\n",
      "Steps : 24100, \t Total Gen Loss : 26.27756118774414, \t Total Dis Loss : 0.000332999334204942\n",
      "Steps : 24200, \t Total Gen Loss : 26.986331939697266, \t Total Dis Loss : 0.0001355201966362074\n",
      "Steps : 24300, \t Total Gen Loss : 25.650503158569336, \t Total Dis Loss : 0.0001845146471168846\n",
      "Steps : 24400, \t Total Gen Loss : 25.155860900878906, \t Total Dis Loss : 0.000863593362737447\n",
      "Steps : 24500, \t Total Gen Loss : 29.374370574951172, \t Total Dis Loss : 5.916277950746007e-05\n",
      "Steps : 24600, \t Total Gen Loss : 24.868854522705078, \t Total Dis Loss : 0.0007632644847035408\n",
      "Steps : 24700, \t Total Gen Loss : 27.45522689819336, \t Total Dis Loss : 0.0014398966450244188\n",
      "Steps : 24800, \t Total Gen Loss : 29.150089263916016, \t Total Dis Loss : 8.391973096877337e-05\n",
      "Steps : 24900, \t Total Gen Loss : 24.689105987548828, \t Total Dis Loss : 0.00013021641643717885\n",
      "Steps : 25000, \t Total Gen Loss : 28.510122299194336, \t Total Dis Loss : 0.0002812071470543742\n",
      "Steps : 25100, \t Total Gen Loss : 24.966724395751953, \t Total Dis Loss : 0.00014064155402593315\n",
      "Steps : 25200, \t Total Gen Loss : 26.07594108581543, \t Total Dis Loss : 0.0006518017034977674\n",
      "Steps : 25300, \t Total Gen Loss : 26.24008560180664, \t Total Dis Loss : 0.0015602083876729012\n",
      "Steps : 25400, \t Total Gen Loss : 23.082128524780273, \t Total Dis Loss : 0.000730070925783366\n",
      "Steps : 25500, \t Total Gen Loss : 28.358619689941406, \t Total Dis Loss : 0.00046910138917155564\n",
      "Steps : 25600, \t Total Gen Loss : 26.908998489379883, \t Total Dis Loss : 0.00010223056597169489\n",
      "Steps : 25700, \t Total Gen Loss : 26.31441879272461, \t Total Dis Loss : 0.00015062237798701972\n",
      "Steps : 25800, \t Total Gen Loss : 24.582164764404297, \t Total Dis Loss : 0.00015825298032723367\n",
      "Steps : 25900, \t Total Gen Loss : 25.0014591217041, \t Total Dis Loss : 0.00029540053219534457\n",
      "Steps : 26000, \t Total Gen Loss : 28.365921020507812, \t Total Dis Loss : 0.00010227644088445231\n",
      "Steps : 26100, \t Total Gen Loss : 27.82284164428711, \t Total Dis Loss : 0.0001308307982981205\n",
      "Steps : 26200, \t Total Gen Loss : 30.27410316467285, \t Total Dis Loss : 0.00013028398097958416\n",
      "Steps : 26300, \t Total Gen Loss : 27.30137062072754, \t Total Dis Loss : 2.1211848434177227e-05\n",
      "Steps : 26400, \t Total Gen Loss : 28.51319122314453, \t Total Dis Loss : 6.15903118159622e-05\n",
      "Steps : 26500, \t Total Gen Loss : 24.20117950439453, \t Total Dis Loss : 6.0065591242164373e-05\n",
      "Steps : 26600, \t Total Gen Loss : 27.245712280273438, \t Total Dis Loss : 3.506368739181198e-05\n",
      "Steps : 26700, \t Total Gen Loss : 28.58661460876465, \t Total Dis Loss : 5.1215090934420004e-05\n",
      "Steps : 26800, \t Total Gen Loss : 28.904932022094727, \t Total Dis Loss : 0.00014308918616734445\n",
      "Steps : 26900, \t Total Gen Loss : 26.915241241455078, \t Total Dis Loss : 3.24714019370731e-05\n",
      "Steps : 27000, \t Total Gen Loss : 26.195056915283203, \t Total Dis Loss : 6.130061228759587e-05\n",
      "Steps : 27100, \t Total Gen Loss : 24.008342742919922, \t Total Dis Loss : 0.00017280029715038836\n",
      "Steps : 27200, \t Total Gen Loss : 25.198692321777344, \t Total Dis Loss : 0.0004899718915112317\n",
      "Steps : 27300, \t Total Gen Loss : 26.842557907104492, \t Total Dis Loss : 0.00020695512648671865\n",
      "Steps : 27400, \t Total Gen Loss : 26.99896240234375, \t Total Dis Loss : 2.43090125877643e-05\n",
      "Steps : 27500, \t Total Gen Loss : 28.759801864624023, \t Total Dis Loss : 6.67700805934146e-05\n",
      "Steps : 27600, \t Total Gen Loss : 24.523353576660156, \t Total Dis Loss : 5.0583159463712946e-05\n",
      "Steps : 27700, \t Total Gen Loss : 28.4196720123291, \t Total Dis Loss : 0.00010493066656636074\n",
      "Steps : 27800, \t Total Gen Loss : 29.422924041748047, \t Total Dis Loss : 2.481317460478749e-05\n",
      "Steps : 27900, \t Total Gen Loss : 23.45309829711914, \t Total Dis Loss : 0.00012296128261368722\n",
      "Steps : 28000, \t Total Gen Loss : 23.92630386352539, \t Total Dis Loss : 0.0001568455045344308\n",
      "Steps : 28100, \t Total Gen Loss : 26.51870346069336, \t Total Dis Loss : 6.250701699173078e-05\n",
      "Time for epoch 5 is 71.99688673019409 sec\n",
      "Steps : 28200, \t Total Gen Loss : 28.84549331665039, \t Total Dis Loss : 3.7303019780665636e-05\n",
      "Steps : 28300, \t Total Gen Loss : 26.8245849609375, \t Total Dis Loss : 3.134086728096008e-05\n",
      "Steps : 28400, \t Total Gen Loss : 28.859527587890625, \t Total Dis Loss : 3.0430093829636462e-05\n",
      "Steps : 28500, \t Total Gen Loss : 28.806732177734375, \t Total Dis Loss : 2.063505417027045e-05\n",
      "Steps : 28600, \t Total Gen Loss : 29.00834083557129, \t Total Dis Loss : 0.00016379440785385668\n",
      "Steps : 28700, \t Total Gen Loss : 27.614517211914062, \t Total Dis Loss : 2.0248204236850142e-05\n",
      "Steps : 28800, \t Total Gen Loss : 27.2369384765625, \t Total Dis Loss : 6.184415542520583e-05\n",
      "Steps : 28900, \t Total Gen Loss : 25.427501678466797, \t Total Dis Loss : 4.4075848563807085e-05\n",
      "Steps : 29000, \t Total Gen Loss : 25.523422241210938, \t Total Dis Loss : 6.247744749998674e-05\n",
      "Steps : 29100, \t Total Gen Loss : 24.650829315185547, \t Total Dis Loss : 0.0005220529274083674\n",
      "Steps : 29200, \t Total Gen Loss : 25.94634246826172, \t Total Dis Loss : 0.0005119424895383418\n",
      "Steps : 29300, \t Total Gen Loss : 27.843467712402344, \t Total Dis Loss : 0.000148077480844222\n",
      "Steps : 29400, \t Total Gen Loss : 24.075420379638672, \t Total Dis Loss : 0.00012786976003553718\n",
      "Steps : 29500, \t Total Gen Loss : 25.886192321777344, \t Total Dis Loss : 0.00010703576117521152\n",
      "Steps : 29600, \t Total Gen Loss : 23.529865264892578, \t Total Dis Loss : 0.0009381714044138789\n",
      "Steps : 29700, \t Total Gen Loss : 28.429428100585938, \t Total Dis Loss : 4.66219280497171e-05\n",
      "Steps : 29800, \t Total Gen Loss : 26.219894409179688, \t Total Dis Loss : 9.847412002272904e-05\n",
      "Steps : 29900, \t Total Gen Loss : 20.82444953918457, \t Total Dis Loss : 0.0004370913084130734\n",
      "Steps : 30000, \t Total Gen Loss : 23.717164993286133, \t Total Dis Loss : 0.0002229373640147969\n",
      "Steps : 30100, \t Total Gen Loss : 22.747217178344727, \t Total Dis Loss : 0.0005380720249377191\n",
      "Steps : 30200, \t Total Gen Loss : 25.183013916015625, \t Total Dis Loss : 0.0005421160603873432\n",
      "Steps : 30300, \t Total Gen Loss : 26.022571563720703, \t Total Dis Loss : 8.695870928931981e-05\n",
      "Steps : 30400, \t Total Gen Loss : 25.426767349243164, \t Total Dis Loss : 9.111618419410661e-05\n",
      "Steps : 30500, \t Total Gen Loss : 23.79330062866211, \t Total Dis Loss : 0.0035495152696967125\n",
      "Steps : 30600, \t Total Gen Loss : 24.382740020751953, \t Total Dis Loss : 0.0005218056030571461\n",
      "Steps : 30700, \t Total Gen Loss : 25.9871826171875, \t Total Dis Loss : 0.0005453677731566131\n",
      "Steps : 30800, \t Total Gen Loss : 24.386327743530273, \t Total Dis Loss : 0.0001229866611538455\n",
      "Steps : 30900, \t Total Gen Loss : 26.12606430053711, \t Total Dis Loss : 5.9237554523861036e-05\n",
      "Steps : 31000, \t Total Gen Loss : 30.758956909179688, \t Total Dis Loss : 5.483481800183654e-05\n",
      "Steps : 31100, \t Total Gen Loss : 28.603004455566406, \t Total Dis Loss : 5.1482620619935915e-05\n",
      "Steps : 31200, \t Total Gen Loss : 24.870372772216797, \t Total Dis Loss : 9.110497194342315e-05\n",
      "Steps : 31300, \t Total Gen Loss : 27.61280059814453, \t Total Dis Loss : 0.00039671201375313103\n",
      "Steps : 31400, \t Total Gen Loss : 28.43130874633789, \t Total Dis Loss : 0.00035647768527269363\n",
      "Steps : 31500, \t Total Gen Loss : 28.49935531616211, \t Total Dis Loss : 0.00022192821779754013\n",
      "Steps : 31600, \t Total Gen Loss : 33.87174987792969, \t Total Dis Loss : 0.0003395660314708948\n",
      "Steps : 31700, \t Total Gen Loss : 28.503307342529297, \t Total Dis Loss : 8.1087120634038e-05\n",
      "Steps : 31800, \t Total Gen Loss : 25.18053436279297, \t Total Dis Loss : 0.0018278990173712373\n",
      "Steps : 31900, \t Total Gen Loss : 24.468931198120117, \t Total Dis Loss : 0.0001625175355002284\n",
      "Steps : 32000, \t Total Gen Loss : 25.095252990722656, \t Total Dis Loss : 0.00040477945003658533\n",
      "Steps : 32100, \t Total Gen Loss : 24.154560089111328, \t Total Dis Loss : 0.00031862332252785563\n",
      "Steps : 32200, \t Total Gen Loss : 29.789798736572266, \t Total Dis Loss : 4.787807120010257e-05\n",
      "Steps : 32300, \t Total Gen Loss : 24.987384796142578, \t Total Dis Loss : 0.0011261421022936702\n",
      "Steps : 32400, \t Total Gen Loss : 27.199661254882812, \t Total Dis Loss : 8.736428571864963e-05\n",
      "Steps : 32500, \t Total Gen Loss : 30.15007209777832, \t Total Dis Loss : 0.0008004402043297887\n",
      "Steps : 32600, \t Total Gen Loss : 31.332582473754883, \t Total Dis Loss : 0.00013033236609771848\n",
      "Steps : 32700, \t Total Gen Loss : 30.836483001708984, \t Total Dis Loss : 0.00010384758934378624\n",
      "Steps : 32800, \t Total Gen Loss : 25.293365478515625, \t Total Dis Loss : 0.00028093892615288496\n",
      "Steps : 32900, \t Total Gen Loss : 27.236305236816406, \t Total Dis Loss : 0.00020043710537720472\n",
      "Steps : 33000, \t Total Gen Loss : 21.086204528808594, \t Total Dis Loss : 0.0026563762221485376\n",
      "Steps : 33100, \t Total Gen Loss : 23.52813720703125, \t Total Dis Loss : 0.0001840291079133749\n",
      "Steps : 33200, \t Total Gen Loss : 25.882476806640625, \t Total Dis Loss : 6.534492422360927e-05\n",
      "Steps : 33300, \t Total Gen Loss : 24.197723388671875, \t Total Dis Loss : 0.00017903000116348267\n",
      "Steps : 33400, \t Total Gen Loss : 25.746164321899414, \t Total Dis Loss : 4.390910908114165e-05\n",
      "Steps : 33500, \t Total Gen Loss : 25.060375213623047, \t Total Dis Loss : 0.00013204367132857442\n",
      "Steps : 33600, \t Total Gen Loss : 24.60907554626465, \t Total Dis Loss : 0.0007880529738031328\n",
      "Steps : 33700, \t Total Gen Loss : 25.390825271606445, \t Total Dis Loss : 0.00018406541494186968\n",
      "Time for epoch 6 is 71.83751654624939 sec\n",
      "Steps : 33800, \t Total Gen Loss : 29.865108489990234, \t Total Dis Loss : 6.040493826731108e-05\n",
      "Steps : 33900, \t Total Gen Loss : 25.53925132751465, \t Total Dis Loss : 0.0003532581322360784\n",
      "Steps : 34000, \t Total Gen Loss : 24.30221176147461, \t Total Dis Loss : 0.00024950472288765013\n",
      "Steps : 34100, \t Total Gen Loss : 26.420276641845703, \t Total Dis Loss : 6.223595119081438e-05\n",
      "Steps : 34200, \t Total Gen Loss : 30.091506958007812, \t Total Dis Loss : 7.428597746184096e-05\n",
      "Steps : 34300, \t Total Gen Loss : 23.918075561523438, \t Total Dis Loss : 0.00010987091081915423\n",
      "Steps : 34400, \t Total Gen Loss : 26.699880599975586, \t Total Dis Loss : 0.00011061641271226108\n",
      "Steps : 34500, \t Total Gen Loss : 23.97967529296875, \t Total Dis Loss : 5.344770033843815e-05\n",
      "Steps : 34600, \t Total Gen Loss : 28.385597229003906, \t Total Dis Loss : 4.784654447576031e-05\n",
      "Steps : 34700, \t Total Gen Loss : 22.788461685180664, \t Total Dis Loss : 0.00010464143997523934\n",
      "Steps : 34800, \t Total Gen Loss : 26.312503814697266, \t Total Dis Loss : 9.445338218938559e-05\n",
      "Steps : 34900, \t Total Gen Loss : 22.292476654052734, \t Total Dis Loss : 0.00013119143841322511\n",
      "Steps : 35000, \t Total Gen Loss : 26.952388763427734, \t Total Dis Loss : 0.10129913687705994\n",
      "Steps : 35100, \t Total Gen Loss : 27.452613830566406, \t Total Dis Loss : 0.00018500925216358155\n",
      "Steps : 35200, \t Total Gen Loss : 28.642086029052734, \t Total Dis Loss : 0.0055308109149336815\n",
      "Steps : 35300, \t Total Gen Loss : 28.970314025878906, \t Total Dis Loss : 0.0001527785207144916\n",
      "Steps : 35400, \t Total Gen Loss : 26.24829864501953, \t Total Dis Loss : 0.0010841322364285588\n",
      "Steps : 35500, \t Total Gen Loss : 26.625450134277344, \t Total Dis Loss : 0.0003084968775510788\n",
      "Steps : 35600, \t Total Gen Loss : 23.824661254882812, \t Total Dis Loss : 0.003536050207912922\n",
      "Steps : 35700, \t Total Gen Loss : 26.214208602905273, \t Total Dis Loss : 0.00023264939954970032\n",
      "Steps : 35800, \t Total Gen Loss : 25.942626953125, \t Total Dis Loss : 0.00010928247502306476\n",
      "Steps : 35900, \t Total Gen Loss : 25.424036026000977, \t Total Dis Loss : 0.00012596107262652367\n",
      "Steps : 36000, \t Total Gen Loss : 26.971328735351562, \t Total Dis Loss : 6.228683923836797e-05\n",
      "Steps : 36100, \t Total Gen Loss : 22.532447814941406, \t Total Dis Loss : 0.000980354961939156\n",
      "Steps : 36200, \t Total Gen Loss : 30.605403900146484, \t Total Dis Loss : 4.019443440483883e-05\n",
      "Steps : 36300, \t Total Gen Loss : 32.589698791503906, \t Total Dis Loss : 5.903461715206504e-05\n",
      "Steps : 36400, \t Total Gen Loss : 23.388641357421875, \t Total Dis Loss : 0.000331901537720114\n",
      "Steps : 36500, \t Total Gen Loss : 24.027114868164062, \t Total Dis Loss : 0.14083144068717957\n",
      "Steps : 36600, \t Total Gen Loss : 34.557373046875, \t Total Dis Loss : 5.989585042698309e-05\n",
      "Steps : 36700, \t Total Gen Loss : 31.54996681213379, \t Total Dis Loss : 0.0014547889586538076\n",
      "Steps : 36800, \t Total Gen Loss : 26.484678268432617, \t Total Dis Loss : 0.00869573000818491\n",
      "Steps : 36900, \t Total Gen Loss : 32.32225799560547, \t Total Dis Loss : 0.0003610220446716994\n",
      "Steps : 37000, \t Total Gen Loss : 26.13524627685547, \t Total Dis Loss : 0.0042597949504852295\n",
      "Steps : 37100, \t Total Gen Loss : 28.091577529907227, \t Total Dis Loss : 0.004145280458033085\n",
      "Steps : 37200, \t Total Gen Loss : 26.859621047973633, \t Total Dis Loss : 0.006196718662977219\n",
      "Steps : 37300, \t Total Gen Loss : 27.319726943969727, \t Total Dis Loss : 0.0015486677875742316\n",
      "Steps : 37400, \t Total Gen Loss : 26.736473083496094, \t Total Dis Loss : 0.0010395529679954052\n",
      "Steps : 37500, \t Total Gen Loss : 23.86277961730957, \t Total Dis Loss : 0.003142521483823657\n",
      "Steps : 37600, \t Total Gen Loss : 27.404647827148438, \t Total Dis Loss : 0.00019103707745671272\n",
      "Steps : 37700, \t Total Gen Loss : 23.186206817626953, \t Total Dis Loss : 0.0008451471221633255\n",
      "Steps : 37800, \t Total Gen Loss : 28.47581672668457, \t Total Dis Loss : 0.0016435852739959955\n",
      "Steps : 37900, \t Total Gen Loss : 28.57170867919922, \t Total Dis Loss : 0.0005349201383069158\n",
      "Steps : 38000, \t Total Gen Loss : 27.005271911621094, \t Total Dis Loss : 0.0002197917056037113\n",
      "Steps : 38100, \t Total Gen Loss : 22.927227020263672, \t Total Dis Loss : 0.003494289703667164\n",
      "Steps : 38200, \t Total Gen Loss : 20.070770263671875, \t Total Dis Loss : 0.001419982872903347\n",
      "Steps : 38300, \t Total Gen Loss : 23.152301788330078, \t Total Dis Loss : 0.0004632014315575361\n",
      "Steps : 38400, \t Total Gen Loss : 22.475366592407227, \t Total Dis Loss : 0.003389084944501519\n",
      "Steps : 38500, \t Total Gen Loss : 24.698055267333984, \t Total Dis Loss : 0.00034746050368994474\n",
      "Steps : 38600, \t Total Gen Loss : 25.07732391357422, \t Total Dis Loss : 0.0001731512020342052\n",
      "Steps : 38700, \t Total Gen Loss : 23.397024154663086, \t Total Dis Loss : 0.0009906748309731483\n",
      "Steps : 38800, \t Total Gen Loss : 24.628864288330078, \t Total Dis Loss : 0.0004951534210704267\n",
      "Steps : 38900, \t Total Gen Loss : 25.957992553710938, \t Total Dis Loss : 0.0001471716386731714\n",
      "Steps : 39000, \t Total Gen Loss : 24.656341552734375, \t Total Dis Loss : 0.00018452003132551908\n",
      "Steps : 39100, \t Total Gen Loss : 25.520729064941406, \t Total Dis Loss : 0.00031955292797647417\n",
      "Steps : 39200, \t Total Gen Loss : 27.253875732421875, \t Total Dis Loss : 0.00015211851859930903\n",
      "Steps : 39300, \t Total Gen Loss : 24.295427322387695, \t Total Dis Loss : 0.0004295078106224537\n",
      "Time for epoch 7 is 72.62516450881958 sec\n",
      "Steps : 39400, \t Total Gen Loss : 25.85226821899414, \t Total Dis Loss : 0.00014666345668956637\n",
      "Steps : 39500, \t Total Gen Loss : 29.829206466674805, \t Total Dis Loss : 7.15060014044866e-05\n",
      "Steps : 39600, \t Total Gen Loss : 23.535072326660156, \t Total Dis Loss : 0.00020310451509431005\n",
      "Steps : 39700, \t Total Gen Loss : 26.491823196411133, \t Total Dis Loss : 0.009779274463653564\n",
      "Steps : 39800, \t Total Gen Loss : 32.943870544433594, \t Total Dis Loss : 0.0009784222347661853\n",
      "Steps : 39900, \t Total Gen Loss : 34.25547409057617, \t Total Dis Loss : 0.006378388497978449\n",
      "Steps : 40000, \t Total Gen Loss : 28.13075065612793, \t Total Dis Loss : 0.0007541287341155112\n",
      "Steps : 40100, \t Total Gen Loss : 27.573776245117188, \t Total Dis Loss : 0.0008631438831798732\n",
      "Steps : 40200, \t Total Gen Loss : 25.558399200439453, \t Total Dis Loss : 0.0011077207745984197\n",
      "Steps : 40300, \t Total Gen Loss : 28.31158447265625, \t Total Dis Loss : 0.0001999851083382964\n",
      "Steps : 40400, \t Total Gen Loss : 26.23909568786621, \t Total Dis Loss : 0.00015387828170787543\n",
      "Steps : 40500, \t Total Gen Loss : 25.60761833190918, \t Total Dis Loss : 0.0007098499336279929\n",
      "Steps : 40600, \t Total Gen Loss : 26.039182662963867, \t Total Dis Loss : 0.0003060587332583964\n",
      "Steps : 40700, \t Total Gen Loss : 27.8112735748291, \t Total Dis Loss : 0.0002620812738314271\n",
      "Steps : 40800, \t Total Gen Loss : 30.2496280670166, \t Total Dis Loss : 0.0004298635758459568\n",
      "Steps : 40900, \t Total Gen Loss : 23.21393585205078, \t Total Dis Loss : 0.00038367201341316104\n",
      "Steps : 41000, \t Total Gen Loss : 27.844310760498047, \t Total Dis Loss : 0.0003241162048652768\n",
      "Steps : 41100, \t Total Gen Loss : 29.737871170043945, \t Total Dis Loss : 0.0004439358308445662\n",
      "Steps : 41200, \t Total Gen Loss : 27.175983428955078, \t Total Dis Loss : 0.00029973458731547\n",
      "Steps : 41300, \t Total Gen Loss : 25.492374420166016, \t Total Dis Loss : 0.0002697190793696791\n",
      "Steps : 41400, \t Total Gen Loss : 24.21204376220703, \t Total Dis Loss : 0.00019337267440278083\n",
      "Steps : 41500, \t Total Gen Loss : 26.451805114746094, \t Total Dis Loss : 0.00011084461584687233\n",
      "Steps : 41600, \t Total Gen Loss : 28.824665069580078, \t Total Dis Loss : 0.00019388284999877214\n",
      "Steps : 41700, \t Total Gen Loss : 27.626956939697266, \t Total Dis Loss : 0.00047251308569684625\n",
      "Steps : 41800, \t Total Gen Loss : 25.491714477539062, \t Total Dis Loss : 0.0002348876150790602\n",
      "Steps : 41900, \t Total Gen Loss : 30.37781524658203, \t Total Dis Loss : 0.00025389372603967786\n",
      "Steps : 42000, \t Total Gen Loss : 23.765396118164062, \t Total Dis Loss : 0.0013464299263432622\n",
      "Steps : 42100, \t Total Gen Loss : 28.236984252929688, \t Total Dis Loss : 0.000499867252074182\n",
      "Steps : 42200, \t Total Gen Loss : 24.479318618774414, \t Total Dis Loss : 0.00026125129079446197\n",
      "Steps : 42300, \t Total Gen Loss : 23.589523315429688, \t Total Dis Loss : 0.00028055981965735555\n",
      "Steps : 42400, \t Total Gen Loss : 26.274330139160156, \t Total Dis Loss : 0.00020736476290039718\n",
      "Steps : 42500, \t Total Gen Loss : 26.685564041137695, \t Total Dis Loss : 0.00025062623899430037\n",
      "Steps : 42600, \t Total Gen Loss : 25.639362335205078, \t Total Dis Loss : 0.0001372139377053827\n",
      "Steps : 42700, \t Total Gen Loss : 25.339954376220703, \t Total Dis Loss : 8.719133620616049e-05\n",
      "Steps : 42800, \t Total Gen Loss : 26.018817901611328, \t Total Dis Loss : 0.00013309701171237975\n",
      "Steps : 42900, \t Total Gen Loss : 25.653404235839844, \t Total Dis Loss : 0.000189854676136747\n",
      "Steps : 43000, \t Total Gen Loss : 28.5673770904541, \t Total Dis Loss : 0.000103787038824521\n",
      "Steps : 43100, \t Total Gen Loss : 26.191343307495117, \t Total Dis Loss : 0.00011326726962579414\n",
      "Steps : 43200, \t Total Gen Loss : 24.418506622314453, \t Total Dis Loss : 6.636253237957135e-05\n",
      "Steps : 43300, \t Total Gen Loss : 27.607940673828125, \t Total Dis Loss : 8.458045340375975e-05\n",
      "Steps : 43400, \t Total Gen Loss : 24.086183547973633, \t Total Dis Loss : 0.00023008389689493924\n",
      "Steps : 43500, \t Total Gen Loss : 25.082244873046875, \t Total Dis Loss : 0.0001201536797452718\n",
      "Steps : 43600, \t Total Gen Loss : 28.268146514892578, \t Total Dis Loss : 0.0002678913588169962\n",
      "Steps : 43700, \t Total Gen Loss : 25.879131317138672, \t Total Dis Loss : 0.0005986467585898936\n",
      "Steps : 43800, \t Total Gen Loss : 24.494352340698242, \t Total Dis Loss : 6.254082109080628e-05\n",
      "Steps : 43900, \t Total Gen Loss : 24.365814208984375, \t Total Dis Loss : 8.542596333427355e-05\n",
      "Steps : 44000, \t Total Gen Loss : 28.249263763427734, \t Total Dis Loss : 2.5961326173273847e-05\n",
      "Steps : 44100, \t Total Gen Loss : 27.679981231689453, \t Total Dis Loss : 5.199617226026021e-05\n",
      "Steps : 44200, \t Total Gen Loss : 28.489959716796875, \t Total Dis Loss : 4.874560909229331e-05\n",
      "Steps : 44300, \t Total Gen Loss : 25.96848487854004, \t Total Dis Loss : 9.625874372432008e-05\n",
      "Steps : 44400, \t Total Gen Loss : 26.87737274169922, \t Total Dis Loss : 3.955392458010465e-05\n",
      "Steps : 44500, \t Total Gen Loss : 27.134132385253906, \t Total Dis Loss : 1.8953522157971747e-05\n",
      "Steps : 44600, \t Total Gen Loss : 26.55858612060547, \t Total Dis Loss : 2.7710433641914278e-05\n",
      "Steps : 44700, \t Total Gen Loss : 28.985057830810547, \t Total Dis Loss : 4.429212276590988e-05\n",
      "Steps : 44800, \t Total Gen Loss : 28.5894775390625, \t Total Dis Loss : 6.776390364393592e-05\n",
      "Steps : 44900, \t Total Gen Loss : 26.94972038269043, \t Total Dis Loss : 5.608163701253943e-05\n",
      "Steps : 45000, \t Total Gen Loss : 23.653318405151367, \t Total Dis Loss : 0.0025613976176828146\n",
      "Time for epoch 8 is 72.02320909500122 sec\n",
      "Steps : 45100, \t Total Gen Loss : 23.50093650817871, \t Total Dis Loss : 0.0007713190279901028\n",
      "Steps : 45200, \t Total Gen Loss : 26.520647048950195, \t Total Dis Loss : 0.00036188538069836795\n",
      "Steps : 45300, \t Total Gen Loss : 26.015335083007812, \t Total Dis Loss : 0.00015854224329814315\n",
      "Steps : 45400, \t Total Gen Loss : 26.945823669433594, \t Total Dis Loss : 0.00012149296526331455\n",
      "Steps : 45500, \t Total Gen Loss : 25.65556526184082, \t Total Dis Loss : 4.885324233327992e-05\n",
      "Steps : 45600, \t Total Gen Loss : 25.73772430419922, \t Total Dis Loss : 3.346138691995293e-05\n",
      "Steps : 45700, \t Total Gen Loss : 25.116981506347656, \t Total Dis Loss : 0.00016870145918801427\n",
      "Steps : 45800, \t Total Gen Loss : 22.77478790283203, \t Total Dis Loss : 0.00012391569907777011\n",
      "Steps : 45900, \t Total Gen Loss : 25.87184715270996, \t Total Dis Loss : 0.00013002328341826797\n",
      "Steps : 46000, \t Total Gen Loss : 22.409664154052734, \t Total Dis Loss : 0.00034046295331791043\n",
      "Steps : 46100, \t Total Gen Loss : 28.445514678955078, \t Total Dis Loss : 0.00019205879652872682\n",
      "Steps : 46200, \t Total Gen Loss : 25.316469192504883, \t Total Dis Loss : 0.00012487878848332912\n",
      "Steps : 46300, \t Total Gen Loss : 24.190357208251953, \t Total Dis Loss : 0.00036681286292150617\n",
      "Steps : 46400, \t Total Gen Loss : 24.86101722717285, \t Total Dis Loss : 0.00024118476721923798\n",
      "Steps : 46500, \t Total Gen Loss : 24.543956756591797, \t Total Dis Loss : 0.00024043559096753597\n",
      "Steps : 46600, \t Total Gen Loss : 26.224761962890625, \t Total Dis Loss : 0.00012346326548140496\n",
      "Steps : 46700, \t Total Gen Loss : 23.98748779296875, \t Total Dis Loss : 0.00020955412765033543\n",
      "Steps : 46800, \t Total Gen Loss : 21.666072845458984, \t Total Dis Loss : 0.014403199777007103\n",
      "Steps : 46900, \t Total Gen Loss : 22.03217124938965, \t Total Dis Loss : 0.00019079972116742283\n",
      "Steps : 47000, \t Total Gen Loss : 23.419479370117188, \t Total Dis Loss : 0.0004514525062404573\n",
      "Steps : 47100, \t Total Gen Loss : 26.867746353149414, \t Total Dis Loss : 9.454988321522251e-05\n",
      "Steps : 47200, \t Total Gen Loss : 26.316062927246094, \t Total Dis Loss : 0.00022604504192713648\n",
      "Steps : 47300, \t Total Gen Loss : 26.245738983154297, \t Total Dis Loss : 0.0001139084342867136\n",
      "Steps : 47400, \t Total Gen Loss : 24.744043350219727, \t Total Dis Loss : 0.00011818793427664787\n",
      "Steps : 47500, \t Total Gen Loss : 23.253955841064453, \t Total Dis Loss : 4.68357939098496e-05\n",
      "Steps : 47600, \t Total Gen Loss : 24.01071548461914, \t Total Dis Loss : 0.00016952813894022256\n",
      "Steps : 47700, \t Total Gen Loss : 25.40498161315918, \t Total Dis Loss : 0.0003213983727619052\n",
      "Steps : 47800, \t Total Gen Loss : 25.899768829345703, \t Total Dis Loss : 0.0001297617855016142\n",
      "Steps : 47900, \t Total Gen Loss : 25.134326934814453, \t Total Dis Loss : 0.000382687576347962\n",
      "Steps : 48000, \t Total Gen Loss : 23.85125160217285, \t Total Dis Loss : 0.0001723912137094885\n",
      "Steps : 48100, \t Total Gen Loss : 26.76085662841797, \t Total Dis Loss : 0.00012995403085369617\n",
      "Steps : 48200, \t Total Gen Loss : 24.24026870727539, \t Total Dis Loss : 6.651089643128216e-05\n",
      "Steps : 48300, \t Total Gen Loss : 25.777883529663086, \t Total Dis Loss : 6.37494886177592e-05\n",
      "Steps : 48400, \t Total Gen Loss : 26.068981170654297, \t Total Dis Loss : 6.884020694997162e-05\n",
      "Steps : 48500, \t Total Gen Loss : 25.19566535949707, \t Total Dis Loss : 4.556277417577803e-05\n",
      "Steps : 48600, \t Total Gen Loss : 27.990798950195312, \t Total Dis Loss : 2.36524883803213e-05\n",
      "Steps : 48700, \t Total Gen Loss : 27.782939910888672, \t Total Dis Loss : 0.002587829949334264\n",
      "Steps : 48800, \t Total Gen Loss : 24.843589782714844, \t Total Dis Loss : 0.00024239685444626957\n",
      "Steps : 48900, \t Total Gen Loss : 24.517818450927734, \t Total Dis Loss : 0.00035795941948890686\n",
      "Steps : 49000, \t Total Gen Loss : 26.469425201416016, \t Total Dis Loss : 0.00016252044588327408\n",
      "Steps : 49100, \t Total Gen Loss : 25.282756805419922, \t Total Dis Loss : 0.00013196252984926105\n",
      "Steps : 49200, \t Total Gen Loss : 28.544708251953125, \t Total Dis Loss : 0.00015715464542154223\n",
      "Steps : 49300, \t Total Gen Loss : 27.597396850585938, \t Total Dis Loss : 3.8365848013199866e-05\n",
      "Steps : 49400, \t Total Gen Loss : 28.361230850219727, \t Total Dis Loss : 9.6839712568908e-06\n",
      "Steps : 49500, \t Total Gen Loss : 24.83855438232422, \t Total Dis Loss : 8.298900866066106e-06\n",
      "Steps : 49600, \t Total Gen Loss : 27.585433959960938, \t Total Dis Loss : 4.283936868887395e-05\n",
      "Steps : 49700, \t Total Gen Loss : 26.641239166259766, \t Total Dis Loss : 4.541525413515046e-05\n",
      "Steps : 49800, \t Total Gen Loss : 25.980770111083984, \t Total Dis Loss : 0.00012299655645620078\n",
      "Steps : 49900, \t Total Gen Loss : 25.936662673950195, \t Total Dis Loss : 0.00010600221139611676\n",
      "Steps : 50000, \t Total Gen Loss : 31.30982780456543, \t Total Dis Loss : 7.511481089750305e-05\n",
      "Steps : 50100, \t Total Gen Loss : 28.932907104492188, \t Total Dis Loss : 1.439920742996037e-05\n",
      "Steps : 50200, \t Total Gen Loss : 29.10953712463379, \t Total Dis Loss : 8.41074506752193e-05\n",
      "Steps : 50300, \t Total Gen Loss : 27.339027404785156, \t Total Dis Loss : 5.486802547238767e-05\n",
      "Steps : 50400, \t Total Gen Loss : 32.53696060180664, \t Total Dis Loss : 0.00032521042157895863\n",
      "Steps : 50500, \t Total Gen Loss : 34.770851135253906, \t Total Dis Loss : 0.0013575536431744695\n",
      "Steps : 50600, \t Total Gen Loss : 30.151180267333984, \t Total Dis Loss : 0.000950668181758374\n",
      "Time for epoch 9 is 72.11645340919495 sec\n",
      "Steps : 50700, \t Total Gen Loss : 34.54637145996094, \t Total Dis Loss : 0.000436511734733358\n",
      "Steps : 50800, \t Total Gen Loss : 30.80457305908203, \t Total Dis Loss : 0.004503739532083273\n",
      "Steps : 50900, \t Total Gen Loss : 31.669639587402344, \t Total Dis Loss : 0.0018095375271514058\n",
      "Steps : 51000, \t Total Gen Loss : 28.65829849243164, \t Total Dis Loss : 0.11909360438585281\n",
      "Steps : 51100, \t Total Gen Loss : 35.89966583251953, \t Total Dis Loss : 9.361366392113268e-05\n",
      "Steps : 51200, \t Total Gen Loss : 29.939701080322266, \t Total Dis Loss : 9.3287875643e-05\n",
      "Steps : 51300, \t Total Gen Loss : 31.0851993560791, \t Total Dis Loss : 0.00010055507300421596\n",
      "Steps : 51400, \t Total Gen Loss : 33.15653991699219, \t Total Dis Loss : 0.00032212998485192657\n",
      "Steps : 51500, \t Total Gen Loss : 33.05574035644531, \t Total Dis Loss : 0.0003359298571012914\n",
      "Steps : 51600, \t Total Gen Loss : 37.62249755859375, \t Total Dis Loss : 0.0008851010352373123\n",
      "Steps : 51700, \t Total Gen Loss : 33.75102996826172, \t Total Dis Loss : 0.002135208109393716\n",
      "Steps : 51800, \t Total Gen Loss : 30.543292999267578, \t Total Dis Loss : 0.00026164803421124816\n",
      "Steps : 51900, \t Total Gen Loss : 33.148887634277344, \t Total Dis Loss : 0.000756602908950299\n",
      "Steps : 52000, \t Total Gen Loss : 31.997314453125, \t Total Dis Loss : 0.04054345190525055\n",
      "Steps : 52100, \t Total Gen Loss : 33.47365188598633, \t Total Dis Loss : 0.0009766859002411366\n",
      "Steps : 52200, \t Total Gen Loss : 31.720935821533203, \t Total Dis Loss : 0.0002796532353386283\n",
      "Steps : 52300, \t Total Gen Loss : 31.020414352416992, \t Total Dis Loss : 0.00043770179036073387\n",
      "Steps : 52400, \t Total Gen Loss : 33.45178985595703, \t Total Dis Loss : 0.00017508624296169728\n",
      "Steps : 52500, \t Total Gen Loss : 31.017234802246094, \t Total Dis Loss : 0.0003562266065273434\n",
      "Steps : 52600, \t Total Gen Loss : 30.154882431030273, \t Total Dis Loss : 0.00023347316891886294\n",
      "Steps : 52700, \t Total Gen Loss : 27.85826873779297, \t Total Dis Loss : 0.0007137347711250186\n",
      "Steps : 52800, \t Total Gen Loss : 34.23231506347656, \t Total Dis Loss : 0.0030568945221602917\n",
      "Steps : 52900, \t Total Gen Loss : 38.639808654785156, \t Total Dis Loss : 0.0002740340423770249\n",
      "Steps : 53000, \t Total Gen Loss : 30.52450180053711, \t Total Dis Loss : 0.00037907552905380726\n",
      "Steps : 53100, \t Total Gen Loss : 31.394882202148438, \t Total Dis Loss : 0.009886311367154121\n",
      "Steps : 53200, \t Total Gen Loss : 30.73077964782715, \t Total Dis Loss : 0.0003013415262103081\n",
      "Steps : 53300, \t Total Gen Loss : 27.505619049072266, \t Total Dis Loss : 0.0007420210167765617\n",
      "Steps : 53400, \t Total Gen Loss : 27.930931091308594, \t Total Dis Loss : 0.00016544733080081642\n",
      "Steps : 53500, \t Total Gen Loss : 25.952587127685547, \t Total Dis Loss : 0.00034668302396312356\n",
      "Steps : 53600, \t Total Gen Loss : 27.383888244628906, \t Total Dis Loss : 0.00039131235098466277\n",
      "Steps : 53700, \t Total Gen Loss : 27.779216766357422, \t Total Dis Loss : 0.0002794619940686971\n",
      "Steps : 53800, \t Total Gen Loss : 29.71295928955078, \t Total Dis Loss : 0.005642575211822987\n",
      "Steps : 53900, \t Total Gen Loss : 28.570575714111328, \t Total Dis Loss : 0.00065225875005126\n",
      "Steps : 54000, \t Total Gen Loss : 32.54865264892578, \t Total Dis Loss : 0.028093574568629265\n",
      "Steps : 54100, \t Total Gen Loss : 27.49690818786621, \t Total Dis Loss : 6.098153971834108e-05\n",
      "Steps : 54200, \t Total Gen Loss : 29.673965454101562, \t Total Dis Loss : 9.713043255032972e-05\n",
      "Steps : 54300, \t Total Gen Loss : 28.994091033935547, \t Total Dis Loss : 3.0259237973950803e-05\n",
      "Steps : 54400, \t Total Gen Loss : 30.023141860961914, \t Total Dis Loss : 0.00013777831918559968\n",
      "Steps : 54500, \t Total Gen Loss : 27.644420623779297, \t Total Dis Loss : 0.0006506156641989946\n",
      "Steps : 54600, \t Total Gen Loss : 28.739654541015625, \t Total Dis Loss : 0.0002160183503292501\n",
      "Steps : 54700, \t Total Gen Loss : 25.23657989501953, \t Total Dis Loss : 7.437934254994616e-05\n",
      "Steps : 54800, \t Total Gen Loss : 27.80641746520996, \t Total Dis Loss : 6.553102866746485e-05\n",
      "Steps : 54900, \t Total Gen Loss : 28.13583755493164, \t Total Dis Loss : 0.00012759522360283881\n",
      "Steps : 55000, \t Total Gen Loss : 27.40604019165039, \t Total Dis Loss : 4.035168240079656e-05\n",
      "Steps : 55100, \t Total Gen Loss : 28.394527435302734, \t Total Dis Loss : 6.471587403211743e-05\n",
      "Steps : 55200, \t Total Gen Loss : 26.726383209228516, \t Total Dis Loss : 0.0012340052053332329\n",
      "Steps : 55300, \t Total Gen Loss : 24.85924530029297, \t Total Dis Loss : 0.0005102823488414288\n",
      "Steps : 55400, \t Total Gen Loss : 31.26214599609375, \t Total Dis Loss : 0.0005147827323526144\n",
      "Steps : 55500, \t Total Gen Loss : 26.773853302001953, \t Total Dis Loss : 0.0001483237574575469\n",
      "Steps : 55600, \t Total Gen Loss : 28.6704158782959, \t Total Dis Loss : 2.9377064493019134e-05\n",
      "Steps : 55700, \t Total Gen Loss : 27.039791107177734, \t Total Dis Loss : 6.154608854558319e-05\n",
      "Steps : 55800, \t Total Gen Loss : 31.358339309692383, \t Total Dis Loss : 2.3946658984641545e-05\n",
      "Steps : 55900, \t Total Gen Loss : 27.824254989624023, \t Total Dis Loss : 4.43112658103928e-05\n",
      "Steps : 56000, \t Total Gen Loss : 29.728321075439453, \t Total Dis Loss : 4.870901466347277e-05\n",
      "Steps : 56100, \t Total Gen Loss : 26.04608726501465, \t Total Dis Loss : 4.256536340108141e-05\n",
      "Steps : 56200, \t Total Gen Loss : 31.532222747802734, \t Total Dis Loss : 2.7205933292862028e-05\n",
      "Time for epoch 10 is 72.18586778640747 sec\n",
      "Steps : 56300, \t Total Gen Loss : 27.371559143066406, \t Total Dis Loss : 4.505447577685118e-05\n",
      "Steps : 56400, \t Total Gen Loss : 27.91623878479004, \t Total Dis Loss : 3.720321547007188e-05\n",
      "Steps : 56500, \t Total Gen Loss : 28.453458786010742, \t Total Dis Loss : 3.199502680217847e-05\n",
      "Steps : 56600, \t Total Gen Loss : 29.292760848999023, \t Total Dis Loss : 3.4141055948566645e-05\n",
      "Steps : 56700, \t Total Gen Loss : 25.25943374633789, \t Total Dis Loss : 6.122887134552002e-05\n",
      "Steps : 56800, \t Total Gen Loss : 25.489656448364258, \t Total Dis Loss : 5.231462273513898e-05\n",
      "Steps : 56900, \t Total Gen Loss : 27.549449920654297, \t Total Dis Loss : 8.294456347357482e-05\n",
      "Steps : 57000, \t Total Gen Loss : 26.248088836669922, \t Total Dis Loss : 5.212336691329256e-05\n",
      "Steps : 57100, \t Total Gen Loss : 29.374740600585938, \t Total Dis Loss : 4.629789327736944e-05\n",
      "Steps : 57200, \t Total Gen Loss : 27.859821319580078, \t Total Dis Loss : 4.3361622374504805e-05\n",
      "Steps : 57300, \t Total Gen Loss : 25.43197250366211, \t Total Dis Loss : 2.9287901270436123e-05\n",
      "Steps : 57400, \t Total Gen Loss : 26.590028762817383, \t Total Dis Loss : 5.025958671467379e-05\n",
      "Steps : 57500, \t Total Gen Loss : 28.620891571044922, \t Total Dis Loss : 2.7240903364145197e-05\n",
      "Steps : 57600, \t Total Gen Loss : 29.3701171875, \t Total Dis Loss : 0.002517000073567033\n",
      "Steps : 57700, \t Total Gen Loss : 23.921663284301758, \t Total Dis Loss : 0.00034160533687099814\n",
      "Steps : 57800, \t Total Gen Loss : 26.139678955078125, \t Total Dis Loss : 7.738536078250036e-05\n",
      "Steps : 57900, \t Total Gen Loss : 30.131502151489258, \t Total Dis Loss : 5.589480133494362e-05\n",
      "Steps : 58000, \t Total Gen Loss : 27.72979164123535, \t Total Dis Loss : 1.957901258720085e-05\n",
      "Steps : 58100, \t Total Gen Loss : 28.75710678100586, \t Total Dis Loss : 9.286242129746825e-05\n",
      "Steps : 58200, \t Total Gen Loss : 29.4534912109375, \t Total Dis Loss : 3.777545134653337e-05\n",
      "Steps : 58300, \t Total Gen Loss : 29.374799728393555, \t Total Dis Loss : 8.261592302005738e-05\n",
      "Steps : 58400, \t Total Gen Loss : 30.779142379760742, \t Total Dis Loss : 6.279047374846414e-05\n",
      "Steps : 58500, \t Total Gen Loss : 27.274749755859375, \t Total Dis Loss : 6.770069740014151e-05\n",
      "Steps : 58600, \t Total Gen Loss : 27.88744354248047, \t Total Dis Loss : 7.601828110637143e-05\n",
      "Steps : 58700, \t Total Gen Loss : 27.758668899536133, \t Total Dis Loss : 0.0004647370660677552\n",
      "Steps : 58800, \t Total Gen Loss : 36.871639251708984, \t Total Dis Loss : 0.0008533223299309611\n",
      "Steps : 58900, \t Total Gen Loss : 34.25749969482422, \t Total Dis Loss : 0.00937669724225998\n",
      "Steps : 59000, \t Total Gen Loss : 23.795391082763672, \t Total Dis Loss : 0.1148570328950882\n",
      "Steps : 59100, \t Total Gen Loss : 36.87192153930664, \t Total Dis Loss : 0.0007548571447841823\n",
      "Steps : 59200, \t Total Gen Loss : 31.496519088745117, \t Total Dis Loss : 0.14268438518047333\n",
      "Steps : 59300, \t Total Gen Loss : 35.72405242919922, \t Total Dis Loss : 7.350098894676194e-05\n",
      "Steps : 59400, \t Total Gen Loss : 29.942964553833008, \t Total Dis Loss : 5.3335417760536075e-05\n",
      "Steps : 59500, \t Total Gen Loss : 27.664348602294922, \t Total Dis Loss : 0.0001270138454856351\n",
      "Steps : 59600, \t Total Gen Loss : 29.728063583374023, \t Total Dis Loss : 0.0002221843460574746\n",
      "Steps : 59700, \t Total Gen Loss : 30.015369415283203, \t Total Dis Loss : 0.00024147566000465304\n",
      "Steps : 59800, \t Total Gen Loss : 29.05419921875, \t Total Dis Loss : 6.932306860107929e-05\n",
      "Steps : 59900, \t Total Gen Loss : 26.733003616333008, \t Total Dis Loss : 5.306199091137387e-05\n",
      "Steps : 60000, \t Total Gen Loss : 28.678268432617188, \t Total Dis Loss : 9.849558409769088e-05\n",
      "Steps : 60100, \t Total Gen Loss : 31.713088989257812, \t Total Dis Loss : 8.038429950829595e-05\n",
      "Steps : 60200, \t Total Gen Loss : 31.372055053710938, \t Total Dis Loss : 8.914279896998778e-05\n",
      "Steps : 60300, \t Total Gen Loss : 28.046836853027344, \t Total Dis Loss : 0.0007743003079667687\n",
      "Steps : 60400, \t Total Gen Loss : 29.296188354492188, \t Total Dis Loss : 0.00032818756881169975\n",
      "Steps : 60500, \t Total Gen Loss : 31.225811004638672, \t Total Dis Loss : 0.00010245078010484576\n",
      "Steps : 60600, \t Total Gen Loss : 28.54438018798828, \t Total Dis Loss : 3.409267810638994e-05\n",
      "Steps : 60700, \t Total Gen Loss : 27.83868408203125, \t Total Dis Loss : 5.507951573235914e-05\n",
      "Steps : 60800, \t Total Gen Loss : 27.367473602294922, \t Total Dis Loss : 0.00012886800686828792\n",
      "Steps : 60900, \t Total Gen Loss : 30.025829315185547, \t Total Dis Loss : 5.917247472098097e-05\n",
      "Steps : 61000, \t Total Gen Loss : 29.438888549804688, \t Total Dis Loss : 0.00010810163803398609\n",
      "Steps : 61100, \t Total Gen Loss : 26.591577529907227, \t Total Dis Loss : 0.00010088853014167398\n",
      "Steps : 61200, \t Total Gen Loss : 27.606307983398438, \t Total Dis Loss : 4.053930751979351e-05\n",
      "Steps : 61300, \t Total Gen Loss : 29.377239227294922, \t Total Dis Loss : 3.153371289954521e-05\n",
      "Steps : 61400, \t Total Gen Loss : 28.945690155029297, \t Total Dis Loss : 5.2089046221226454e-05\n",
      "Steps : 61500, \t Total Gen Loss : 29.45862579345703, \t Total Dis Loss : 4.7657551476731896e-05\n",
      "Steps : 61600, \t Total Gen Loss : 25.95462989807129, \t Total Dis Loss : 0.0001905850658658892\n",
      "Steps : 61700, \t Total Gen Loss : 28.4616641998291, \t Total Dis Loss : 7.057619222905487e-05\n",
      "Steps : 61800, \t Total Gen Loss : 27.18365478515625, \t Total Dis Loss : 0.0004597342340275645\n",
      "Time for epoch 11 is 72.07740783691406 sec\n",
      "Steps : 61900, \t Total Gen Loss : 27.77984619140625, \t Total Dis Loss : 0.0011471475008875132\n",
      "Steps : 62000, \t Total Gen Loss : 29.443899154663086, \t Total Dis Loss : 0.00023029914882499725\n",
      "Steps : 62100, \t Total Gen Loss : 30.818979263305664, \t Total Dis Loss : 0.07660321891307831\n",
      "Steps : 62200, \t Total Gen Loss : 27.824989318847656, \t Total Dis Loss : 2.3843123926781118e-05\n",
      "Steps : 62300, \t Total Gen Loss : 30.34676742553711, \t Total Dis Loss : 4.2279956687707454e-05\n",
      "Steps : 62400, \t Total Gen Loss : 30.166799545288086, \t Total Dis Loss : 0.00011240197636652738\n",
      "Steps : 62500, \t Total Gen Loss : 29.097915649414062, \t Total Dis Loss : 0.00011615561379585415\n",
      "Steps : 62600, \t Total Gen Loss : 34.127044677734375, \t Total Dis Loss : 4.555318446364254e-05\n",
      "Steps : 62700, \t Total Gen Loss : 30.22507095336914, \t Total Dis Loss : 9.693906758911908e-05\n",
      "Steps : 62800, \t Total Gen Loss : 33.33203887939453, \t Total Dis Loss : 0.0097916629165411\n",
      "Steps : 62900, \t Total Gen Loss : 31.841096878051758, \t Total Dis Loss : 3.5905501135857776e-05\n",
      "Steps : 63000, \t Total Gen Loss : 24.519290924072266, \t Total Dis Loss : 2.528705358505249\n",
      "Steps : 63100, \t Total Gen Loss : 29.899837493896484, \t Total Dis Loss : 0.00022975727915763855\n",
      "Steps : 63200, \t Total Gen Loss : 28.329303741455078, \t Total Dis Loss : 0.00030932785011827946\n",
      "Steps : 63300, \t Total Gen Loss : 28.929683685302734, \t Total Dis Loss : 0.00015076530689839274\n",
      "Steps : 63400, \t Total Gen Loss : 30.062175750732422, \t Total Dis Loss : 0.0002180129085900262\n",
      "Steps : 63500, \t Total Gen Loss : 28.60259437561035, \t Total Dis Loss : 8.97968711797148e-05\n",
      "Steps : 63600, \t Total Gen Loss : 28.166648864746094, \t Total Dis Loss : 0.0003812667855527252\n",
      "Steps : 63700, \t Total Gen Loss : 30.35977554321289, \t Total Dis Loss : 0.00011557312973309308\n",
      "Steps : 63800, \t Total Gen Loss : 28.968360900878906, \t Total Dis Loss : 0.00014093892241362482\n",
      "Steps : 63900, \t Total Gen Loss : 31.556730270385742, \t Total Dis Loss : 2.5476740120211616e-05\n",
      "Steps : 64000, \t Total Gen Loss : 31.350107192993164, \t Total Dis Loss : 6.182848301250488e-05\n",
      "Steps : 64100, \t Total Gen Loss : 28.03145408630371, \t Total Dis Loss : 2.0936357032041997e-05\n",
      "Steps : 64200, \t Total Gen Loss : 29.218246459960938, \t Total Dis Loss : 6.518377631437033e-05\n",
      "Steps : 64300, \t Total Gen Loss : 28.144533157348633, \t Total Dis Loss : 0.001433999859727919\n",
      "Steps : 64400, \t Total Gen Loss : 28.681968688964844, \t Total Dis Loss : 0.0005147544434294105\n",
      "Steps : 64500, \t Total Gen Loss : 30.515419006347656, \t Total Dis Loss : 1.5298857761081308e-05\n",
      "Steps : 64600, \t Total Gen Loss : 29.003137588500977, \t Total Dis Loss : 3.469055445748381e-05\n",
      "Steps : 64700, \t Total Gen Loss : 28.975589752197266, \t Total Dis Loss : 5.3164192650001496e-05\n",
      "Steps : 64800, \t Total Gen Loss : 25.249176025390625, \t Total Dis Loss : 9.443160524824634e-05\n",
      "Steps : 64900, \t Total Gen Loss : 23.707683563232422, \t Total Dis Loss : 0.017098363488912582\n",
      "Steps : 65000, \t Total Gen Loss : 27.064077377319336, \t Total Dis Loss : 0.0003227402630727738\n",
      "Steps : 65100, \t Total Gen Loss : 27.15135383605957, \t Total Dis Loss : 0.00017630192451179028\n",
      "Steps : 65200, \t Total Gen Loss : 26.78003692626953, \t Total Dis Loss : 0.0002768109552562237\n",
      "Steps : 65300, \t Total Gen Loss : 26.701622009277344, \t Total Dis Loss : 0.0003506449575070292\n",
      "Steps : 65400, \t Total Gen Loss : 24.916481018066406, \t Total Dis Loss : 0.0033699716441333294\n",
      "Steps : 65500, \t Total Gen Loss : 25.279071807861328, \t Total Dis Loss : 0.0006906489725224674\n",
      "Steps : 65600, \t Total Gen Loss : 29.158851623535156, \t Total Dis Loss : 0.0001460958446841687\n",
      "Steps : 65700, \t Total Gen Loss : 28.79849624633789, \t Total Dis Loss : 0.00011292176350252703\n",
      "Steps : 65800, \t Total Gen Loss : 26.603328704833984, \t Total Dis Loss : 4.9758364184526727e-05\n",
      "Steps : 65900, \t Total Gen Loss : 27.143978118896484, \t Total Dis Loss : 0.00014020013622939587\n",
      "Steps : 66000, \t Total Gen Loss : 28.12574577331543, \t Total Dis Loss : 2.70225755230058e-05\n",
      "Steps : 66100, \t Total Gen Loss : 30.152950286865234, \t Total Dis Loss : 7.665099110454321e-05\n",
      "Steps : 66200, \t Total Gen Loss : 34.025001525878906, \t Total Dis Loss : 6.574919825652614e-05\n",
      "Steps : 66300, \t Total Gen Loss : 25.99306297302246, \t Total Dis Loss : 0.00015842262655496597\n",
      "Steps : 66400, \t Total Gen Loss : 29.834606170654297, \t Total Dis Loss : 0.00012951955432072282\n",
      "Steps : 66500, \t Total Gen Loss : 30.507671356201172, \t Total Dis Loss : 0.00015130975225474685\n",
      "Steps : 66600, \t Total Gen Loss : 26.272022247314453, \t Total Dis Loss : 4.397235534270294e-05\n",
      "Steps : 66700, \t Total Gen Loss : 28.154151916503906, \t Total Dis Loss : 2.7728981876862235e-05\n",
      "Steps : 66800, \t Total Gen Loss : 25.812118530273438, \t Total Dis Loss : 0.0002890974865294993\n",
      "Steps : 66900, \t Total Gen Loss : 26.14556121826172, \t Total Dis Loss : 0.00017281882173847407\n",
      "Steps : 67000, \t Total Gen Loss : 24.222877502441406, \t Total Dis Loss : 0.00011829643335659057\n",
      "Steps : 67100, \t Total Gen Loss : 29.512409210205078, \t Total Dis Loss : 4.644582804758102e-05\n",
      "Steps : 67200, \t Total Gen Loss : 23.928680419921875, \t Total Dis Loss : 7.119191286619753e-05\n",
      "Steps : 67300, \t Total Gen Loss : 26.821088790893555, \t Total Dis Loss : 4.9754049541661516e-05\n",
      "Steps : 67400, \t Total Gen Loss : 27.019493103027344, \t Total Dis Loss : 2.8742286303895526e-05\n",
      "Steps : 67500, \t Total Gen Loss : 28.304903030395508, \t Total Dis Loss : 4.117375647183508e-05\n",
      "Time for epoch 12 is 72.08270955085754 sec\n",
      "Steps : 67600, \t Total Gen Loss : 31.24636459350586, \t Total Dis Loss : 2.539376509957947e-05\n",
      "Steps : 67700, \t Total Gen Loss : 27.039926528930664, \t Total Dis Loss : 2.3918797523947433e-05\n",
      "Steps : 67800, \t Total Gen Loss : 30.53351593017578, \t Total Dis Loss : 3.876550181303173e-05\n",
      "Steps : 67900, \t Total Gen Loss : 27.132080078125, \t Total Dis Loss : 3.4177421184722334e-05\n",
      "Steps : 68000, \t Total Gen Loss : 27.834875106811523, \t Total Dis Loss : 3.6021621781401336e-05\n",
      "Steps : 68100, \t Total Gen Loss : 36.66486358642578, \t Total Dis Loss : 0.000485109630972147\n",
      "Steps : 68200, \t Total Gen Loss : 39.998069763183594, \t Total Dis Loss : 0.0003894013643730432\n",
      "Steps : 68300, \t Total Gen Loss : 33.89793395996094, \t Total Dis Loss : 0.00011778467160183936\n",
      "Steps : 68400, \t Total Gen Loss : 27.288236618041992, \t Total Dis Loss : 0.0006111764232628047\n",
      "Steps : 68500, \t Total Gen Loss : 30.443391799926758, \t Total Dis Loss : 0.0013994673499837518\n",
      "Steps : 68600, \t Total Gen Loss : 35.84272766113281, \t Total Dis Loss : 0.000704870792105794\n",
      "Steps : 68700, \t Total Gen Loss : 31.905576705932617, \t Total Dis Loss : 0.00011993394582532346\n",
      "Steps : 68800, \t Total Gen Loss : 31.369029998779297, \t Total Dis Loss : 0.020079299807548523\n",
      "Steps : 68900, \t Total Gen Loss : 30.44381332397461, \t Total Dis Loss : 0.0005021306569688022\n",
      "Steps : 69000, \t Total Gen Loss : 29.574182510375977, \t Total Dis Loss : 0.01570156402885914\n",
      "Steps : 69100, \t Total Gen Loss : 28.490943908691406, \t Total Dis Loss : 0.0003995862207375467\n",
      "Steps : 69200, \t Total Gen Loss : 29.12018585205078, \t Total Dis Loss : 0.02009422890841961\n",
      "Steps : 69300, \t Total Gen Loss : 31.0347957611084, \t Total Dis Loss : 0.00041547053842805326\n",
      "Steps : 69400, \t Total Gen Loss : 34.176719665527344, \t Total Dis Loss : 1.68622937053442e-05\n",
      "Steps : 69500, \t Total Gen Loss : 37.17683792114258, \t Total Dis Loss : 9.758920350577682e-05\n",
      "Steps : 69600, \t Total Gen Loss : 31.70941162109375, \t Total Dis Loss : 0.0008682811167091131\n",
      "Steps : 69700, \t Total Gen Loss : 28.5831241607666, \t Total Dis Loss : 0.0005612183012999594\n",
      "Steps : 69800, \t Total Gen Loss : 31.605426788330078, \t Total Dis Loss : 0.00012206123938085511\n",
      "Steps : 69900, \t Total Gen Loss : 35.83439254760742, \t Total Dis Loss : 7.340044976444915e-05\n",
      "Steps : 70000, \t Total Gen Loss : 33.513916015625, \t Total Dis Loss : 0.000199434463866055\n",
      "Steps : 70100, \t Total Gen Loss : 31.242692947387695, \t Total Dis Loss : 0.00030130345840007067\n",
      "Steps : 70200, \t Total Gen Loss : 27.31337547302246, \t Total Dis Loss : 0.0005465538124553859\n",
      "Steps : 70300, \t Total Gen Loss : 25.970808029174805, \t Total Dis Loss : 0.0002753576554823667\n",
      "Steps : 70400, \t Total Gen Loss : 27.67694091796875, \t Total Dis Loss : 0.00013427447993308306\n",
      "Steps : 70500, \t Total Gen Loss : 29.343029022216797, \t Total Dis Loss : 0.00010634779027896002\n",
      "Steps : 70600, \t Total Gen Loss : 29.33085060119629, \t Total Dis Loss : 9.35256975935772e-05\n",
      "Steps : 70700, \t Total Gen Loss : 26.01361846923828, \t Total Dis Loss : 0.0007701935828663409\n",
      "Steps : 70800, \t Total Gen Loss : 30.782957077026367, \t Total Dis Loss : 5.756484461016953e-05\n",
      "Steps : 70900, \t Total Gen Loss : 32.02009582519531, \t Total Dis Loss : 0.0018167307134717703\n",
      "Steps : 71000, \t Total Gen Loss : 30.97993278503418, \t Total Dis Loss : 0.00016185561253223568\n",
      "Steps : 71100, \t Total Gen Loss : 28.29519271850586, \t Total Dis Loss : 0.00030297544435597956\n",
      "Steps : 71200, \t Total Gen Loss : 33.22101593017578, \t Total Dis Loss : 0.00035171344643458724\n",
      "Steps : 71300, \t Total Gen Loss : 33.52410888671875, \t Total Dis Loss : 0.0006485217600129545\n",
      "Steps : 71400, \t Total Gen Loss : 32.54018020629883, \t Total Dis Loss : 0.00017748161917552352\n",
      "Steps : 71500, \t Total Gen Loss : 29.436965942382812, \t Total Dis Loss : 0.003392026061192155\n",
      "Steps : 71600, \t Total Gen Loss : 29.707712173461914, \t Total Dis Loss : 0.00041729267104528844\n",
      "Steps : 71700, \t Total Gen Loss : 27.955904006958008, \t Total Dis Loss : 0.00012047351629007608\n",
      "Steps : 71800, \t Total Gen Loss : 26.49889373779297, \t Total Dis Loss : 0.00015269263531081378\n",
      "Steps : 71900, \t Total Gen Loss : 39.59138870239258, \t Total Dis Loss : 0.0007487866678275168\n",
      "Steps : 72000, \t Total Gen Loss : 30.67284393310547, \t Total Dis Loss : 0.00039626809302717447\n",
      "Steps : 72100, \t Total Gen Loss : 31.647109985351562, \t Total Dis Loss : 0.0020541518460959196\n",
      "Steps : 72200, \t Total Gen Loss : 33.5339469909668, \t Total Dis Loss : 0.00015999554307200015\n",
      "Steps : 72300, \t Total Gen Loss : 32.09516143798828, \t Total Dis Loss : 0.00028590246802195907\n",
      "Steps : 72400, \t Total Gen Loss : 29.311521530151367, \t Total Dis Loss : 0.00015369753236882389\n",
      "Steps : 72500, \t Total Gen Loss : 32.13797378540039, \t Total Dis Loss : 0.00036608020309358835\n",
      "Steps : 72600, \t Total Gen Loss : 35.66532516479492, \t Total Dis Loss : 0.000379545905161649\n",
      "Steps : 72700, \t Total Gen Loss : 35.17412185668945, \t Total Dis Loss : 0.00020390230929479003\n",
      "Steps : 72800, \t Total Gen Loss : 31.81890106201172, \t Total Dis Loss : 8.401145169045776e-05\n",
      "Steps : 72900, \t Total Gen Loss : 30.840656280517578, \t Total Dis Loss : 0.0002830128069035709\n",
      "Steps : 73000, \t Total Gen Loss : 28.679248809814453, \t Total Dis Loss : 0.00024023756850510836\n",
      "Steps : 73100, \t Total Gen Loss : 30.876550674438477, \t Total Dis Loss : 8.074137440416962e-05\n",
      "Time for epoch 13 is 72.11580443382263 sec\n",
      "Steps : 73200, \t Total Gen Loss : 30.31631088256836, \t Total Dis Loss : 9.316114301327616e-05\n",
      "Steps : 73300, \t Total Gen Loss : 34.47782897949219, \t Total Dis Loss : 0.015820814296603203\n",
      "Steps : 73400, \t Total Gen Loss : 34.982688903808594, \t Total Dis Loss : 0.00023604882881045341\n",
      "Steps : 73500, \t Total Gen Loss : 32.189762115478516, \t Total Dis Loss : 0.00021873039077036083\n",
      "Steps : 73600, \t Total Gen Loss : 31.095829010009766, \t Total Dis Loss : 0.00040109577821567655\n",
      "Steps : 73700, \t Total Gen Loss : 31.804481506347656, \t Total Dis Loss : 0.00029181517311371863\n",
      "Steps : 73800, \t Total Gen Loss : 35.82904052734375, \t Total Dis Loss : 9.513933036942035e-05\n",
      "Steps : 73900, \t Total Gen Loss : 33.41035461425781, \t Total Dis Loss : 0.0001270085631404072\n",
      "Steps : 74000, \t Total Gen Loss : 30.99321937561035, \t Total Dis Loss : 9.474910621065646e-05\n",
      "Steps : 74100, \t Total Gen Loss : 31.178001403808594, \t Total Dis Loss : 8.804661047179252e-05\n",
      "Steps : 74200, \t Total Gen Loss : 29.840408325195312, \t Total Dis Loss : 7.728444325039163e-05\n",
      "Steps : 74300, \t Total Gen Loss : 27.949214935302734, \t Total Dis Loss : 7.326460763579234e-05\n",
      "Steps : 74400, \t Total Gen Loss : 29.980182647705078, \t Total Dis Loss : 8.073498611338437e-05\n",
      "Steps : 74500, \t Total Gen Loss : 32.79410171508789, \t Total Dis Loss : 4.26132173743099e-05\n",
      "Steps : 74600, \t Total Gen Loss : 30.60377311706543, \t Total Dis Loss : 0.0017859026556834579\n",
      "Steps : 74700, \t Total Gen Loss : 27.925846099853516, \t Total Dis Loss : 0.00043355958769097924\n",
      "Steps : 74800, \t Total Gen Loss : 31.68866539001465, \t Total Dis Loss : 0.00011930732580367476\n",
      "Steps : 74900, \t Total Gen Loss : 29.737131118774414, \t Total Dis Loss : 0.0002990530338138342\n",
      "Steps : 75000, \t Total Gen Loss : 30.41979217529297, \t Total Dis Loss : 0.0007027990650385618\n",
      "Steps : 75100, \t Total Gen Loss : 29.340328216552734, \t Total Dis Loss : 0.00015956124116200954\n",
      "Steps : 75200, \t Total Gen Loss : 35.01006317138672, \t Total Dis Loss : 0.0007278923294506967\n",
      "Steps : 75300, \t Total Gen Loss : 30.991079330444336, \t Total Dis Loss : 0.00010649271280271932\n",
      "Steps : 75400, \t Total Gen Loss : 32.19928741455078, \t Total Dis Loss : 0.0002876801881939173\n",
      "Steps : 75500, \t Total Gen Loss : 31.65151023864746, \t Total Dis Loss : 0.0005349417915567756\n",
      "Steps : 75600, \t Total Gen Loss : 32.0451774597168, \t Total Dis Loss : 0.0017612045630812645\n",
      "Steps : 75700, \t Total Gen Loss : 31.06658172607422, \t Total Dis Loss : 0.0005077726091258228\n",
      "Steps : 75800, \t Total Gen Loss : 31.262678146362305, \t Total Dis Loss : 0.001502028084360063\n",
      "Steps : 75900, \t Total Gen Loss : 28.832942962646484, \t Total Dis Loss : 9.03376640053466e-05\n",
      "Steps : 76000, \t Total Gen Loss : 29.737659454345703, \t Total Dis Loss : 6.842147558927536e-05\n",
      "Steps : 76100, \t Total Gen Loss : 31.085895538330078, \t Total Dis Loss : 2.69372139882762e-05\n",
      "Steps : 76200, \t Total Gen Loss : 23.077316284179688, \t Total Dis Loss : 0.0005556323449127376\n",
      "Steps : 76300, \t Total Gen Loss : 27.819534301757812, \t Total Dis Loss : 0.0007265484309755266\n",
      "Steps : 76400, \t Total Gen Loss : 33.304786682128906, \t Total Dis Loss : 0.00026646829792298377\n",
      "Steps : 76500, \t Total Gen Loss : 33.18177795410156, \t Total Dis Loss : 0.00018564301717560738\n",
      "Steps : 76600, \t Total Gen Loss : 29.625667572021484, \t Total Dis Loss : 0.0008785392856225371\n",
      "Steps : 76700, \t Total Gen Loss : 33.602508544921875, \t Total Dis Loss : 0.0002512533392291516\n",
      "Steps : 76800, \t Total Gen Loss : 26.506465911865234, \t Total Dis Loss : 0.0007694252417422831\n",
      "Steps : 76900, \t Total Gen Loss : 28.570541381835938, \t Total Dis Loss : 0.0014714134158566594\n",
      "Steps : 77000, \t Total Gen Loss : 26.169797897338867, \t Total Dis Loss : 0.00026826298562809825\n",
      "Steps : 77100, \t Total Gen Loss : 28.334321975708008, \t Total Dis Loss : 0.0002418324293103069\n",
      "Steps : 77200, \t Total Gen Loss : 25.768871307373047, \t Total Dis Loss : 0.0006260485388338566\n",
      "Steps : 77300, \t Total Gen Loss : 27.57141876220703, \t Total Dis Loss : 0.00012462688027881086\n",
      "Steps : 77400, \t Total Gen Loss : 27.005966186523438, \t Total Dis Loss : 4.197809175821021e-05\n",
      "Steps : 77500, \t Total Gen Loss : 27.982229232788086, \t Total Dis Loss : 0.00011632740643108264\n",
      "Steps : 77600, \t Total Gen Loss : 27.4554500579834, \t Total Dis Loss : 0.000594317214563489\n",
      "Steps : 77700, \t Total Gen Loss : 24.11199378967285, \t Total Dis Loss : 9.629796113586053e-05\n",
      "Steps : 77800, \t Total Gen Loss : 24.065258026123047, \t Total Dis Loss : 0.0001474747114116326\n",
      "Steps : 77900, \t Total Gen Loss : 26.30288314819336, \t Total Dis Loss : 7.324466423597187e-05\n",
      "Steps : 78000, \t Total Gen Loss : 31.172996520996094, \t Total Dis Loss : 3.419106360524893e-05\n",
      "Steps : 78100, \t Total Gen Loss : 25.004554748535156, \t Total Dis Loss : 7.050277781672776e-05\n",
      "Steps : 78200, \t Total Gen Loss : 27.38409423828125, \t Total Dis Loss : 0.00023034337209537625\n",
      "Steps : 78300, \t Total Gen Loss : 25.739837646484375, \t Total Dis Loss : 0.00016851862892508507\n",
      "Steps : 78400, \t Total Gen Loss : 26.098079681396484, \t Total Dis Loss : 0.0001361747708870098\n",
      "Steps : 78500, \t Total Gen Loss : 23.856510162353516, \t Total Dis Loss : 0.0007541201775893569\n",
      "Steps : 78600, \t Total Gen Loss : 25.472217559814453, \t Total Dis Loss : 0.00023995504307094961\n",
      "Steps : 78700, \t Total Gen Loss : 26.101072311401367, \t Total Dis Loss : 0.00012945425987709314\n",
      "Time for epoch 14 is 72.09571838378906 sec\n",
      "Steps : 78800, \t Total Gen Loss : 23.980182647705078, \t Total Dis Loss : 0.0002416669885860756\n",
      "Steps : 78900, \t Total Gen Loss : 29.24889373779297, \t Total Dis Loss : 5.810175207443535e-05\n",
      "Steps : 79000, \t Total Gen Loss : 29.54138946533203, \t Total Dis Loss : 0.00010670052870409563\n",
      "Steps : 79100, \t Total Gen Loss : 24.530895233154297, \t Total Dis Loss : 0.007860233075916767\n",
      "Steps : 79200, \t Total Gen Loss : 27.16890525817871, \t Total Dis Loss : 0.00010954854951705784\n",
      "Steps : 79300, \t Total Gen Loss : 26.623306274414062, \t Total Dis Loss : 0.0003006633778568357\n",
      "Steps : 79400, \t Total Gen Loss : 28.809539794921875, \t Total Dis Loss : 0.0001892175350803882\n",
      "Steps : 79500, \t Total Gen Loss : 25.736968994140625, \t Total Dis Loss : 0.00011984557204414159\n",
      "Steps : 79600, \t Total Gen Loss : 26.842411041259766, \t Total Dis Loss : 0.00015689294377807528\n",
      "Steps : 79700, \t Total Gen Loss : 28.965742111206055, \t Total Dis Loss : 6.964625208638608e-05\n",
      "Steps : 79800, \t Total Gen Loss : 25.307994842529297, \t Total Dis Loss : 5.6195603974629194e-05\n",
      "Steps : 79900, \t Total Gen Loss : 27.01825714111328, \t Total Dis Loss : 0.0006111591355875134\n",
      "Steps : 80000, \t Total Gen Loss : 25.780517578125, \t Total Dis Loss : 0.00010821693285834044\n",
      "Steps : 80100, \t Total Gen Loss : 28.432411193847656, \t Total Dis Loss : 0.0005494393990375102\n",
      "Steps : 80200, \t Total Gen Loss : 28.148712158203125, \t Total Dis Loss : 0.00013892004790250212\n",
      "Steps : 80300, \t Total Gen Loss : 23.708723068237305, \t Total Dis Loss : 0.0002580390719231218\n",
      "Steps : 80400, \t Total Gen Loss : 26.348602294921875, \t Total Dis Loss : 9.43729464779608e-05\n",
      "Steps : 80500, \t Total Gen Loss : 26.338699340820312, \t Total Dis Loss : 0.00011944771540584043\n",
      "Steps : 80600, \t Total Gen Loss : 25.531553268432617, \t Total Dis Loss : 4.316270860726945e-05\n",
      "Steps : 80700, \t Total Gen Loss : 26.23956871032715, \t Total Dis Loss : 4.8889789468375966e-05\n",
      "Steps : 80800, \t Total Gen Loss : 29.2642822265625, \t Total Dis Loss : 0.00023730783141218126\n",
      "Steps : 80900, \t Total Gen Loss : 26.725940704345703, \t Total Dis Loss : 0.0001223909348482266\n",
      "Steps : 81000, \t Total Gen Loss : 32.09276580810547, \t Total Dis Loss : 2.9494625778170303e-05\n",
      "Steps : 81100, \t Total Gen Loss : 28.82295036315918, \t Total Dis Loss : 0.000537332147359848\n",
      "Steps : 81200, \t Total Gen Loss : 27.070457458496094, \t Total Dis Loss : 0.00010797633876791224\n",
      "Steps : 81300, \t Total Gen Loss : 28.424882888793945, \t Total Dis Loss : 0.00011841995001304895\n",
      "Steps : 81400, \t Total Gen Loss : 33.144561767578125, \t Total Dis Loss : 2.9558699679910205e-05\n",
      "Steps : 81500, \t Total Gen Loss : 22.625370025634766, \t Total Dis Loss : 0.014544452540576458\n",
      "Steps : 81600, \t Total Gen Loss : 26.340656280517578, \t Total Dis Loss : 0.000967581057921052\n",
      "Steps : 81700, \t Total Gen Loss : 26.386144638061523, \t Total Dis Loss : 5.461552063934505e-05\n",
      "Steps : 81800, \t Total Gen Loss : 27.938600540161133, \t Total Dis Loss : 0.00010095969628309831\n",
      "Steps : 81900, \t Total Gen Loss : 25.480098724365234, \t Total Dis Loss : 0.0014244916383177042\n",
      "Steps : 82000, \t Total Gen Loss : 25.177753448486328, \t Total Dis Loss : 0.0007341899327002466\n",
      "Steps : 82100, \t Total Gen Loss : 27.42800521850586, \t Total Dis Loss : 3.595031739678234e-05\n",
      "Steps : 82200, \t Total Gen Loss : 25.882787704467773, \t Total Dis Loss : 0.00021239671332295984\n",
      "Steps : 82300, \t Total Gen Loss : 23.138294219970703, \t Total Dis Loss : 0.0012466878397390246\n",
      "Steps : 82400, \t Total Gen Loss : 32.61334228515625, \t Total Dis Loss : 8.15347011666745e-05\n",
      "Steps : 82500, \t Total Gen Loss : 32.22991943359375, \t Total Dis Loss : 2.3408661945722997e-05\n",
      "Steps : 82600, \t Total Gen Loss : 27.55608367919922, \t Total Dis Loss : 0.0005241338512860239\n",
      "Steps : 82700, \t Total Gen Loss : 24.754802703857422, \t Total Dis Loss : 0.00038822295027785003\n",
      "Steps : 82800, \t Total Gen Loss : 26.697004318237305, \t Total Dis Loss : 0.0005644885823130608\n",
      "Steps : 82900, \t Total Gen Loss : 27.604999542236328, \t Total Dis Loss : 3.940631722798571e-05\n",
      "Steps : 83000, \t Total Gen Loss : 26.928176879882812, \t Total Dis Loss : 0.00020675668201874942\n",
      "Steps : 83100, \t Total Gen Loss : 30.706560134887695, \t Total Dis Loss : 2.0657531422330067e-05\n",
      "Steps : 83200, \t Total Gen Loss : 28.051002502441406, \t Total Dis Loss : 0.00015363965940196067\n",
      "Steps : 83300, \t Total Gen Loss : 29.686466217041016, \t Total Dis Loss : 0.0001741795422276482\n",
      "Steps : 83400, \t Total Gen Loss : 26.11726951599121, \t Total Dis Loss : 0.00011922230623895302\n",
      "Steps : 83500, \t Total Gen Loss : 28.531780242919922, \t Total Dis Loss : 0.0008613768732175231\n",
      "Steps : 83600, \t Total Gen Loss : 26.816585540771484, \t Total Dis Loss : 0.00020901091920677572\n",
      "Steps : 83700, \t Total Gen Loss : 27.65157699584961, \t Total Dis Loss : 6.464905163738877e-05\n",
      "Steps : 83800, \t Total Gen Loss : 30.478092193603516, \t Total Dis Loss : 1.7673912225291133e-05\n",
      "Steps : 83900, \t Total Gen Loss : 27.166080474853516, \t Total Dis Loss : 3.930068123736419e-05\n",
      "Steps : 84000, \t Total Gen Loss : 27.382139205932617, \t Total Dis Loss : 6.567057425854728e-05\n",
      "Steps : 84100, \t Total Gen Loss : 28.35696029663086, \t Total Dis Loss : 0.00018540018936619163\n",
      "Steps : 84200, \t Total Gen Loss : 27.456768035888672, \t Total Dis Loss : 5.236595097812824e-05\n",
      "Steps : 84300, \t Total Gen Loss : 33.40502166748047, \t Total Dis Loss : 2.0178540580673143e-05\n",
      "Time for epoch 15 is 72.11862659454346 sec\n",
      "Steps : 84400, \t Total Gen Loss : 33.37086868286133, \t Total Dis Loss : 0.00015716769848950207\n",
      "Steps : 84500, \t Total Gen Loss : 33.354766845703125, \t Total Dis Loss : 6.679654325125739e-05\n",
      "Steps : 84600, \t Total Gen Loss : 34.87238311767578, \t Total Dis Loss : 1.849517138907686e-05\n",
      "Steps : 84700, \t Total Gen Loss : 35.0637092590332, \t Total Dis Loss : 4.7016652388265356e-05\n",
      "Steps : 84800, \t Total Gen Loss : 27.33043670654297, \t Total Dis Loss : 0.0004840367764700204\n",
      "Steps : 84900, \t Total Gen Loss : 28.035587310791016, \t Total Dis Loss : 0.00034606328699737787\n",
      "Steps : 85000, \t Total Gen Loss : 33.3514289855957, \t Total Dis Loss : 0.00019137370691169053\n",
      "Steps : 85100, \t Total Gen Loss : 32.19884490966797, \t Total Dis Loss : 0.0008222382748499513\n",
      "Steps : 85200, \t Total Gen Loss : 34.005516052246094, \t Total Dis Loss : 8.06453317636624e-05\n",
      "Steps : 85300, \t Total Gen Loss : 31.32122802734375, \t Total Dis Loss : 2.5293458747910336e-05\n",
      "Steps : 85400, \t Total Gen Loss : 35.270957946777344, \t Total Dis Loss : 0.00014414773613680154\n",
      "Steps : 85500, \t Total Gen Loss : 31.488622665405273, \t Total Dis Loss : 7.965481199789792e-05\n",
      "Steps : 85600, \t Total Gen Loss : 31.703126907348633, \t Total Dis Loss : 0.0009826957248151302\n",
      "Steps : 85700, \t Total Gen Loss : 30.031496047973633, \t Total Dis Loss : 8.473377238260582e-05\n",
      "Steps : 85800, \t Total Gen Loss : 36.84897232055664, \t Total Dis Loss : 0.0008084520231932402\n",
      "Steps : 85900, \t Total Gen Loss : 32.31837844848633, \t Total Dis Loss : 0.0001349990488961339\n",
      "Steps : 86000, \t Total Gen Loss : 35.48848342895508, \t Total Dis Loss : 0.0002478595415595919\n",
      "Steps : 86100, \t Total Gen Loss : 36.28338623046875, \t Total Dis Loss : 0.0004274935054127127\n",
      "Steps : 86200, \t Total Gen Loss : 30.29297637939453, \t Total Dis Loss : 4.632714262697846e-05\n",
      "Steps : 86300, \t Total Gen Loss : 32.14509582519531, \t Total Dis Loss : 0.00019116094335913658\n",
      "Steps : 86400, \t Total Gen Loss : 28.0358829498291, \t Total Dis Loss : 0.00012686476111412048\n",
      "Steps : 86500, \t Total Gen Loss : 27.506397247314453, \t Total Dis Loss : 6.728524022037163e-05\n",
      "Steps : 86600, \t Total Gen Loss : 27.03962516784668, \t Total Dis Loss : 5.647601938107982e-05\n",
      "Steps : 86700, \t Total Gen Loss : 27.58462142944336, \t Total Dis Loss : 0.0003644109528977424\n",
      "Steps : 86800, \t Total Gen Loss : 28.71258544921875, \t Total Dis Loss : 0.00015425555466208607\n",
      "Steps : 86900, \t Total Gen Loss : 28.940143585205078, \t Total Dis Loss : 0.0002902208361774683\n",
      "Steps : 87000, \t Total Gen Loss : 28.125354766845703, \t Total Dis Loss : 0.00021598595776595175\n",
      "Steps : 87100, \t Total Gen Loss : 28.794811248779297, \t Total Dis Loss : 9.96324815787375e-05\n",
      "Steps : 87200, \t Total Gen Loss : 28.132427215576172, \t Total Dis Loss : 3.32935233018361e-05\n",
      "Steps : 87300, \t Total Gen Loss : 27.19057846069336, \t Total Dis Loss : 3.935821951017715e-05\n",
      "Steps : 87400, \t Total Gen Loss : 26.325334548950195, \t Total Dis Loss : 8.811577572487295e-05\n",
      "Steps : 87500, \t Total Gen Loss : 28.1201171875, \t Total Dis Loss : 4.871005148743279e-05\n",
      "Steps : 87600, \t Total Gen Loss : 24.632593154907227, \t Total Dis Loss : 0.00011508038005558774\n",
      "Steps : 87700, \t Total Gen Loss : 26.88348388671875, \t Total Dis Loss : 0.00011342056677676737\n",
      "Steps : 87800, \t Total Gen Loss : 25.91944122314453, \t Total Dis Loss : 0.0011419414076954126\n",
      "Steps : 87900, \t Total Gen Loss : 23.63058853149414, \t Total Dis Loss : 0.0012388653121888638\n",
      "Steps : 88000, \t Total Gen Loss : 27.342525482177734, \t Total Dis Loss : 0.0003251001180615276\n",
      "Steps : 88100, \t Total Gen Loss : 27.585124969482422, \t Total Dis Loss : 0.00024579488672316074\n",
      "Steps : 88200, \t Total Gen Loss : 25.406625747680664, \t Total Dis Loss : 0.0001271079236175865\n",
      "Steps : 88300, \t Total Gen Loss : 22.078046798706055, \t Total Dis Loss : 0.00021597821614705026\n",
      "Steps : 88400, \t Total Gen Loss : 26.316829681396484, \t Total Dis Loss : 0.00012847989273723215\n",
      "Steps : 88500, \t Total Gen Loss : 26.74368667602539, \t Total Dis Loss : 0.00016566032718401402\n",
      "Steps : 88600, \t Total Gen Loss : 25.61897087097168, \t Total Dis Loss : 0.0002881996042560786\n",
      "Steps : 88700, \t Total Gen Loss : 26.933204650878906, \t Total Dis Loss : 0.00010418772581033409\n",
      "Steps : 88800, \t Total Gen Loss : 27.228044509887695, \t Total Dis Loss : 0.00020158688130322844\n",
      "Steps : 88900, \t Total Gen Loss : 27.929447174072266, \t Total Dis Loss : 0.0001505185355199501\n",
      "Steps : 89000, \t Total Gen Loss : 27.541637420654297, \t Total Dis Loss : 0.0004307776689529419\n",
      "Steps : 89100, \t Total Gen Loss : 26.876527786254883, \t Total Dis Loss : 7.38778617233038e-05\n",
      "Steps : 89200, \t Total Gen Loss : 26.175315856933594, \t Total Dis Loss : 0.00015143098426051438\n",
      "Steps : 89300, \t Total Gen Loss : 28.50992202758789, \t Total Dis Loss : 0.0002178785507567227\n",
      "Steps : 89400, \t Total Gen Loss : 28.316333770751953, \t Total Dis Loss : 5.934315413469449e-05\n",
      "Steps : 89500, \t Total Gen Loss : 28.136808395385742, \t Total Dis Loss : 7.462042412953451e-05\n",
      "Steps : 89600, \t Total Gen Loss : 30.328567504882812, \t Total Dis Loss : 6.238396599655971e-05\n",
      "Steps : 89700, \t Total Gen Loss : 28.455841064453125, \t Total Dis Loss : 5.5596119636902586e-05\n",
      "Steps : 89800, \t Total Gen Loss : 29.589000701904297, \t Total Dis Loss : 2.883297929656692e-05\n",
      "Steps : 89900, \t Total Gen Loss : 22.427322387695312, \t Total Dis Loss : 1.0291191339492798\n",
      "Steps : 90000, \t Total Gen Loss : 27.287616729736328, \t Total Dis Loss : 0.0008348487317562103\n",
      "Time for epoch 16 is 71.95956802368164 sec\n",
      "Steps : 90100, \t Total Gen Loss : 29.81753921508789, \t Total Dis Loss : 0.00010307278716936707\n",
      "Steps : 90200, \t Total Gen Loss : 25.585670471191406, \t Total Dis Loss : 0.0025863887276500463\n",
      "Steps : 90300, \t Total Gen Loss : 27.75460433959961, \t Total Dis Loss : 0.00032282411120831966\n",
      "Steps : 90400, \t Total Gen Loss : 28.00728988647461, \t Total Dis Loss : 0.00015142236952669919\n",
      "Steps : 90500, \t Total Gen Loss : 26.519550323486328, \t Total Dis Loss : 0.00045350193977355957\n",
      "Steps : 90600, \t Total Gen Loss : 25.533714294433594, \t Total Dis Loss : 0.00012653347221203148\n",
      "Steps : 90700, \t Total Gen Loss : 29.77267837524414, \t Total Dis Loss : 0.00015914894174784422\n",
      "Steps : 90800, \t Total Gen Loss : 28.288244247436523, \t Total Dis Loss : 0.00018528837244957685\n",
      "Steps : 90900, \t Total Gen Loss : 27.270551681518555, \t Total Dis Loss : 0.00030548899667337537\n",
      "Steps : 91000, \t Total Gen Loss : 26.478574752807617, \t Total Dis Loss : 0.00011002617247868329\n",
      "Steps : 91100, \t Total Gen Loss : 24.828651428222656, \t Total Dis Loss : 0.00013122240488883108\n",
      "Steps : 91200, \t Total Gen Loss : 24.317989349365234, \t Total Dis Loss : 8.078665268840268e-05\n",
      "Steps : 91300, \t Total Gen Loss : 26.683448791503906, \t Total Dis Loss : 6.763816782040522e-05\n",
      "Steps : 91400, \t Total Gen Loss : 25.16118812561035, \t Total Dis Loss : 7.5641117291525e-05\n",
      "Steps : 91500, \t Total Gen Loss : 27.231843948364258, \t Total Dis Loss : 0.00010462682257639244\n",
      "Steps : 91600, \t Total Gen Loss : 25.165813446044922, \t Total Dis Loss : 4.019567131763324e-05\n",
      "Steps : 91700, \t Total Gen Loss : 28.35941505432129, \t Total Dis Loss : 0.000289223826257512\n",
      "Steps : 91800, \t Total Gen Loss : 29.72609519958496, \t Total Dis Loss : 6.163752550492063e-05\n",
      "Steps : 91900, \t Total Gen Loss : 29.935680389404297, \t Total Dis Loss : 0.00013449156540445983\n",
      "Steps : 92000, \t Total Gen Loss : 34.16189193725586, \t Total Dis Loss : 0.0003258513461332768\n",
      "Steps : 92100, \t Total Gen Loss : 32.11747741699219, \t Total Dis Loss : 0.0022449009120464325\n",
      "Steps : 92200, \t Total Gen Loss : 31.98827362060547, \t Total Dis Loss : 3.269950320827775e-05\n",
      "Steps : 92300, \t Total Gen Loss : 28.457199096679688, \t Total Dis Loss : 0.000613207696005702\n",
      "Steps : 92400, \t Total Gen Loss : 27.84832000732422, \t Total Dis Loss : 0.00011762821668526158\n",
      "Steps : 92500, \t Total Gen Loss : 28.532108306884766, \t Total Dis Loss : 4.1048519051400945e-05\n",
      "Steps : 92600, \t Total Gen Loss : 30.82762336730957, \t Total Dis Loss : 0.0001288119819946587\n",
      "Steps : 92700, \t Total Gen Loss : 28.22552490234375, \t Total Dis Loss : 0.0003324642311781645\n",
      "Steps : 92800, \t Total Gen Loss : 25.03200340270996, \t Total Dis Loss : 6.844408198958263e-05\n",
      "Steps : 92900, \t Total Gen Loss : 25.80131721496582, \t Total Dis Loss : 3.9518356061307713e-05\n",
      "Steps : 93000, \t Total Gen Loss : 31.365989685058594, \t Total Dis Loss : 4.103815808775835e-05\n",
      "Steps : 93100, \t Total Gen Loss : 22.647502899169922, \t Total Dis Loss : 0.002516684588044882\n",
      "Steps : 93200, \t Total Gen Loss : 25.57764434814453, \t Total Dis Loss : 0.0001790798269212246\n",
      "Steps : 93300, \t Total Gen Loss : 25.004554748535156, \t Total Dis Loss : 0.00017902112449519336\n",
      "Steps : 93400, \t Total Gen Loss : 22.026371002197266, \t Total Dis Loss : 0.0023854412138462067\n",
      "Steps : 93500, \t Total Gen Loss : 29.4820613861084, \t Total Dis Loss : 0.0004071742296218872\n",
      "Steps : 93600, \t Total Gen Loss : 25.73037338256836, \t Total Dis Loss : 0.00026857617194764316\n",
      "Steps : 93700, \t Total Gen Loss : 22.936094284057617, \t Total Dis Loss : 0.0006638779304921627\n",
      "Steps : 93800, \t Total Gen Loss : 27.34943199157715, \t Total Dis Loss : 0.00023553693608846515\n",
      "Steps : 93900, \t Total Gen Loss : 27.2053165435791, \t Total Dis Loss : 0.0002964558079838753\n",
      "Steps : 94000, \t Total Gen Loss : 26.71887969970703, \t Total Dis Loss : 8.679729944560677e-05\n",
      "Steps : 94100, \t Total Gen Loss : 27.542842864990234, \t Total Dis Loss : 0.00014617436681874096\n",
      "Steps : 94200, \t Total Gen Loss : 27.821195602416992, \t Total Dis Loss : 4.409056418808177e-05\n",
      "Steps : 94300, \t Total Gen Loss : 26.917415618896484, \t Total Dis Loss : 5.378218702389859e-05\n",
      "Steps : 94400, \t Total Gen Loss : 25.50448989868164, \t Total Dis Loss : 5.319573756423779e-05\n",
      "Steps : 94500, \t Total Gen Loss : 28.04378890991211, \t Total Dis Loss : 3.8558424421353266e-05\n",
      "Steps : 94600, \t Total Gen Loss : 31.591176986694336, \t Total Dis Loss : 1.1148711564601399e-05\n",
      "Steps : 94700, \t Total Gen Loss : 23.85771942138672, \t Total Dis Loss : 0.0003904601326212287\n",
      "Steps : 94800, \t Total Gen Loss : 26.212905883789062, \t Total Dis Loss : 0.0003139604814350605\n",
      "Steps : 94900, \t Total Gen Loss : 26.879440307617188, \t Total Dis Loss : 0.00025095674209296703\n",
      "Steps : 95000, \t Total Gen Loss : 29.931228637695312, \t Total Dis Loss : 7.050600834190845e-05\n",
      "Steps : 95100, \t Total Gen Loss : 26.692455291748047, \t Total Dis Loss : 0.0003625011595431715\n",
      "Steps : 95200, \t Total Gen Loss : 26.433502197265625, \t Total Dis Loss : 0.00031202062382362783\n",
      "Steps : 95300, \t Total Gen Loss : 25.839954376220703, \t Total Dis Loss : 4.6615554310847074e-05\n",
      "Steps : 95400, \t Total Gen Loss : 30.0074520111084, \t Total Dis Loss : 0.00012617056199815124\n",
      "Steps : 95500, \t Total Gen Loss : 31.031984329223633, \t Total Dis Loss : 2.1823569113621488e-05\n",
      "Steps : 95600, \t Total Gen Loss : 27.332429885864258, \t Total Dis Loss : 6.230897270143032e-05\n",
      "Time for epoch 17 is 71.94007754325867 sec\n",
      "Steps : 95700, \t Total Gen Loss : 27.48989486694336, \t Total Dis Loss : 4.191646075923927e-05\n",
      "Steps : 95800, \t Total Gen Loss : 34.75046920776367, \t Total Dis Loss : 0.00024269412097055465\n",
      "Steps : 95900, \t Total Gen Loss : 28.227752685546875, \t Total Dis Loss : 0.00021915505931247026\n",
      "Steps : 96000, \t Total Gen Loss : 28.764358520507812, \t Total Dis Loss : 5.4388863645726815e-05\n",
      "Steps : 96100, \t Total Gen Loss : 29.844242095947266, \t Total Dis Loss : 7.76294618844986e-05\n",
      "Steps : 96200, \t Total Gen Loss : 30.75933837890625, \t Total Dis Loss : 0.00011582252045627683\n",
      "Steps : 96300, \t Total Gen Loss : 28.544885635375977, \t Total Dis Loss : 4.464371522772126e-05\n",
      "Steps : 96400, \t Total Gen Loss : 28.37257957458496, \t Total Dis Loss : 2.739994670264423e-05\n",
      "Steps : 96500, \t Total Gen Loss : 34.30799865722656, \t Total Dis Loss : 0.0004900491330772638\n",
      "Steps : 96600, \t Total Gen Loss : 34.186256408691406, \t Total Dis Loss : 4.212640487821773e-05\n",
      "Steps : 96700, \t Total Gen Loss : 29.419462203979492, \t Total Dis Loss : 4.918601189274341e-05\n",
      "Steps : 96800, \t Total Gen Loss : 32.68732452392578, \t Total Dis Loss : 0.0019177451031282544\n",
      "Steps : 96900, \t Total Gen Loss : 33.54973602294922, \t Total Dis Loss : 8.544499723939225e-05\n",
      "Steps : 97000, \t Total Gen Loss : 34.57613754272461, \t Total Dis Loss : 0.00012146912922617048\n",
      "Steps : 97100, \t Total Gen Loss : 29.169485092163086, \t Total Dis Loss : 4.685739986598492e-05\n",
      "Steps : 97200, \t Total Gen Loss : 28.957128524780273, \t Total Dis Loss : 2.450907413731329e-05\n",
      "Steps : 97300, \t Total Gen Loss : 24.015159606933594, \t Total Dis Loss : 0.00035303743788972497\n",
      "Steps : 97400, \t Total Gen Loss : 27.378326416015625, \t Total Dis Loss : 7.104167889337987e-05\n",
      "Steps : 97500, \t Total Gen Loss : 28.357261657714844, \t Total Dis Loss : 8.854391489876434e-05\n",
      "Steps : 97600, \t Total Gen Loss : 24.04126739501953, \t Total Dis Loss : 0.00029723302577622235\n",
      "Steps : 97700, \t Total Gen Loss : 26.943920135498047, \t Total Dis Loss : 4.0304646972799674e-05\n",
      "Steps : 97800, \t Total Gen Loss : 28.928638458251953, \t Total Dis Loss : 3.316978472867049e-05\n",
      "Steps : 97900, \t Total Gen Loss : 26.47233009338379, \t Total Dis Loss : 9.13829353521578e-05\n",
      "Steps : 98000, \t Total Gen Loss : 28.814590454101562, \t Total Dis Loss : 4.226134478813037e-05\n",
      "Steps : 98100, \t Total Gen Loss : 29.83502197265625, \t Total Dis Loss : 3.367174576851539e-05\n",
      "Steps : 98200, \t Total Gen Loss : 30.7572021484375, \t Total Dis Loss : 2.5539538910379633e-05\n",
      "Steps : 98300, \t Total Gen Loss : 29.011672973632812, \t Total Dis Loss : 3.0445156880887225e-05\n",
      "Steps : 98400, \t Total Gen Loss : 27.889333724975586, \t Total Dis Loss : 1.4532402019540314e-05\n",
      "Steps : 98500, \t Total Gen Loss : 26.956037521362305, \t Total Dis Loss : 3.926916542695835e-05\n",
      "Steps : 98600, \t Total Gen Loss : 29.307485580444336, \t Total Dis Loss : 6.067418144084513e-05\n",
      "Steps : 98700, \t Total Gen Loss : 30.786422729492188, \t Total Dis Loss : 4.878304389421828e-05\n",
      "Steps : 98800, \t Total Gen Loss : 29.675151824951172, \t Total Dis Loss : 0.00011585316678974777\n",
      "Steps : 98900, \t Total Gen Loss : 29.00260353088379, \t Total Dis Loss : 2.1968811779515818e-05\n",
      "Steps : 99000, \t Total Gen Loss : 34.287784576416016, \t Total Dis Loss : 3.341801493661478e-05\n",
      "Steps : 99100, \t Total Gen Loss : 28.44126319885254, \t Total Dis Loss : 3.098193337791599e-05\n",
      "Steps : 99200, \t Total Gen Loss : 27.61241340637207, \t Total Dis Loss : 0.00014944958093110472\n",
      "Steps : 99300, \t Total Gen Loss : 30.3421630859375, \t Total Dis Loss : 1.515436997578945e-05\n",
      "Steps : 99400, \t Total Gen Loss : 24.88933753967285, \t Total Dis Loss : 1.7762857169145718e-05\n",
      "Steps : 99500, \t Total Gen Loss : 25.32590103149414, \t Total Dis Loss : 3.5239310818724334e-05\n",
      "Steps : 99600, \t Total Gen Loss : 25.861831665039062, \t Total Dis Loss : 3.7901350879110396e-05\n",
      "Steps : 99700, \t Total Gen Loss : 30.362564086914062, \t Total Dis Loss : 1.9002864064532332e-05\n",
      "Steps : 99800, \t Total Gen Loss : 27.64411163330078, \t Total Dis Loss : 1.7639840734773315e-05\n",
      "Steps : 99900, \t Total Gen Loss : 27.44482421875, \t Total Dis Loss : 8.833540778141469e-05\n",
      "Steps : 100000, \t Total Gen Loss : 27.746980667114258, \t Total Dis Loss : 0.0003867780033033341\n",
      "Steps : 100100, \t Total Gen Loss : 24.94364356994629, \t Total Dis Loss : 1.9245637304265983e-05\n",
      "Steps : 100200, \t Total Gen Loss : 26.404850006103516, \t Total Dis Loss : 1.1293753232166637e-05\n",
      "Steps : 100300, \t Total Gen Loss : 27.347652435302734, \t Total Dis Loss : 2.0977820895495825e-05\n",
      "Steps : 100400, \t Total Gen Loss : 29.712844848632812, \t Total Dis Loss : 7.503152301069349e-05\n",
      "Steps : 100500, \t Total Gen Loss : 31.06686019897461, \t Total Dis Loss : 8.643111868877895e-06\n",
      "Steps : 100600, \t Total Gen Loss : 27.121551513671875, \t Total Dis Loss : 4.410744168126257e-06\n",
      "Steps : 100700, \t Total Gen Loss : 26.760643005371094, \t Total Dis Loss : 0.0003951151738874614\n",
      "Steps : 100800, \t Total Gen Loss : 29.122203826904297, \t Total Dis Loss : 2.21474765567109e-05\n",
      "Steps : 100900, \t Total Gen Loss : 27.312213897705078, \t Total Dis Loss : 0.00012657763727474958\n",
      "Steps : 101000, \t Total Gen Loss : 32.224456787109375, \t Total Dis Loss : 0.00014628915232606232\n",
      "Steps : 101100, \t Total Gen Loss : 28.41632843017578, \t Total Dis Loss : 8.91388626769185e-05\n",
      "Steps : 101200, \t Total Gen Loss : 28.222320556640625, \t Total Dis Loss : 0.00010645820293575525\n",
      "Time for epoch 18 is 71.89549040794373 sec\n",
      "Steps : 101300, \t Total Gen Loss : 29.45114517211914, \t Total Dis Loss : 2.3085509383236058e-05\n",
      "Steps : 101400, \t Total Gen Loss : 31.82303237915039, \t Total Dis Loss : 0.00033439547405578196\n",
      "Steps : 101500, \t Total Gen Loss : 34.13418960571289, \t Total Dis Loss : 0.0003455256810411811\n",
      "Steps : 101600, \t Total Gen Loss : 32.18252944946289, \t Total Dis Loss : 1.643838913878426e-05\n",
      "Steps : 101700, \t Total Gen Loss : 36.26155090332031, \t Total Dis Loss : 1.0541974916122854e-05\n",
      "Steps : 101800, \t Total Gen Loss : 29.386104583740234, \t Total Dis Loss : 5.826015240018023e-06\n",
      "Steps : 101900, \t Total Gen Loss : 29.63718032836914, \t Total Dis Loss : 5.24460074302624e-06\n",
      "Steps : 102000, \t Total Gen Loss : 29.117618560791016, \t Total Dis Loss : 1.424803940608399e-05\n",
      "Steps : 102100, \t Total Gen Loss : 28.050914764404297, \t Total Dis Loss : 2.494937325536739e-05\n",
      "Steps : 102200, \t Total Gen Loss : 29.841541290283203, \t Total Dis Loss : 4.881835775449872e-05\n",
      "Steps : 102300, \t Total Gen Loss : 30.66613006591797, \t Total Dis Loss : 7.355608977377415e-05\n",
      "Steps : 102400, \t Total Gen Loss : 29.298965454101562, \t Total Dis Loss : 1.1700265531544574e-05\n",
      "Steps : 102500, \t Total Gen Loss : 30.40646743774414, \t Total Dis Loss : 1.4849446415610146e-05\n",
      "Steps : 102600, \t Total Gen Loss : 26.28900718688965, \t Total Dis Loss : 0.00010690122871892527\n",
      "Steps : 102700, \t Total Gen Loss : 29.083581924438477, \t Total Dis Loss : 1.6659891116432846e-05\n",
      "Steps : 102800, \t Total Gen Loss : 30.41066551208496, \t Total Dis Loss : 0.0001137975777965039\n",
      "Steps : 102900, \t Total Gen Loss : 23.906028747558594, \t Total Dis Loss : 2.5540115530020557e-05\n",
      "Steps : 103000, \t Total Gen Loss : 27.092052459716797, \t Total Dis Loss : 0.0004697136173490435\n",
      "Steps : 103100, \t Total Gen Loss : 25.718894958496094, \t Total Dis Loss : 0.0005860567325726151\n",
      "Steps : 103200, \t Total Gen Loss : 25.891870498657227, \t Total Dis Loss : 5.8261077356291935e-05\n",
      "Steps : 103300, \t Total Gen Loss : 27.032024383544922, \t Total Dis Loss : 2.8104685043217614e-05\n",
      "Steps : 103400, \t Total Gen Loss : 29.076709747314453, \t Total Dis Loss : 2.105518797179684e-05\n",
      "Steps : 103500, \t Total Gen Loss : 25.611698150634766, \t Total Dis Loss : 7.040669879643247e-05\n",
      "Steps : 103600, \t Total Gen Loss : 27.977466583251953, \t Total Dis Loss : 0.00022398348664864898\n",
      "Steps : 103700, \t Total Gen Loss : 26.91275405883789, \t Total Dis Loss : 2.9507022190955468e-05\n",
      "Steps : 103800, \t Total Gen Loss : 26.394834518432617, \t Total Dis Loss : 4.166240250924602e-05\n",
      "Steps : 103900, \t Total Gen Loss : 26.57942771911621, \t Total Dis Loss : 4.029011688544415e-05\n",
      "Steps : 104000, \t Total Gen Loss : 28.85028839111328, \t Total Dis Loss : 2.482075615262147e-05\n",
      "Steps : 104100, \t Total Gen Loss : 27.444087982177734, \t Total Dis Loss : 1.6526873878319748e-05\n",
      "Steps : 104200, \t Total Gen Loss : 30.838476181030273, \t Total Dis Loss : 1.6291727661155164e-05\n",
      "Steps : 104300, \t Total Gen Loss : 28.733610153198242, \t Total Dis Loss : 0.00011376395559636876\n",
      "Steps : 104400, \t Total Gen Loss : 25.26428985595703, \t Total Dis Loss : 5.639460141537711e-05\n",
      "Steps : 104500, \t Total Gen Loss : 30.771656036376953, \t Total Dis Loss : 7.571109017590061e-05\n",
      "Steps : 104600, \t Total Gen Loss : 28.23965835571289, \t Total Dis Loss : 0.00016848644008859992\n",
      "Steps : 104700, \t Total Gen Loss : 29.517292022705078, \t Total Dis Loss : 3.460946390987374e-05\n",
      "Steps : 104800, \t Total Gen Loss : 31.121763229370117, \t Total Dis Loss : 0.00013700046110898256\n",
      "Steps : 104900, \t Total Gen Loss : 26.98617172241211, \t Total Dis Loss : 0.00025044145877473056\n",
      "Steps : 105000, \t Total Gen Loss : 26.49811363220215, \t Total Dis Loss : 0.000224131130380556\n",
      "Steps : 105100, \t Total Gen Loss : 25.121212005615234, \t Total Dis Loss : 0.00014418929640669376\n",
      "Steps : 105200, \t Total Gen Loss : 28.817291259765625, \t Total Dis Loss : 4.175425419816747e-05\n",
      "Steps : 105300, \t Total Gen Loss : 26.77825164794922, \t Total Dis Loss : 2.461825351929292e-05\n",
      "Steps : 105400, \t Total Gen Loss : 27.19801139831543, \t Total Dis Loss : 6.738175579812378e-05\n",
      "Steps : 105500, \t Total Gen Loss : 28.856307983398438, \t Total Dis Loss : 0.0001026084428303875\n",
      "Steps : 105600, \t Total Gen Loss : 25.501693725585938, \t Total Dis Loss : 1.9775521650444716e-05\n",
      "Steps : 105700, \t Total Gen Loss : 28.07985496520996, \t Total Dis Loss : 4.2699033656390384e-05\n",
      "Steps : 105800, \t Total Gen Loss : 25.17812728881836, \t Total Dis Loss : 0.00021330686286091805\n",
      "Steps : 105900, \t Total Gen Loss : 24.211196899414062, \t Total Dis Loss : 0.0005112569779157639\n",
      "Steps : 106000, \t Total Gen Loss : 27.66619873046875, \t Total Dis Loss : 0.0002536574611440301\n",
      "Steps : 106100, \t Total Gen Loss : 26.512144088745117, \t Total Dis Loss : 3.805465530604124e-05\n",
      "Steps : 106200, \t Total Gen Loss : 24.061641693115234, \t Total Dis Loss : 0.0002347463887417689\n",
      "Steps : 106300, \t Total Gen Loss : 25.347850799560547, \t Total Dis Loss : 0.00014882830146234483\n",
      "Steps : 106400, \t Total Gen Loss : 28.580440521240234, \t Total Dis Loss : 6.12817020737566e-05\n",
      "Steps : 106500, \t Total Gen Loss : 25.777324676513672, \t Total Dis Loss : 0.00015768657613079995\n",
      "Steps : 106600, \t Total Gen Loss : 23.953617095947266, \t Total Dis Loss : 0.0006885117036290467\n",
      "Steps : 106700, \t Total Gen Loss : 24.520782470703125, \t Total Dis Loss : 0.0001877031900221482\n",
      "Steps : 106800, \t Total Gen Loss : 29.03365707397461, \t Total Dis Loss : 4.393341805553064e-05\n",
      "Time for epoch 19 is 71.94031929969788 sec\n",
      "Steps : 106900, \t Total Gen Loss : 27.494216918945312, \t Total Dis Loss : 6.703393592033535e-05\n",
      "Steps : 107000, \t Total Gen Loss : 26.03057861328125, \t Total Dis Loss : 0.00020031348685733974\n",
      "Steps : 107100, \t Total Gen Loss : 26.068477630615234, \t Total Dis Loss : 5.1612827519420534e-05\n",
      "Steps : 107200, \t Total Gen Loss : 24.85172462463379, \t Total Dis Loss : 0.0001150175157818012\n",
      "Steps : 107300, \t Total Gen Loss : 24.05763053894043, \t Total Dis Loss : 9.245782712241635e-05\n",
      "Steps : 107400, \t Total Gen Loss : 25.245311737060547, \t Total Dis Loss : 8.900866669137031e-05\n",
      "Steps : 107500, \t Total Gen Loss : 25.966825485229492, \t Total Dis Loss : 3.088658559136093e-05\n",
      "Steps : 107600, \t Total Gen Loss : 25.06985092163086, \t Total Dis Loss : 5.288799729896709e-05\n",
      "Steps : 107700, \t Total Gen Loss : 30.381986618041992, \t Total Dis Loss : 4.250873826094903e-05\n",
      "Steps : 107800, \t Total Gen Loss : 25.173702239990234, \t Total Dis Loss : 0.00017218015273101628\n",
      "Steps : 107900, \t Total Gen Loss : 25.491880416870117, \t Total Dis Loss : 0.00011384524987079203\n",
      "Steps : 108000, \t Total Gen Loss : 24.3690185546875, \t Total Dis Loss : 0.0002834693295881152\n",
      "Steps : 108100, \t Total Gen Loss : 24.103534698486328, \t Total Dis Loss : 9.997116285376251e-05\n",
      "Steps : 108200, \t Total Gen Loss : 25.68377113342285, \t Total Dis Loss : 0.0004062615626025945\n",
      "Steps : 108300, \t Total Gen Loss : 24.488140106201172, \t Total Dis Loss : 0.0001654409134062007\n",
      "Steps : 108400, \t Total Gen Loss : 27.09750747680664, \t Total Dis Loss : 3.547115557012148e-05\n",
      "Steps : 108500, \t Total Gen Loss : 30.102027893066406, \t Total Dis Loss : 8.283197530545294e-06\n",
      "Steps : 108600, \t Total Gen Loss : 24.743295669555664, \t Total Dis Loss : 0.008158004842698574\n",
      "Steps : 108700, \t Total Gen Loss : 27.369338989257812, \t Total Dis Loss : 0.0007164161070249975\n",
      "Steps : 108800, \t Total Gen Loss : 27.178302764892578, \t Total Dis Loss : 8.713755960343406e-05\n",
      "Steps : 108900, \t Total Gen Loss : 27.278038024902344, \t Total Dis Loss : 0.000140686723170802\n",
      "Steps : 109000, \t Total Gen Loss : 26.876731872558594, \t Total Dis Loss : 0.00022362949675880373\n",
      "Steps : 109100, \t Total Gen Loss : 25.880874633789062, \t Total Dis Loss : 9.954378037946299e-05\n",
      "Steps : 109200, \t Total Gen Loss : 25.549644470214844, \t Total Dis Loss : 4.957609053235501e-05\n",
      "Steps : 109300, \t Total Gen Loss : 28.75640106201172, \t Total Dis Loss : 9.855004464043304e-05\n",
      "Steps : 109400, \t Total Gen Loss : 22.471969604492188, \t Total Dis Loss : 0.0009172017453238368\n",
      "Steps : 109500, \t Total Gen Loss : 24.367198944091797, \t Total Dis Loss : 0.0002153085224563256\n",
      "Steps : 109600, \t Total Gen Loss : 27.916196823120117, \t Total Dis Loss : 0.00033203052589669824\n",
      "Steps : 109700, \t Total Gen Loss : 27.62799644470215, \t Total Dis Loss : 5.458104351419024e-05\n",
      "Steps : 109800, \t Total Gen Loss : 24.710205078125, \t Total Dis Loss : 0.003020690521225333\n",
      "Steps : 109900, \t Total Gen Loss : 28.66097640991211, \t Total Dis Loss : 0.00011611197260208428\n",
      "Steps : 110000, \t Total Gen Loss : 25.420276641845703, \t Total Dis Loss : 0.00041985471034422517\n",
      "Steps : 110100, \t Total Gen Loss : 30.527450561523438, \t Total Dis Loss : 6.792988278903067e-05\n",
      "Steps : 110200, \t Total Gen Loss : 24.568592071533203, \t Total Dis Loss : 0.00019388212240301073\n",
      "Steps : 110300, \t Total Gen Loss : 27.011484146118164, \t Total Dis Loss : 7.24883284419775e-05\n",
      "Steps : 110400, \t Total Gen Loss : 24.493379592895508, \t Total Dis Loss : 3.5896133340429515e-05\n",
      "Steps : 110500, \t Total Gen Loss : 22.75349235534668, \t Total Dis Loss : 0.00044707278721034527\n",
      "Steps : 110600, \t Total Gen Loss : 25.30916404724121, \t Total Dis Loss : 0.00012431124923750758\n",
      "Steps : 110700, \t Total Gen Loss : 24.295421600341797, \t Total Dis Loss : 0.0003270886663813144\n",
      "Steps : 110800, \t Total Gen Loss : 22.569236755371094, \t Total Dis Loss : 0.00019814686675090343\n",
      "Steps : 110900, \t Total Gen Loss : 24.71042251586914, \t Total Dis Loss : 0.0003621743235271424\n",
      "Steps : 111000, \t Total Gen Loss : 27.64061164855957, \t Total Dis Loss : 0.0001482261868659407\n",
      "Steps : 111100, \t Total Gen Loss : 27.91149139404297, \t Total Dis Loss : 6.763239798601717e-05\n",
      "Steps : 111200, \t Total Gen Loss : 24.09031105041504, \t Total Dis Loss : 4.433239155332558e-05\n",
      "Steps : 111300, \t Total Gen Loss : 24.880475997924805, \t Total Dis Loss : 2.5979757992899977e-05\n",
      "Steps : 111400, \t Total Gen Loss : 25.89535903930664, \t Total Dis Loss : 0.00032714218832552433\n",
      "Steps : 111500, \t Total Gen Loss : 23.933277130126953, \t Total Dis Loss : 0.001126613700762391\n",
      "Steps : 111600, \t Total Gen Loss : 28.775049209594727, \t Total Dis Loss : 0.00018660194473341107\n",
      "Steps : 111700, \t Total Gen Loss : 25.21728515625, \t Total Dis Loss : 0.00047853836440481246\n",
      "Steps : 111800, \t Total Gen Loss : 24.029296875, \t Total Dis Loss : 0.00014849354920443147\n",
      "Steps : 111900, \t Total Gen Loss : 25.906408309936523, \t Total Dis Loss : 5.41945883014705e-05\n",
      "Steps : 112000, \t Total Gen Loss : 25.125728607177734, \t Total Dis Loss : 0.0001066985132638365\n",
      "Steps : 112100, \t Total Gen Loss : 24.567325592041016, \t Total Dis Loss : 4.505219476413913e-05\n",
      "Steps : 112200, \t Total Gen Loss : 27.015304565429688, \t Total Dis Loss : 0.0007784292101860046\n",
      "Steps : 112300, \t Total Gen Loss : 26.40700912475586, \t Total Dis Loss : 3.687680509756319e-05\n",
      "Steps : 112400, \t Total Gen Loss : 27.382448196411133, \t Total Dis Loss : 5.442664769361727e-05\n",
      "Steps : 112500, \t Total Gen Loss : 29.037782669067383, \t Total Dis Loss : 2.6430085199535824e-05\n",
      "Time for epoch 20 is 72.09693813323975 sec\n",
      "Steps : 112600, \t Total Gen Loss : 25.07056999206543, \t Total Dis Loss : 0.00021657755132764578\n",
      "Steps : 112700, \t Total Gen Loss : 26.306095123291016, \t Total Dis Loss : 0.00010263427247991785\n",
      "Steps : 112800, \t Total Gen Loss : 29.63408660888672, \t Total Dis Loss : 0.00010689062037272379\n",
      "Steps : 112900, \t Total Gen Loss : 24.65715789794922, \t Total Dis Loss : 0.0006552591803483665\n",
      "Steps : 113000, \t Total Gen Loss : 28.566499710083008, \t Total Dis Loss : 0.00015706669364590198\n",
      "Steps : 113100, \t Total Gen Loss : 25.616186141967773, \t Total Dis Loss : 0.0003038814174942672\n",
      "Steps : 113200, \t Total Gen Loss : 25.49862289428711, \t Total Dis Loss : 0.00012462471204344183\n",
      "Steps : 113300, \t Total Gen Loss : 28.71002197265625, \t Total Dis Loss : 0.00032947794534265995\n",
      "Steps : 113400, \t Total Gen Loss : 34.3238525390625, \t Total Dis Loss : 1.9181056813977193e-06\n",
      "Steps : 113500, \t Total Gen Loss : 30.28183364868164, \t Total Dis Loss : 2.793442035908811e-06\n",
      "Steps : 113600, \t Total Gen Loss : 28.31076431274414, \t Total Dis Loss : 0.00018525503401178867\n",
      "Steps : 113700, \t Total Gen Loss : 27.92017364501953, \t Total Dis Loss : 1.5029860151116736e-05\n",
      "Steps : 113800, \t Total Gen Loss : 26.869361877441406, \t Total Dis Loss : 3.26415611198172e-05\n",
      "Steps : 113900, \t Total Gen Loss : 29.377662658691406, \t Total Dis Loss : 5.460234660858987e-06\n",
      "Steps : 114000, \t Total Gen Loss : 28.84768295288086, \t Total Dis Loss : 1.4201355952536687e-05\n",
      "Steps : 114100, \t Total Gen Loss : 27.541549682617188, \t Total Dis Loss : 0.00011583611194510013\n",
      "Steps : 114200, \t Total Gen Loss : 32.02854919433594, \t Total Dis Loss : 7.084839580784319e-06\n",
      "Steps : 114300, \t Total Gen Loss : 32.638702392578125, \t Total Dis Loss : 9.298288205172867e-06\n",
      "Steps : 114400, \t Total Gen Loss : 27.316831588745117, \t Total Dis Loss : 8.011675527086481e-05\n",
      "Steps : 114500, \t Total Gen Loss : 31.11155891418457, \t Total Dis Loss : 1.653276922297664e-05\n",
      "Steps : 114600, \t Total Gen Loss : 28.499547958374023, \t Total Dis Loss : 4.476152753341012e-05\n",
      "Steps : 114700, \t Total Gen Loss : 26.630273818969727, \t Total Dis Loss : 1.2761242032865994e-05\n",
      "Steps : 114800, \t Total Gen Loss : 25.024173736572266, \t Total Dis Loss : 0.00021201038907747716\n",
      "Steps : 114900, \t Total Gen Loss : 32.67897415161133, \t Total Dis Loss : 2.7159526325704064e-06\n",
      "Steps : 115000, \t Total Gen Loss : 28.62009048461914, \t Total Dis Loss : 1.1815569450845942e-05\n",
      "Steps : 115100, \t Total Gen Loss : 31.418407440185547, \t Total Dis Loss : 1.3945777936896775e-05\n",
      "Steps : 115200, \t Total Gen Loss : 30.462154388427734, \t Total Dis Loss : 2.5833325707935728e-05\n",
      "Steps : 115300, \t Total Gen Loss : 27.076854705810547, \t Total Dis Loss : 9.36393735173624e-06\n",
      "Steps : 115400, \t Total Gen Loss : 31.270090103149414, \t Total Dis Loss : 1.1457696928118821e-05\n",
      "Steps : 115500, \t Total Gen Loss : 27.53113555908203, \t Total Dis Loss : 2.7050424250774086e-05\n",
      "Steps : 115600, \t Total Gen Loss : 30.162174224853516, \t Total Dis Loss : 2.4629565814393573e-05\n",
      "Steps : 115700, \t Total Gen Loss : 29.535856246948242, \t Total Dis Loss : 1.7032629330060445e-05\n",
      "Steps : 115800, \t Total Gen Loss : 28.143840789794922, \t Total Dis Loss : 0.00010333061072742566\n",
      "Steps : 115900, \t Total Gen Loss : 29.372940063476562, \t Total Dis Loss : 7.494611054426059e-06\n",
      "Steps : 116000, \t Total Gen Loss : 26.287757873535156, \t Total Dis Loss : 0.0031838298309594393\n",
      "Steps : 116100, \t Total Gen Loss : 26.347192764282227, \t Total Dis Loss : 7.866963278502226e-05\n",
      "Steps : 116200, \t Total Gen Loss : 28.747154235839844, \t Total Dis Loss : 0.0002731473359744996\n",
      "Steps : 116300, \t Total Gen Loss : 26.357833862304688, \t Total Dis Loss : 0.000231204554438591\n",
      "Steps : 116400, \t Total Gen Loss : 25.86098289489746, \t Total Dis Loss : 6.796277011744678e-05\n",
      "Steps : 116500, \t Total Gen Loss : 25.161327362060547, \t Total Dis Loss : 0.00020571511413436383\n",
      "Steps : 116600, \t Total Gen Loss : 27.4025936126709, \t Total Dis Loss : 8.81563319126144e-05\n",
      "Steps : 116700, \t Total Gen Loss : 26.596378326416016, \t Total Dis Loss : 3.933805419364944e-05\n",
      "Steps : 116800, \t Total Gen Loss : 27.701942443847656, \t Total Dis Loss : 2.4447004761896096e-05\n",
      "Steps : 116900, \t Total Gen Loss : 25.65619468688965, \t Total Dis Loss : 1.655991218285635e-05\n",
      "Steps : 117000, \t Total Gen Loss : 25.863611221313477, \t Total Dis Loss : 0.00020449327712412924\n",
      "Steps : 117100, \t Total Gen Loss : 27.264114379882812, \t Total Dis Loss : 3.590149208321236e-05\n",
      "Steps : 117200, \t Total Gen Loss : 29.307453155517578, \t Total Dis Loss : 7.258297046064399e-06\n",
      "Steps : 117300, \t Total Gen Loss : 27.927921295166016, \t Total Dis Loss : 0.0002579843276180327\n",
      "Steps : 117400, \t Total Gen Loss : 25.3416690826416, \t Total Dis Loss : 5.528928159037605e-05\n",
      "Steps : 117500, \t Total Gen Loss : 27.20490264892578, \t Total Dis Loss : 8.07836331659928e-05\n",
      "Steps : 117600, \t Total Gen Loss : 27.629308700561523, \t Total Dis Loss : 0.00010151010792469606\n",
      "Steps : 117700, \t Total Gen Loss : 31.149105072021484, \t Total Dis Loss : 2.8156657208455727e-05\n",
      "Steps : 117800, \t Total Gen Loss : 24.31210708618164, \t Total Dis Loss : 0.00018058004206977785\n",
      "Steps : 117900, \t Total Gen Loss : 25.757152557373047, \t Total Dis Loss : 0.00038589839823544025\n",
      "Steps : 118000, \t Total Gen Loss : 29.135330200195312, \t Total Dis Loss : 2.983397280331701e-05\n",
      "Steps : 118100, \t Total Gen Loss : 29.021556854248047, \t Total Dis Loss : 1.4624130017182324e-05\n",
      "Time for epoch 21 is 72.01944994926453 sec\n",
      "Steps : 118200, \t Total Gen Loss : 26.53923225402832, \t Total Dis Loss : 6.103579653427005e-05\n",
      "Steps : 118300, \t Total Gen Loss : 25.712583541870117, \t Total Dis Loss : 4.376009383122437e-05\n",
      "Steps : 118400, \t Total Gen Loss : 28.143436431884766, \t Total Dis Loss : 5.206109563005157e-05\n",
      "Steps : 118500, \t Total Gen Loss : 28.91482925415039, \t Total Dis Loss : 4.8910267651081085e-05\n",
      "Steps : 118600, \t Total Gen Loss : 30.61587905883789, \t Total Dis Loss : 0.00017287970695178956\n",
      "Steps : 118700, \t Total Gen Loss : 27.516164779663086, \t Total Dis Loss : 1.529215842310805e-05\n",
      "Steps : 118800, \t Total Gen Loss : 25.91153335571289, \t Total Dis Loss : 8.404131949646398e-05\n",
      "Steps : 118900, \t Total Gen Loss : 27.898895263671875, \t Total Dis Loss : 5.346877878764644e-05\n",
      "Steps : 119000, \t Total Gen Loss : 27.30615997314453, \t Total Dis Loss : 6.585456867469475e-05\n",
      "Steps : 119100, \t Total Gen Loss : 29.597999572753906, \t Total Dis Loss : 7.273536903085187e-05\n",
      "Steps : 119200, \t Total Gen Loss : 24.007186889648438, \t Total Dis Loss : 5.55423139303457e-05\n",
      "Steps : 119300, \t Total Gen Loss : 27.95948028564453, \t Total Dis Loss : 2.0361449060146697e-05\n",
      "Steps : 119400, \t Total Gen Loss : 29.410009384155273, \t Total Dis Loss : 4.975513002136722e-05\n",
      "Steps : 119500, \t Total Gen Loss : 25.317718505859375, \t Total Dis Loss : 2.686994594114367e-05\n",
      "Steps : 119600, \t Total Gen Loss : 30.888612747192383, \t Total Dis Loss : 4.5487489842344075e-05\n",
      "Steps : 119700, \t Total Gen Loss : 26.279624938964844, \t Total Dis Loss : 0.0001605524157639593\n",
      "Steps : 119800, \t Total Gen Loss : 27.34793472290039, \t Total Dis Loss : 6.69872752041556e-05\n",
      "Steps : 119900, \t Total Gen Loss : 29.285778045654297, \t Total Dis Loss : 4.4092324969824404e-05\n",
      "Steps : 120000, \t Total Gen Loss : 26.752029418945312, \t Total Dis Loss : 8.428731234744191e-05\n",
      "Steps : 120100, \t Total Gen Loss : 25.430356979370117, \t Total Dis Loss : 7.814932178007439e-05\n",
      "Steps : 120200, \t Total Gen Loss : 29.811965942382812, \t Total Dis Loss : 0.0001491072034696117\n",
      "Steps : 120300, \t Total Gen Loss : 29.268821716308594, \t Total Dis Loss : 0.00014461342652793974\n",
      "Steps : 120400, \t Total Gen Loss : 28.35671043395996, \t Total Dis Loss : 0.00030374404741451144\n",
      "Steps : 120500, \t Total Gen Loss : 23.829750061035156, \t Total Dis Loss : 0.00035426669637672603\n",
      "Steps : 120600, \t Total Gen Loss : 27.2893123626709, \t Total Dis Loss : 5.7816745538730174e-05\n",
      "Steps : 120700, \t Total Gen Loss : 25.185762405395508, \t Total Dis Loss : 6.319220847217366e-05\n",
      "Steps : 120800, \t Total Gen Loss : 27.002967834472656, \t Total Dis Loss : 9.331878391094506e-05\n",
      "Steps : 120900, \t Total Gen Loss : 28.601913452148438, \t Total Dis Loss : 3.339569957461208e-05\n",
      "Steps : 121000, \t Total Gen Loss : 24.511215209960938, \t Total Dis Loss : 9.876667900243774e-05\n",
      "Steps : 121100, \t Total Gen Loss : 25.80364990234375, \t Total Dis Loss : 5.298450923874043e-05\n",
      "Steps : 121200, \t Total Gen Loss : 24.613109588623047, \t Total Dis Loss : 7.747488416498527e-05\n",
      "Steps : 121300, \t Total Gen Loss : 29.047941207885742, \t Total Dis Loss : 2.4946497433120385e-05\n",
      "Steps : 121400, \t Total Gen Loss : 30.486968994140625, \t Total Dis Loss : 6.655778634012677e-06\n",
      "Steps : 121500, \t Total Gen Loss : 29.519058227539062, \t Total Dis Loss : 7.752491001156159e-06\n",
      "Steps : 121600, \t Total Gen Loss : 31.69350814819336, \t Total Dis Loss : 0.00800190307199955\n",
      "Steps : 121700, \t Total Gen Loss : 28.985122680664062, \t Total Dis Loss : 2.876661892514676e-05\n",
      "Steps : 121800, \t Total Gen Loss : 29.896398544311523, \t Total Dis Loss : 2.297824357810896e-05\n",
      "Steps : 121900, \t Total Gen Loss : 35.81718063354492, \t Total Dis Loss : 9.309210145147517e-05\n",
      "Steps : 122000, \t Total Gen Loss : 34.20600891113281, \t Total Dis Loss : 1.932502163981553e-05\n",
      "Steps : 122100, \t Total Gen Loss : 30.78887939453125, \t Total Dis Loss : 7.75277367210947e-05\n",
      "Steps : 122200, \t Total Gen Loss : 30.15060043334961, \t Total Dis Loss : 7.917968468973413e-05\n",
      "Steps : 122300, \t Total Gen Loss : 27.26543426513672, \t Total Dis Loss : 0.000226544652832672\n",
      "Steps : 122400, \t Total Gen Loss : 26.69107437133789, \t Total Dis Loss : 0.00014875624037813395\n",
      "Steps : 122500, \t Total Gen Loss : 29.687610626220703, \t Total Dis Loss : 8.559972047805786e-05\n",
      "Steps : 122600, \t Total Gen Loss : 29.459083557128906, \t Total Dis Loss : 5.602849705610424e-05\n",
      "Steps : 122700, \t Total Gen Loss : 30.107698440551758, \t Total Dis Loss : 0.0001132054821937345\n",
      "Steps : 122800, \t Total Gen Loss : 30.898902893066406, \t Total Dis Loss : 0.00018178577010985464\n",
      "Steps : 122900, \t Total Gen Loss : 33.41960525512695, \t Total Dis Loss : 7.47655940358527e-05\n",
      "Steps : 123000, \t Total Gen Loss : 33.78436279296875, \t Total Dis Loss : 0.00015212684229481965\n",
      "Steps : 123100, \t Total Gen Loss : 34.089439392089844, \t Total Dis Loss : 5.2137140301056206e-05\n",
      "Steps : 123200, \t Total Gen Loss : 32.016788482666016, \t Total Dis Loss : 4.42018463218119e-05\n",
      "Steps : 123300, \t Total Gen Loss : 33.87752914428711, \t Total Dis Loss : 9.637923358241096e-05\n",
      "Steps : 123400, \t Total Gen Loss : 31.777877807617188, \t Total Dis Loss : 6.615940219489858e-05\n",
      "Steps : 123500, \t Total Gen Loss : 30.161781311035156, \t Total Dis Loss : 0.00023804776719771326\n",
      "Steps : 123600, \t Total Gen Loss : 29.435636520385742, \t Total Dis Loss : 0.00013755539839621633\n",
      "Steps : 123700, \t Total Gen Loss : 31.631820678710938, \t Total Dis Loss : 2.8113181542721577e-05\n",
      "Time for epoch 22 is 72.11863231658936 sec\n",
      "Steps : 123800, \t Total Gen Loss : 32.17383575439453, \t Total Dis Loss : 4.6197914343792945e-05\n",
      "Steps : 123900, \t Total Gen Loss : 26.87925910949707, \t Total Dis Loss : 4.905035166302696e-05\n",
      "Steps : 124000, \t Total Gen Loss : 29.553760528564453, \t Total Dis Loss : 1.3606450011138804e-05\n",
      "Steps : 124100, \t Total Gen Loss : 26.448959350585938, \t Total Dis Loss : 0.0004871648270636797\n",
      "Steps : 124200, \t Total Gen Loss : 26.170772552490234, \t Total Dis Loss : 0.00018862856086343527\n",
      "Steps : 124300, \t Total Gen Loss : 27.44189453125, \t Total Dis Loss : 0.00023371826682705432\n",
      "Steps : 124400, \t Total Gen Loss : 27.396385192871094, \t Total Dis Loss : 2.6506562790018506e-05\n",
      "Steps : 124500, \t Total Gen Loss : 27.08087921142578, \t Total Dis Loss : 4.31691987614613e-05\n",
      "Steps : 124600, \t Total Gen Loss : 29.280969619750977, \t Total Dis Loss : 3.286052742623724e-05\n",
      "Steps : 124700, \t Total Gen Loss : 25.932071685791016, \t Total Dis Loss : 5.5792850616853684e-05\n",
      "Steps : 124800, \t Total Gen Loss : 23.87515640258789, \t Total Dis Loss : 0.0001417683088220656\n",
      "Steps : 124900, \t Total Gen Loss : 26.919233322143555, \t Total Dis Loss : 7.374781125690788e-05\n",
      "Steps : 125000, \t Total Gen Loss : 26.847393035888672, \t Total Dis Loss : 0.00012592619168572128\n",
      "Steps : 125100, \t Total Gen Loss : 27.180923461914062, \t Total Dis Loss : 7.373604603344575e-05\n",
      "Steps : 125200, \t Total Gen Loss : 28.77825164794922, \t Total Dis Loss : 5.204401895753108e-05\n",
      "Steps : 125300, \t Total Gen Loss : 32.24501037597656, \t Total Dis Loss : 5.6922988733276725e-05\n",
      "Steps : 125400, \t Total Gen Loss : 25.936721801757812, \t Total Dis Loss : 5.371760198613629e-05\n",
      "Steps : 125500, \t Total Gen Loss : 28.47361946105957, \t Total Dis Loss : 6.428424967452884e-05\n",
      "Steps : 125600, \t Total Gen Loss : 31.925086975097656, \t Total Dis Loss : 0.0007905648089945316\n",
      "Steps : 125700, \t Total Gen Loss : 27.696125030517578, \t Total Dis Loss : 5.5098225857364014e-05\n",
      "Steps : 125800, \t Total Gen Loss : 25.355649948120117, \t Total Dis Loss : 5.529249756364152e-05\n",
      "Steps : 125900, \t Total Gen Loss : 28.31229591369629, \t Total Dis Loss : 5.804598913528025e-05\n",
      "Steps : 126000, \t Total Gen Loss : 26.366512298583984, \t Total Dis Loss : 1.7969847249332815e-05\n",
      "Steps : 126100, \t Total Gen Loss : 25.719318389892578, \t Total Dis Loss : 0.0002472217020113021\n",
      "Steps : 126200, \t Total Gen Loss : 25.709495544433594, \t Total Dis Loss : 0.00025292913778685033\n",
      "Steps : 126300, \t Total Gen Loss : 29.029390335083008, \t Total Dis Loss : 0.009810040704905987\n",
      "Steps : 126400, \t Total Gen Loss : 31.84259796142578, \t Total Dis Loss : 0.00015191615966614336\n",
      "Steps : 126500, \t Total Gen Loss : 30.558094024658203, \t Total Dis Loss : 7.207037015177775e-06\n",
      "Steps : 126600, \t Total Gen Loss : 30.763057708740234, \t Total Dis Loss : 4.285937393433414e-06\n",
      "Steps : 126700, \t Total Gen Loss : 31.316699981689453, \t Total Dis Loss : 2.2217873265617527e-05\n",
      "Steps : 126800, \t Total Gen Loss : 31.82975196838379, \t Total Dis Loss : 6.225558172445744e-05\n",
      "Steps : 126900, \t Total Gen Loss : 30.206035614013672, \t Total Dis Loss : 0.00012010072532575577\n",
      "Steps : 127000, \t Total Gen Loss : 27.77499771118164, \t Total Dis Loss : 3.323406053823419e-05\n",
      "Steps : 127100, \t Total Gen Loss : 30.29058837890625, \t Total Dis Loss : 2.7589596356847323e-05\n",
      "Steps : 127200, \t Total Gen Loss : 28.771099090576172, \t Total Dis Loss : 7.892551366239786e-05\n",
      "Steps : 127300, \t Total Gen Loss : 28.474308013916016, \t Total Dis Loss : 0.000271417637122795\n",
      "Steps : 127400, \t Total Gen Loss : 31.659713745117188, \t Total Dis Loss : 0.0006178240873850882\n",
      "Steps : 127500, \t Total Gen Loss : 31.434555053710938, \t Total Dis Loss : 2.3093396521289833e-05\n",
      "Steps : 127600, \t Total Gen Loss : 30.234081268310547, \t Total Dis Loss : 4.605036519933492e-05\n",
      "Steps : 127700, \t Total Gen Loss : 33.2938346862793, \t Total Dis Loss : 2.3209533537738025e-05\n",
      "Steps : 127800, \t Total Gen Loss : 27.80762481689453, \t Total Dis Loss : 0.002958082826808095\n",
      "Steps : 127900, \t Total Gen Loss : 29.092151641845703, \t Total Dis Loss : 0.00011795885802712291\n",
      "Steps : 128000, \t Total Gen Loss : 29.565696716308594, \t Total Dis Loss : 5.104351293994114e-05\n",
      "Steps : 128100, \t Total Gen Loss : 26.9968204498291, \t Total Dis Loss : 5.9458077885210514e-05\n",
      "Steps : 128200, \t Total Gen Loss : 26.307369232177734, \t Total Dis Loss : 0.0001548520231153816\n",
      "Steps : 128300, \t Total Gen Loss : 29.395238876342773, \t Total Dis Loss : 2.125598803104367e-05\n",
      "Steps : 128400, \t Total Gen Loss : 30.42474365234375, \t Total Dis Loss : 2.4012013454921544e-05\n",
      "Steps : 128500, \t Total Gen Loss : 31.718460083007812, \t Total Dis Loss : 7.438086322508752e-05\n",
      "Steps : 128600, \t Total Gen Loss : 31.269237518310547, \t Total Dis Loss : 8.847686694934964e-05\n",
      "Steps : 128700, \t Total Gen Loss : 33.86662292480469, \t Total Dis Loss : 0.006214266177266836\n",
      "Steps : 128800, \t Total Gen Loss : 33.901424407958984, \t Total Dis Loss : 5.548601257032715e-05\n",
      "Steps : 128900, \t Total Gen Loss : 34.75604248046875, \t Total Dis Loss : 3.2233809179160744e-05\n",
      "Steps : 129000, \t Total Gen Loss : 33.388099670410156, \t Total Dis Loss : 6.348193892335985e-06\n",
      "Steps : 129100, \t Total Gen Loss : 32.01210021972656, \t Total Dis Loss : 4.175861249677837e-05\n",
      "Steps : 129200, \t Total Gen Loss : 29.682533264160156, \t Total Dis Loss : 0.00010186172585235909\n",
      "Steps : 129300, \t Total Gen Loss : 24.81077766418457, \t Total Dis Loss : 5.66245180380065e-05\n",
      "Time for epoch 23 is 74.50615954399109 sec\n",
      "Steps : 129400, \t Total Gen Loss : 25.77817153930664, \t Total Dis Loss : 4.388089291751385e-05\n",
      "Steps : 129500, \t Total Gen Loss : 28.146209716796875, \t Total Dis Loss : 2.1925119654042646e-05\n",
      "Steps : 129600, \t Total Gen Loss : 29.083494186401367, \t Total Dis Loss : 0.00010612055484671146\n",
      "Steps : 129700, \t Total Gen Loss : 30.14688491821289, \t Total Dis Loss : 0.00048638181760907173\n",
      "Steps : 129800, \t Total Gen Loss : 27.308731079101562, \t Total Dis Loss : 2.4235261662397534e-05\n",
      "Steps : 129900, \t Total Gen Loss : 28.57937240600586, \t Total Dis Loss : 1.4249589185055811e-05\n",
      "Steps : 130000, \t Total Gen Loss : 28.227947235107422, \t Total Dis Loss : 1.8817123418557458e-05\n",
      "Steps : 130100, \t Total Gen Loss : 32.09313201904297, \t Total Dis Loss : 1.526433516119141e-05\n",
      "Steps : 130200, \t Total Gen Loss : 25.640987396240234, \t Total Dis Loss : 0.00029156310483813286\n",
      "Steps : 130300, \t Total Gen Loss : 28.048431396484375, \t Total Dis Loss : 1.0984495929733384e-05\n",
      "Steps : 130400, \t Total Gen Loss : 28.061832427978516, \t Total Dis Loss : 3.331253174110316e-05\n",
      "Steps : 130500, \t Total Gen Loss : 25.165924072265625, \t Total Dis Loss : 7.649648614460602e-05\n",
      "Steps : 130600, \t Total Gen Loss : 25.663225173950195, \t Total Dis Loss : 1.862737553892657e-05\n",
      "Steps : 130700, \t Total Gen Loss : 27.96941566467285, \t Total Dis Loss : 8.355666977877263e-06\n",
      "Steps : 130800, \t Total Gen Loss : 28.93602752685547, \t Total Dis Loss : 1.9184484699508175e-05\n",
      "Steps : 130900, \t Total Gen Loss : 25.657642364501953, \t Total Dis Loss : 2.9881088266847655e-05\n",
      "Steps : 131000, \t Total Gen Loss : 27.844465255737305, \t Total Dis Loss : 1.0916900464508217e-05\n",
      "Steps : 131100, \t Total Gen Loss : 27.23796844482422, \t Total Dis Loss : 2.56217535934411e-05\n",
      "Steps : 131200, \t Total Gen Loss : 30.359600067138672, \t Total Dis Loss : 3.1831368687562644e-05\n",
      "Steps : 131300, \t Total Gen Loss : 23.449832916259766, \t Total Dis Loss : 0.00019000166503246874\n",
      "Steps : 131400, \t Total Gen Loss : 26.26858139038086, \t Total Dis Loss : 6.273936742218211e-05\n",
      "Steps : 131500, \t Total Gen Loss : 29.689208984375, \t Total Dis Loss : 1.2791238077625167e-05\n",
      "Steps : 131600, \t Total Gen Loss : 32.822776794433594, \t Total Dis Loss : 1.3816688806400634e-05\n",
      "Steps : 131700, \t Total Gen Loss : 33.37968826293945, \t Total Dis Loss : 6.126163498265669e-06\n",
      "Steps : 131800, \t Total Gen Loss : 28.873666763305664, \t Total Dis Loss : 4.710143002739642e-06\n",
      "Steps : 131900, \t Total Gen Loss : 25.214187622070312, \t Total Dis Loss : 0.3314321041107178\n",
      "Steps : 132000, \t Total Gen Loss : 29.560409545898438, \t Total Dis Loss : 0.0001287774066440761\n",
      "Steps : 132100, \t Total Gen Loss : 28.28168487548828, \t Total Dis Loss : 0.00021266061230562627\n",
      "Steps : 132200, \t Total Gen Loss : 23.212360382080078, \t Total Dis Loss : 0.0001374845887767151\n",
      "Steps : 132300, \t Total Gen Loss : 27.45006561279297, \t Total Dis Loss : 4.0986185922520235e-05\n",
      "Steps : 132400, \t Total Gen Loss : 31.01166343688965, \t Total Dis Loss : 2.550558747316245e-05\n",
      "Steps : 132500, \t Total Gen Loss : 28.542001724243164, \t Total Dis Loss : 3.1446220418729354e-06\n",
      "Steps : 132600, \t Total Gen Loss : 29.286846160888672, \t Total Dis Loss : 2.7330931970936945e-06\n",
      "Steps : 132700, \t Total Gen Loss : 28.1805419921875, \t Total Dis Loss : 3.772330092033371e-05\n",
      "Steps : 132800, \t Total Gen Loss : 29.606658935546875, \t Total Dis Loss : 3.920643212040886e-05\n",
      "Steps : 132900, \t Total Gen Loss : 25.20212173461914, \t Total Dis Loss : 5.006416540709324e-05\n",
      "Steps : 133000, \t Total Gen Loss : 25.198089599609375, \t Total Dis Loss : 0.00011498160165501758\n",
      "Steps : 133100, \t Total Gen Loss : 24.46278953552246, \t Total Dis Loss : 4.5683380449190736e-05\n",
      "Steps : 133200, \t Total Gen Loss : 26.463369369506836, \t Total Dis Loss : 1.3741778275289107e-05\n",
      "Steps : 133300, \t Total Gen Loss : 30.553462982177734, \t Total Dis Loss : 1.7722866687108763e-05\n",
      "Steps : 133400, \t Total Gen Loss : 27.313068389892578, \t Total Dis Loss : 2.546289215388242e-05\n",
      "Steps : 133500, \t Total Gen Loss : 30.396221160888672, \t Total Dis Loss : 7.788197763147764e-06\n",
      "Steps : 133600, \t Total Gen Loss : 28.51156234741211, \t Total Dis Loss : 9.859397323452868e-06\n",
      "Steps : 133700, \t Total Gen Loss : 29.014827728271484, \t Total Dis Loss : 5.416135536506772e-06\n",
      "Steps : 133800, \t Total Gen Loss : 28.012409210205078, \t Total Dis Loss : 1.2543853699753527e-05\n",
      "Steps : 133900, \t Total Gen Loss : 24.33696174621582, \t Total Dis Loss : 0.00025329331401735544\n",
      "Steps : 134000, \t Total Gen Loss : 29.766475677490234, \t Total Dis Loss : 0.0001331250969087705\n",
      "Steps : 134100, \t Total Gen Loss : 28.067138671875, \t Total Dis Loss : 3.3934156817849725e-05\n",
      "Steps : 134200, \t Total Gen Loss : 27.596633911132812, \t Total Dis Loss : 3.747701339307241e-05\n",
      "Steps : 134300, \t Total Gen Loss : 25.47342300415039, \t Total Dis Loss : 4.491493746172637e-05\n",
      "Steps : 134400, \t Total Gen Loss : 28.345090866088867, \t Total Dis Loss : 1.794151285139378e-05\n",
      "Steps : 134500, \t Total Gen Loss : 27.94607162475586, \t Total Dis Loss : 2.008553383348044e-05\n",
      "Steps : 134600, \t Total Gen Loss : 27.779165267944336, \t Total Dis Loss : 3.560585901141167e-05\n",
      "Steps : 134700, \t Total Gen Loss : 30.535018920898438, \t Total Dis Loss : 1.7223477698280476e-05\n",
      "Steps : 134800, \t Total Gen Loss : 27.20867156982422, \t Total Dis Loss : 1.8856559108826332e-05\n",
      "Steps : 134900, \t Total Gen Loss : 26.852827072143555, \t Total Dis Loss : 3.91514076909516e-05\n",
      "Steps : 135000, \t Total Gen Loss : 26.324127197265625, \t Total Dis Loss : 1.5311321476474404e-05\n",
      "Time for epoch 24 is 72.19978260993958 sec\n",
      "Steps : 135100, \t Total Gen Loss : 29.542842864990234, \t Total Dis Loss : 1.0654192919901107e-05\n",
      "Steps : 135200, \t Total Gen Loss : 28.534610748291016, \t Total Dis Loss : 5.103527655592188e-06\n",
      "Steps : 135300, \t Total Gen Loss : 26.44664192199707, \t Total Dis Loss : 2.6561887352727354e-05\n",
      "Steps : 135400, \t Total Gen Loss : 30.161441802978516, \t Total Dis Loss : 0.00025394465774297714\n",
      "Steps : 135500, \t Total Gen Loss : 27.73541259765625, \t Total Dis Loss : 8.71992961037904e-06\n",
      "Steps : 135600, \t Total Gen Loss : 27.371429443359375, \t Total Dis Loss : 1.612393680261448e-05\n",
      "Steps : 135700, \t Total Gen Loss : 31.60165023803711, \t Total Dis Loss : 4.756484486279078e-06\n",
      "Steps : 135800, \t Total Gen Loss : 25.05254364013672, \t Total Dis Loss : 3.64409715984948e-05\n",
      "Steps : 135900, \t Total Gen Loss : 27.999374389648438, \t Total Dis Loss : 1.581036121933721e-05\n",
      "Steps : 136000, \t Total Gen Loss : 29.06610679626465, \t Total Dis Loss : 2.6167266696575098e-05\n",
      "Steps : 136100, \t Total Gen Loss : 30.072996139526367, \t Total Dis Loss : 2.239185050711967e-05\n",
      "Steps : 136200, \t Total Gen Loss : 26.873558044433594, \t Total Dis Loss : 8.433851689915173e-06\n",
      "Steps : 136300, \t Total Gen Loss : 26.166893005371094, \t Total Dis Loss : 6.956127435842063e-06\n",
      "Steps : 136400, \t Total Gen Loss : 37.52512741088867, \t Total Dis Loss : 6.704619295305747e-07\n",
      "Steps : 136500, \t Total Gen Loss : 36.381446838378906, \t Total Dis Loss : 3.58772485924419e-05\n",
      "Steps : 136600, \t Total Gen Loss : 35.200721740722656, \t Total Dis Loss : 5.766102731286082e-06\n",
      "Steps : 136700, \t Total Gen Loss : 29.18294906616211, \t Total Dis Loss : 0.0001086057091015391\n",
      "Steps : 136800, \t Total Gen Loss : 24.54747772216797, \t Total Dis Loss : 0.0012891284422948956\n",
      "Steps : 136900, \t Total Gen Loss : 26.109413146972656, \t Total Dis Loss : 7.008669490460306e-05\n",
      "Steps : 137000, \t Total Gen Loss : 25.127595901489258, \t Total Dis Loss : 8.785916725173593e-05\n",
      "Steps : 137100, \t Total Gen Loss : 31.424341201782227, \t Total Dis Loss : 1.7455999113735743e-05\n",
      "Steps : 137200, \t Total Gen Loss : 24.987428665161133, \t Total Dis Loss : 0.00019745119789149612\n",
      "Steps : 137300, \t Total Gen Loss : 26.808208465576172, \t Total Dis Loss : 3.7393288948806e-05\n",
      "Steps : 137400, \t Total Gen Loss : 27.84401512145996, \t Total Dis Loss : 0.0003263995167799294\n",
      "Steps : 137500, \t Total Gen Loss : 28.678077697753906, \t Total Dis Loss : 0.00014081218978390098\n",
      "Steps : 137600, \t Total Gen Loss : 27.775493621826172, \t Total Dis Loss : 6.517369911307469e-05\n",
      "Steps : 137700, \t Total Gen Loss : 27.147632598876953, \t Total Dis Loss : 4.153120607952587e-05\n",
      "Steps : 137800, \t Total Gen Loss : 26.54071807861328, \t Total Dis Loss : 3.90165405406151e-05\n",
      "Steps : 137900, \t Total Gen Loss : 28.258535385131836, \t Total Dis Loss : 7.19772378943162e-06\n",
      "Steps : 138000, \t Total Gen Loss : 24.2174015045166, \t Total Dis Loss : 0.0032508401200175285\n",
      "Steps : 138100, \t Total Gen Loss : 21.753379821777344, \t Total Dis Loss : 0.0005953695508651435\n",
      "Steps : 138200, \t Total Gen Loss : 25.34899139404297, \t Total Dis Loss : 8.523066207999364e-05\n",
      "Steps : 138300, \t Total Gen Loss : 25.410263061523438, \t Total Dis Loss : 2.9631317374878563e-05\n",
      "Steps : 138400, \t Total Gen Loss : 27.304813385009766, \t Total Dis Loss : 1.7998701878241263e-05\n",
      "Steps : 138500, \t Total Gen Loss : 24.020946502685547, \t Total Dis Loss : 5.301144119584933e-05\n",
      "Steps : 138600, \t Total Gen Loss : 29.123687744140625, \t Total Dis Loss : 8.681166036694776e-06\n",
      "Steps : 138700, \t Total Gen Loss : 30.281475067138672, \t Total Dis Loss : 7.628398179804208e-06\n",
      "Steps : 138800, \t Total Gen Loss : 28.050243377685547, \t Total Dis Loss : 4.9325768486596644e-05\n",
      "Steps : 138900, \t Total Gen Loss : 25.307167053222656, \t Total Dis Loss : 7.086626737873303e-06\n",
      "Steps : 139000, \t Total Gen Loss : 33.32940673828125, \t Total Dis Loss : 1.6630727941446821e-06\n",
      "Steps : 139100, \t Total Gen Loss : 29.784626007080078, \t Total Dis Loss : 4.4471107685239986e-06\n",
      "Steps : 139200, \t Total Gen Loss : 30.525562286376953, \t Total Dis Loss : 1.5718051145086065e-05\n",
      "Steps : 139300, \t Total Gen Loss : 29.659366607666016, \t Total Dis Loss : 9.619017873774283e-06\n",
      "Steps : 139400, \t Total Gen Loss : 27.586410522460938, \t Total Dis Loss : 2.9917198389739497e-06\n",
      "Steps : 139500, \t Total Gen Loss : 28.671722412109375, \t Total Dis Loss : 3.7672782582376385e-06\n",
      "Steps : 139600, \t Total Gen Loss : 31.117755889892578, \t Total Dis Loss : 3.127486479570507e-06\n",
      "Steps : 139700, \t Total Gen Loss : 29.479671478271484, \t Total Dis Loss : 2.111298636009451e-05\n",
      "Steps : 139800, \t Total Gen Loss : 30.622459411621094, \t Total Dis Loss : 2.959459152407362e-06\n",
      "Steps : 139900, \t Total Gen Loss : 31.428234100341797, \t Total Dis Loss : 2.283089770571678e-06\n",
      "Steps : 140000, \t Total Gen Loss : 30.297391891479492, \t Total Dis Loss : 9.908905667543877e-06\n",
      "Steps : 140100, \t Total Gen Loss : 30.336557388305664, \t Total Dis Loss : 9.511896678304765e-06\n",
      "Steps : 140200, \t Total Gen Loss : 24.767189025878906, \t Total Dis Loss : 6.946428038645536e-05\n",
      "Steps : 140300, \t Total Gen Loss : 33.65986633300781, \t Total Dis Loss : 7.0982550823828205e-06\n",
      "Steps : 140400, \t Total Gen Loss : 27.108081817626953, \t Total Dis Loss : 3.939862290280871e-05\n",
      "Steps : 140500, \t Total Gen Loss : 27.787818908691406, \t Total Dis Loss : 1.6822725228848867e-05\n",
      "Steps : 140600, \t Total Gen Loss : 27.95794677734375, \t Total Dis Loss : 1.5464553143829107e-05\n",
      "Time for epoch 25 is 72.3000168800354 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f2f77691c10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3522851 0.33885112\n",
      "0.13266526 0.13558505\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_error = []\n",
    "\n",
    "for i in range(0, 15000):\n",
    "    if int(bol_test_labels[i]) != gt_labels[i]:\n",
    "        detect_error.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.44133333333333336\n"
     ]
    }
   ],
   "source": [
    "print('accuracy :', 1-(15000- len(detect_error))/15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Annomaly detection에서는 gan의 성능은 뛰어난 편은 아닌거 같다.(50%아래니...)\n",
    "- 하지만 GAN의 성능이 향상되면 부족한 이상 데이터에 대해 효과적으로 작용할 가능성이 보였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
