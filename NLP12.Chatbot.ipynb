{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow import keras\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/aiffel/aiffel/songys_chatbot/ChatbotData .csv')\n",
    "data.head()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL 열 제거\n",
    "del data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \"\", sentence)#알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #문장부호 양옆에 공백을 추가합니다.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(row_file):\n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "    \n",
    "    for i in range(0, len(row_file)):\n",
    "        mecab = Mecab()\n",
    "        \n",
    "        que, ans = row_file.loc[i]\n",
    "        \n",
    "        que = preprocess_sentence(que)\n",
    "        que = mecab.morphs(que)\n",
    "        \n",
    "        ans = preprocess_sentence(ans)\n",
    "        ans = mecab.morphs(ans)\n",
    "        \n",
    "        if len(que) < 20 and len(ans) < 20: #일정 길이 이상 제외\n",
    "            if que not in que_corpus:\n",
    "                if ans not in ans_corpus: #소스와 타겟별 중복 검사\n",
    "                    que_corpus.append(que)\n",
    "                    ans_corpus.append(ans)\n",
    "    \n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7519\n",
      "7519\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = build_corpus(data)\n",
    "\n",
    "# 둘이 같아야 함\n",
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('/home/aiffel/aiffel/transformer_chatbot/ko/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7c968fa45545db89e72df87d1f10e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    new_que_corpus.append(que)\n",
    "    new_ans_corpus.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9037cda03ab421fb8e2332718de0cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    new_que_corpus.append(que)\n",
    "    new_ans_corpus.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474abeb2a57a45669d7d11f07ce78eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22557\n",
      "22557\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans = ans_corpus[idx]\n",
    "    new_que_corpus.append(que)\n",
    "    new_ans_corpus.append(ans)\n",
    "\n",
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " '1 중퇴 학교 떨어졌 어 ',\n",
       " None,\n",
       " None,\n",
       " 'sd 카드 망가졌 어서 ',\n",
       " 'sns 맞 팔 과연 안 하 지 ㅠㅠ ',\n",
       " 'sns 시간 낭비 인은 거 아 는데 매일 하 는 중 ',\n",
       " 'sns 보 면 나 만 빼 고 다 사랑 해 보여 ',\n",
       " '이따금 궁금 해 ',\n",
       " '가끔 은 혼자 인 게 괜찮 다 ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_que_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "\n",
    "\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '.': 2,\n",
       " '<start>': 3,\n",
       " '<end>': 4,\n",
       " '이': 5,\n",
       " '는': 6,\n",
       " '하': 7,\n",
       " '을': 8,\n",
       " '가': 9,\n",
       " '세요': 10,\n",
       " '좋': 11,\n",
       " '고': 12,\n",
       " '어': 13,\n",
       " '거': 14,\n",
       " '있': 15,\n",
       " '은': 16,\n",
       " '해': 17,\n",
       " '보': 18,\n",
       " '지': 19,\n",
       " '?': 20,\n",
       " '나': 21,\n",
       " '아': 22,\n",
       " '도': 23,\n",
       " '게': 24,\n",
       " '겠': 25,\n",
       " '에': 26,\n",
       " '사람': 27,\n",
       " '예요': 28,\n",
       " '사랑': 29,\n",
       " '어요': 30,\n",
       " '를': 31,\n",
       " '죠': 32,\n",
       " '한': 33,\n",
       " '같': 34,\n",
       " '다': 35,\n",
       " '네': 36,\n",
       " '수': 37,\n",
       " '면': 38,\n",
       " '네요': 39,\n",
       " '의': 40,\n",
       " '없': 41,\n",
       " '싶': 42,\n",
       " '안': 43,\n",
       " '친구': 44,\n",
       " '것': 45,\n",
       " '생각': 46,\n",
       " '봐요': 47,\n",
       " '는데': 48,\n",
       " '마음': 49,\n",
       " '않': 50,\n",
       " '아요': 51,\n",
       " '말': 52,\n",
       " '할': 53,\n",
       " '너무': 54,\n",
       " '되': 55,\n",
       " '이별': 56,\n",
       " '잘': 57,\n",
       " '주': 58,\n",
       " '했': 59,\n",
       " '었': 60,\n",
       " '남자': 61,\n",
       " '내': 62,\n",
       " '연락': 63,\n",
       " '기': 64,\n",
       " '일': 65,\n",
       " '더': 66,\n",
       " '만': 67,\n",
       " '들': 68,\n",
       " '여자': 69,\n",
       " '힘들': 70,\n",
       " '해요': 71,\n",
       " '시간': 72,\n",
       " '남': 73,\n",
       " '썸': 74,\n",
       " '길': 75,\n",
       " '짝': 76,\n",
       " '많이': 77,\n",
       " '으로': 78,\n",
       " '았': 79,\n",
       " '한테': 80,\n",
       " '으면': 81,\n",
       " '건': 82,\n",
       " '때': 83,\n",
       " '에요': 84,\n",
       " '좀': 85,\n",
       " '에서': 86,\n",
       " '요': 87,\n",
       " '야': 88,\n",
       " '알': 89,\n",
       " '그': 90,\n",
       " '많': 91,\n",
       " '만나': 92,\n",
       " '에게': 93,\n",
       " '습니다': 94,\n",
       " '받': 95,\n",
       " '로': 96,\n",
       " '먹': 97,\n",
       " '을까': 98,\n",
       " '연애': 99,\n",
       " '저': 100,\n",
       " '!': 101,\n",
       " '뭐': 102,\n",
       " '인': 103,\n",
       " '이제': 104,\n",
       " '오늘': 105,\n",
       " '적': 106,\n",
       " '던': 107,\n",
       " '애': 108,\n",
       " '괜찮': 109,\n",
       " '못': 110,\n",
       " '해도': 111,\n",
       " '자신': 112,\n",
       " '마세요': 113,\n",
       " '년': 114,\n",
       " '끝': 115,\n",
       " '아니': 116,\n",
       " '할까': 117,\n",
       " '모르': 118,\n",
       " '다시': 119,\n",
       " '타': 120,\n",
       " '잊': 121,\n",
       " '필요': 122,\n",
       " '전': 123,\n",
       " '당신': 124,\n",
       " '걸': 125,\n",
       " '해야': 126,\n",
       " '다른': 127,\n",
       " '지만': 128,\n",
       " '정말': 129,\n",
       " '라고': 130,\n",
       " '왜': 131,\n",
       " '또': 132,\n",
       " '어떻게': 133,\n",
       " '정리': 134,\n",
       " '랑': 135,\n",
       " '데': 136,\n",
       " '과': 137,\n",
       " '라': 138,\n",
       " '살': 139,\n",
       " '달': 140,\n",
       " '결혼': 141,\n",
       " '짝사랑': 142,\n",
       " '서': 143,\n",
       " '인데': 144,\n",
       " '날': 145,\n",
       " '이랑': 146,\n",
       " '오': 147,\n",
       " '참': 148,\n",
       " '중': 149,\n",
       " '될': 150,\n",
       " '은데': 151,\n",
       " '지금': 152,\n",
       " '같이': 153,\n",
       " '니': 154,\n",
       " '인가': 155,\n",
       " '봐': 156,\n",
       " '돼': 157,\n",
       " '왔': 158,\n",
       " '후회': 159,\n",
       " '아직': 160,\n",
       " '제': 161,\n",
       " '그녀': 162,\n",
       " '행복': 163,\n",
       " '와': 164,\n",
       " '봅니다': 165,\n",
       " '고백': 166,\n",
       " '면서': 167,\n",
       " '중요': 168,\n",
       " '꿈': 169,\n",
       " '혼자': 170,\n",
       " '준비': 171,\n",
       " '보다': 172,\n",
       " '헤어지': 173,\n",
       " '돼요': 174,\n",
       " '좋아하': 175,\n",
       " '바랄게요': 176,\n",
       " '방법': 177,\n",
       " '사': 178,\n",
       " '싫': 179,\n",
       " '고민': 180,\n",
       " '그런': 181,\n",
       " '시작': 182,\n",
       " '먼저': 183,\n",
       " '힘든': 184,\n",
       " '는지': 185,\n",
       " '사귀': 186,\n",
       " '감정': 187,\n",
       " '녀': 188,\n",
       " '선물': 189,\n",
       " '다고': 190,\n",
       " '힘드': 191,\n",
       " '술': 192,\n",
       " '합니다': 193,\n",
       " '서로': 194,\n",
       " '맞': 195,\n",
       " '하나': 196,\n",
       " '물': 197,\n",
       " '줄': 198,\n",
       " '까지': 199,\n",
       " '가능': 200,\n",
       " '조금': 201,\n",
       " '자': 202,\n",
       " '될까': 203,\n",
       " '계속': 204,\n",
       " '다면': 205,\n",
       " '시': 206,\n",
       " '번': 207,\n",
       " '지요': 208,\n",
       " '만큼': 209,\n",
       " '두': 210,\n",
       " '이야기': 211,\n",
       " '자꾸': 212,\n",
       " '해서': 213,\n",
       " '기분': 214,\n",
       " '기억': 215,\n",
       " '그럴': 216,\n",
       " '진짜': 217,\n",
       " '너': 218,\n",
       " '후': 219,\n",
       " '여친': 220,\n",
       " '헤어진지': 221,\n",
       " '1': 222,\n",
       " '나요': 223,\n",
       " '째': 224,\n",
       " '데이트': 225,\n",
       " '든': 226,\n",
       " '카톡': 227,\n",
       " '걸까': 228,\n",
       " '집': 229,\n",
       " '쉽': 230,\n",
       " '믿': 231,\n",
       " '인지': 232,\n",
       " '쓰': 233,\n",
       " '표현': 234,\n",
       " '듯': 235,\n",
       " '라도': 236,\n",
       " '바랍니다': 237,\n",
       " '자고': 238,\n",
       " '다가': 239,\n",
       " '남친': 240,\n",
       " '상처': 241,\n",
       " '미련': 242,\n",
       " '어도': 243,\n",
       " '됐': 244,\n",
       " '볼까': 245,\n",
       " '신경': 246,\n",
       " '눈': 247,\n",
       " '입니다': 248,\n",
       " '니까요': 249,\n",
       " '3': 250,\n",
       " '긴': 251,\n",
       " '이상': 252,\n",
       " '어떨까': 253,\n",
       " '맘': 254,\n",
       " ',': 255,\n",
       " '돈': 256,\n",
       " '그냥': 257,\n",
       " '쉬': 258,\n",
       " '이해': 259,\n",
       " '만들': 260,\n",
       " '함께': 261,\n",
       " '보내': 262,\n",
       " '대화': 263,\n",
       " '하루': 264,\n",
       " '무슨': 265,\n",
       " '2': 266,\n",
       " '걱정': 267,\n",
       " '썸남': 268,\n",
       " '러': 269,\n",
       " '함': 270,\n",
       " '아프': 271,\n",
       " '어떤': 272,\n",
       " '셨': 273,\n",
       " '헤어졌': 274,\n",
       " '몰라요': 275,\n",
       " '여행': 276,\n",
       " '누구': 277,\n",
       " '씩': 278,\n",
       " '놀': 279,\n",
       " '음': 280,\n",
       " '줘': 281,\n",
       " '언제': 282,\n",
       " '기다리': 283,\n",
       " '된': 284,\n",
       " '때문': 285,\n",
       " '관심': 286,\n",
       " '이렇게': 287,\n",
       " '오래': 288,\n",
       " '새로운': 289,\n",
       " '결정': 290,\n",
       " '속': 291,\n",
       " '앞': 292,\n",
       " '곳': 293,\n",
       " '어서': 294,\n",
       " '니까': 295,\n",
       " '이유': 296,\n",
       " '헤어진': 297,\n",
       " '공부': 298,\n",
       " '어떡': 299,\n",
       " '머리': 300,\n",
       " '만날': 301,\n",
       " '개월': 302,\n",
       " '내일': 303,\n",
       " '잡': 304,\n",
       " '생각나': 305,\n",
       " '첫': 306,\n",
       " '이젠': 307,\n",
       " '마지막': 308,\n",
       " '군요': 309,\n",
       " '궁금': 310,\n",
       " '그렇게': 311,\n",
       " '바라': 312,\n",
       " '어디': 313,\n",
       " '선택': 314,\n",
       " '결국': 315,\n",
       " '부담': 316,\n",
       " '님': 317,\n",
       " '라는': 318,\n",
       " '비': 319,\n",
       " '나가': 320,\n",
       " '려고': 321,\n",
       " '따라': 322,\n",
       " '잠': 323,\n",
       " '별': 324,\n",
       " '처럼': 325,\n",
       " '사이': 326,\n",
       " '입': 327,\n",
       " '올': 328,\n",
       " '항상': 329,\n",
       " '추억': 330,\n",
       " '텐데': 331,\n",
       " '노력': 332,\n",
       " '노래': 333,\n",
       " '뿐': 334,\n",
       " '다는': 335,\n",
       " '덜': 336,\n",
       " '부터': 337,\n",
       " '도움': 338,\n",
       " '생겼': 339,\n",
       " '였': 340,\n",
       " '건가': 341,\n",
       " '차': 342,\n",
       " '라면': 343,\n",
       " '죽': 344,\n",
       " '나이': 345,\n",
       " '대': 346,\n",
       " '운동': 347,\n",
       " '전화': 348,\n",
       " '우리': 349,\n",
       " '분': 350,\n",
       " '드': 351,\n",
       " '느낌': 352,\n",
       " '꼭': 353,\n",
       " '만났': 354,\n",
       " '한가': 355,\n",
       " '일까': 356,\n",
       " '인연': 357,\n",
       " '직접': 358,\n",
       " '으니까요': 359,\n",
       " '대로': 360,\n",
       " '순간': 361,\n",
       " '변화': 362,\n",
       " '드세요': 363,\n",
       " '상황': 364,\n",
       " '반': 365,\n",
       " '답답': 366,\n",
       " '못하': 367,\n",
       " '질': 368,\n",
       " '문제': 369,\n",
       " '늦': 370,\n",
       " '충분히': 371,\n",
       " '으세요': 372,\n",
       " '상대': 373,\n",
       " '잘못': 374,\n",
       " '보이': 375,\n",
       " '만남': 376,\n",
       " '정도': 377,\n",
       " '재회': 378,\n",
       " '그게': 379,\n",
       " '스럽': 380,\n",
       " '을까요': 381,\n",
       " '용기': 382,\n",
       " '아서': 383,\n",
       " '관계': 384,\n",
       " '어야': 385,\n",
       " '기대': 386,\n",
       " '인생': 387,\n",
       " '볼': 388,\n",
       " '어제': 389,\n",
       " '헤어짐': 390,\n",
       " '가슴': 391,\n",
       " '모든': 392,\n",
       " '이런': 393,\n",
       " '이나': 394,\n",
       " '세상': 395,\n",
       " '진심': 396,\n",
       " '매일': 397,\n",
       " '나쁜': 398,\n",
       " '건지': 399,\n",
       " '봤': 400,\n",
       " '처음': 401,\n",
       " '큰': 402,\n",
       " '가지': 403,\n",
       " '힘': 404,\n",
       " '현실': 405,\n",
       " '거나': 406,\n",
       " '확인': 407,\n",
       " '놓': 408,\n",
       " '넘': 409,\n",
       " '감': 410,\n",
       " '짧': 411,\n",
       " '의미': 412,\n",
       " '5': 413,\n",
       " '다가가': 414,\n",
       " '찾아보': 415,\n",
       " '밥': 416,\n",
       " '그만': 417,\n",
       " '아닌': 418,\n",
       " '주말': 419,\n",
       " '다음': 420,\n",
       " '듣': 421,\n",
       " '진': 422,\n",
       " '부모': 423,\n",
       " '그렇': 424,\n",
       " '아픔': 425,\n",
       " '그러': 426,\n",
       " '사진': 427,\n",
       " '제일': 428,\n",
       " '둘': 429,\n",
       " '글': 430,\n",
       " '영화': 431,\n",
       " '후폭풍': 432,\n",
       " '찾': 433,\n",
       " '상대방': 434,\n",
       " '모두': 435,\n",
       " '갑자기': 436,\n",
       " '붙잡': 437,\n",
       " '없이': 438,\n",
       " '건강': 439,\n",
       " '귀찮': 440,\n",
       " '졌': 441,\n",
       " '몸': 442,\n",
       " '답': 443,\n",
       " '아도': 444,\n",
       " '아침': 445,\n",
       " '마시': 446,\n",
       " '지내': 447,\n",
       " '차단': 448,\n",
       " '받아들이': 449,\n",
       " '얼른': 450,\n",
       " '눈물': 451,\n",
       " '줬': 452,\n",
       " '습관': 453,\n",
       " '난': 454,\n",
       " '다르': 455,\n",
       " '별로': 456,\n",
       " '몇': 457,\n",
       " '추천': 458,\n",
       " '버리': 459,\n",
       " '을지': 460,\n",
       " '모습': 461,\n",
       " '확실': 462,\n",
       " '마다': 463,\n",
       " '스스로': 464,\n",
       " '4': 465,\n",
       " '까': 466,\n",
       " '실수': 467,\n",
       " '그래도': 468,\n",
       " '맛있': 469,\n",
       " '요즘': 470,\n",
       " '복잡': 471,\n",
       " '티': 472,\n",
       " '언젠간': 473,\n",
       " '행동': 474,\n",
       " '천천히': 475,\n",
       " '갈': 476,\n",
       " '갔': 477,\n",
       " '챙겨': 478,\n",
       " '운명': 479,\n",
       " '화': 480,\n",
       " '밤': 481,\n",
       " '딱': 482,\n",
       " '갖': 483,\n",
       " '다니': 484,\n",
       " '아무': 485,\n",
       " '아픈': 486,\n",
       " '웃': 487,\n",
       " '호감': 488,\n",
       " '6': 489,\n",
       " '났': 490,\n",
       " '가장': 491,\n",
       " '대한': 492,\n",
       " '자주': 493,\n",
       " '익숙': 494,\n",
       " '가끔': 495,\n",
       " '법': 496,\n",
       " '날씨': 497,\n",
       " '도와': 498,\n",
       " '남편': 499,\n",
       " '편': 500,\n",
       " '잊혀': 501,\n",
       " '축하': 502,\n",
       " '톡': 503,\n",
       " '힘내': 504,\n",
       " '드릴게요': 505,\n",
       " '스트레스': 506,\n",
       " '빨리': 507,\n",
       " '나와': 508,\n",
       " '새': 509,\n",
       " '회사': 510,\n",
       " '란': 511,\n",
       " '약': 512,\n",
       " '자기': 513,\n",
       " '울': 514,\n",
       " '만난': 515,\n",
       " '극복': 516,\n",
       " '솔직': 517,\n",
       " '그분': 518,\n",
       " '뭘': 519,\n",
       " '얼굴': 520,\n",
       " '포기': 521,\n",
       " '차이': 522,\n",
       " '편지': 523,\n",
       " '확신': 524,\n",
       " '변하': 525,\n",
       " '형': 526,\n",
       " '자연': 527,\n",
       " '보여': 528,\n",
       " '선': 529,\n",
       " '열심히': 530,\n",
       " '뭘까': 531,\n",
       " '설레': 532,\n",
       " '냐': 533,\n",
       " '벌써': 534,\n",
       " '쉬운': 535,\n",
       " '소개팅': 536,\n",
       " '바쁘': 537,\n",
       " '별후': 538,\n",
       " '마요': 539,\n",
       " 'sns': 540,\n",
       " '게임': 541,\n",
       " '점점': 542,\n",
       " '기다려': 543,\n",
       " '아파': 544,\n",
       " '어때': 545,\n",
       " '정신': 546,\n",
       " '생일': 547,\n",
       " '성공': 548,\n",
       " '낫': 549,\n",
       " '바람': 550,\n",
       " '한다고': 551,\n",
       " '말씀': 552,\n",
       " '인정': 553,\n",
       " 'ㅠ': 554,\n",
       " '한다는': 555,\n",
       " '무시': 556,\n",
       " '관리': 557,\n",
       " '봄': 558,\n",
       " '엄청': 559,\n",
       " '나오': 560,\n",
       " '스러운': 561,\n",
       " '짜증': 562,\n",
       " '예의': 563,\n",
       " '보통': 564,\n",
       " '준': 565,\n",
       " '임': 566,\n",
       " '후련': 567,\n",
       " '척': 568,\n",
       " '문자': 569,\n",
       " '할지': 570,\n",
       " '집착': 571,\n",
       " '커피': 572,\n",
       " '동거': 573,\n",
       " '그리고': 574,\n",
       " '연인': 575,\n",
       " '세': 576,\n",
       " '알아보': 577,\n",
       " '주변': 578,\n",
       " '편하': 579,\n",
       " '연습': 580,\n",
       " '학교': 581,\n",
       " '거짓말': 582,\n",
       " '원': 583,\n",
       " '동안': 584,\n",
       " '예쁘': 585,\n",
       " '옷': 586,\n",
       " '약속': 587,\n",
       " '따뜻': 588,\n",
       " '아야': 589,\n",
       " '마련': 590,\n",
       " '성격': 591,\n",
       " '카페': 592,\n",
       " '여기': 593,\n",
       " '지났': 594,\n",
       " '얘기': 595,\n",
       " '슬픈': 596,\n",
       " '잠시': 597,\n",
       " '위로': 598,\n",
       " '해질': 599,\n",
       " '생활': 600,\n",
       " '지나': 601,\n",
       " '아닌데': 602,\n",
       " '더니': 603,\n",
       " '바': 604,\n",
       " '상관': 605,\n",
       " '대해': 606,\n",
       " '줄까': 607,\n",
       " '어려워': 608,\n",
       " '미치': 609,\n",
       " '구': 610,\n",
       " '떠나': 611,\n",
       " '핸드폰': 612,\n",
       " '한지': 613,\n",
       " '가져': 614,\n",
       " '얼마': 615,\n",
       " '미안': 616,\n",
       " '통보': 617,\n",
       " '우울': 618,\n",
       " '주일': 619,\n",
       " '접': 620,\n",
       " '무엇': 621,\n",
       " '위해': 622,\n",
       " '응원': 623,\n",
       " '충분': 624,\n",
       " '진정': 625,\n",
       " 'ㅠㅠ': 626,\n",
       " '가족': 627,\n",
       " '풀': 628,\n",
       " '해야지': 629,\n",
       " '귀': 630,\n",
       " '맨날': 631,\n",
       " '욕': 632,\n",
       " '여러': 633,\n",
       " '존중': 634,\n",
       " '아무것': 635,\n",
       " '일어나': 636,\n",
       " '피곤': 637,\n",
       " '잔': 638,\n",
       " '여': 639,\n",
       " '돌아오': 640,\n",
       " '아무래도': 641,\n",
       " '조심': 642,\n",
       " '작': 643,\n",
       " '소중': 644,\n",
       " '크': 645,\n",
       " '려': 646,\n",
       " '깊': 647,\n",
       " '스러워': 648,\n",
       " '과정': 649,\n",
       " '흐르': 650,\n",
       " '부분': 651,\n",
       " '탈': 652,\n",
       " '살펴보': 653,\n",
       " '언제나': 654,\n",
       " '달라지': 655,\n",
       " '시켜': 656,\n",
       " '까먹': 657,\n",
       " '뭔지': 658,\n",
       " '어렵': 659,\n",
       " '한데': 660,\n",
       " '소리': 661,\n",
       " '부족': 662,\n",
       " '바로': 663,\n",
       " '끊': 664,\n",
       " '똑같': 665,\n",
       " '여유': 666,\n",
       " '점': 667,\n",
       " '기간': 668,\n",
       " '간': 669,\n",
       " '이성': 670,\n",
       " '못한': 671,\n",
       " '곧': 672,\n",
       " '어쩔': 673,\n",
       " '착각': 674,\n",
       " '숨': 675,\n",
       " '쯤': 676,\n",
       " '는다면': 677,\n",
       " '어느': 678,\n",
       " '사친': 679,\n",
       " '한다면': 680,\n",
       " '다를': 681,\n",
       " '사세요': 682,\n",
       " '최고': 683,\n",
       " '어떻': 684,\n",
       " '도전': 685,\n",
       " '버렸': 686,\n",
       " '재밌': 687,\n",
       " '번호': 688,\n",
       " '멀': 689,\n",
       " '본': 690,\n",
       " '자는': 691,\n",
       " '밖': 692,\n",
       " '원래': 693,\n",
       " '찍': 694,\n",
       " '새벽': 695,\n",
       " '질투': 696,\n",
       " '안녕': 697,\n",
       " '엄마': 698,\n",
       " '잠깐': 699,\n",
       " '장': 700,\n",
       " '헷갈리': 701,\n",
       " '허전': 702,\n",
       " '환승': 703,\n",
       " '보냈': 704,\n",
       " '프': 705,\n",
       " '바보': 706,\n",
       " '두려워': 707,\n",
       " '문득': 708,\n",
       " '어느덧': 709,\n",
       " '맞춰': 710,\n",
       " '본인': 711,\n",
       " '그래요': 712,\n",
       " '자체': 713,\n",
       " '다행': 714,\n",
       " '더라고요': 715,\n",
       " '겠지': 716,\n",
       " '재미': 717,\n",
       " '예민': 718,\n",
       " '엔': 719,\n",
       " '완전': 720,\n",
       " '으니': 721,\n",
       " '써': 722,\n",
       " '종교': 723,\n",
       " '능력': 724,\n",
       " '스타일': 725,\n",
       " '일찍': 726,\n",
       " '손': 727,\n",
       " '뭔가': 728,\n",
       " '인사': 729,\n",
       " '성': 730,\n",
       " '상담': 731,\n",
       " '옆': 732,\n",
       " '인기': 733,\n",
       " '드리': 734,\n",
       " '려면': 735,\n",
       " '깨': 736,\n",
       " '타이밍': 737,\n",
       " '짐': 738,\n",
       " '흘렀': 739,\n",
       " '7': 740,\n",
       " '사실': 741,\n",
       " '전해': 742,\n",
       " '경우': 743,\n",
       " '읽': 744,\n",
       " '며': 745,\n",
       " '흔들리': 746,\n",
       " '반복': 747,\n",
       " '그런가': 748,\n",
       " '아닌지': 749,\n",
       " '장거리': 750,\n",
       " '가요': 751,\n",
       " '삶': 752,\n",
       " '시기': 753,\n",
       " '즐거운': 754,\n",
       " '나눠': 755,\n",
       " '간다': 756,\n",
       " '폰': 757,\n",
       " '어야지': 758,\n",
       " '수록': 759,\n",
       " '어쩌': 760,\n",
       " '기회': 761,\n",
       " '예쁜': 762,\n",
       " '오빠': 763,\n",
       " '배우': 764,\n",
       " '칭찬': 765,\n",
       " '놈': 766,\n",
       " '나중': 767,\n",
       " '소개': 768,\n",
       " '비싸': 769,\n",
       " '드디어': 770,\n",
       " '심해': 771,\n",
       " '분위기': 772,\n",
       " '한잔': 773,\n",
       " '답장': 774,\n",
       " '해졌': 775,\n",
       " '자유': 776,\n",
       " '자존': 777,\n",
       " '최선': 778,\n",
       " '지우': 779,\n",
       " '이기': 780,\n",
       " '그대로': 781,\n",
       " '위한': 782,\n",
       " '답니다': 783,\n",
       " '어려운': 784,\n",
       " '됩니다': 785,\n",
       " '알려': 786,\n",
       " '시원': 787,\n",
       " '매력': 788,\n",
       " '적극': 789,\n",
       " '서운': 790,\n",
       " '인가요': 791,\n",
       " '배려': 792,\n",
       " '걸로': 793,\n",
       " '생길': 794,\n",
       " '금방': 795,\n",
       " '할게요': 796,\n",
       " '주무세요': 797,\n",
       " '아닐까요': 798,\n",
       " '갈까': 799,\n",
       " '당황': 800,\n",
       " '취미': 801,\n",
       " '야지': 802,\n",
       " '괜히': 803,\n",
       " '기본': 804,\n",
       " '상': 805,\n",
       " '커플': 806,\n",
       " '짓': 807,\n",
       " '느껴': 808,\n",
       " '좋아해': 809,\n",
       " '몰랐': 810,\n",
       " '떨려': 811,\n",
       " '뒤': 812,\n",
       " '실': 813,\n",
       " '집중': 814,\n",
       " '아무리': 815,\n",
       " '알바': 816,\n",
       " '돌아가': 817,\n",
       " '찾아가': 818,\n",
       " '자리': 819,\n",
       " '미리': 820,\n",
       " '멋진': 821,\n",
       " '삭제': 822,\n",
       " '하늘': 823,\n",
       " '8': 824,\n",
       " '소식': 825,\n",
       " '땐': 826,\n",
       " '조언': 827,\n",
       " '결심': 828,\n",
       " '잠수': 829,\n",
       " '휴': 830,\n",
       " '절대': 831,\n",
       " '느끼': 832,\n",
       " '쓰이': 833,\n",
       " '좋아할': 834,\n",
       " '생기': 835,\n",
       " '그건': 836,\n",
       " '고생': 837,\n",
       " '나왔': 838,\n",
       " '려나': 839,\n",
       " '키우': 840,\n",
       " '무서워': 841,\n",
       " '시험': 842,\n",
       " '그래': 843,\n",
       " '전환': 844,\n",
       " '한다': 845,\n",
       " '꽃': 846,\n",
       " '이루': 847,\n",
       " '화장': 848,\n",
       " '위': 849,\n",
       " '래': 850,\n",
       " '줘야': 851,\n",
       " '의지': 852,\n",
       " '무': 853,\n",
       " '제대로': 854,\n",
       " '이러': 855,\n",
       " '지쳤': 856,\n",
       " '어색': 857,\n",
       " '단': 858,\n",
       " '싸우': 859,\n",
       " '밀': 860,\n",
       " '비밀': 861,\n",
       " '불안': 862,\n",
       " '사과': 863,\n",
       " '위험': 864,\n",
       " '언젠가': 865,\n",
       " '일상': 866,\n",
       " '해결': 867,\n",
       " '직장': 868,\n",
       " '플': 869,\n",
       " '그리워': 870,\n",
       " '식': 871,\n",
       " '사귄': 872,\n",
       " '상태': 873,\n",
       " '는다는': 874,\n",
       " '경험': 875,\n",
       " '이거': 876,\n",
       " '영원': 877,\n",
       " '거절': 878,\n",
       " '누군가': 879,\n",
       " '그것': 880,\n",
       " '때론': 881,\n",
       " '높': 882,\n",
       " '존재': 883,\n",
       " '은가': 884,\n",
       " '대요': 885,\n",
       " '자책': 886,\n",
       " '놓아주': 887,\n",
       " '떨리': 888,\n",
       " '쇼핑': 889,\n",
       " '걔': 890,\n",
       " '슬프': 891,\n",
       " '내리': 892,\n",
       " '얼마나': 893,\n",
       " '방학': 894,\n",
       " '군대': 895,\n",
       " '기다릴': 896,\n",
       " '기념일': 897,\n",
       " '걷': 898,\n",
       " '질까': 899,\n",
       " '바뀌': 900,\n",
       " '마': 901,\n",
       " '고치': 902,\n",
       " '꿨': 903,\n",
       " '쓰레기': 904,\n",
       " '힘든가': 905,\n",
       " '노': 906,\n",
       " '벌': 907,\n",
       " '도서관': 908,\n",
       " '책': 909,\n",
       " '누가': 910,\n",
       " '피해': 911,\n",
       " '거리': 912,\n",
       " '끝내': 913,\n",
       " '선생': 914,\n",
       " '꺼': 915,\n",
       " '기운': 916,\n",
       " '비슷': 917,\n",
       " '결과': 918,\n",
       " '마무리': 919,\n",
       " '꾸준히': 920,\n",
       " '의심': 921,\n",
       " '냐고': 922,\n",
       " '우산': 923,\n",
       " '으려고': 924,\n",
       " '생': 925,\n",
       " '크리스마스': 926,\n",
       " '올려': 927,\n",
       " '지나가': 928,\n",
       " '명': 929,\n",
       " '귀엽': 930,\n",
       " '헤어': 931,\n",
       " '권태기': 932,\n",
       " '찾아오': 933,\n",
       " '흔적': 934,\n",
       " '가치관': 935,\n",
       " '이혼': 936,\n",
       " '떠난': 937,\n",
       " '제발': 938,\n",
       " '끝난': 939,\n",
       " '부탁': 940,\n",
       " '원망': 941,\n",
       " '마주치': 942,\n",
       " '완전히': 943,\n",
       " '믿음': 944,\n",
       " '할수록': 945,\n",
       " '일지': 946,\n",
       " '썸녀': 947,\n",
       " '견디': 948,\n",
       " '실감': 949,\n",
       " '무덤덤': 950,\n",
       " '될까요': 951,\n",
       " '됨': 952,\n",
       " '즐기': 953,\n",
       " '눈치': 954,\n",
       " '발전': 955,\n",
       " '지켜보': 956,\n",
       " '우선': 957,\n",
       " '그럼': 958,\n",
       " '헤아리': 959,\n",
       " '감기': 960,\n",
       " '더라': 961,\n",
       " '막': 962,\n",
       " '피': 963,\n",
       " '개': 964,\n",
       " '치': 965,\n",
       " '끄': 966,\n",
       " '져': 967,\n",
       " '그랬': 968,\n",
       " '줘도': 969,\n",
       " '순': 970,\n",
       " '월급': 971,\n",
       " '편할': 972,\n",
       " '담배': 973,\n",
       " '당당': 974,\n",
       " '동생': 975,\n",
       " '말투': 976,\n",
       " '신청': 977,\n",
       " '미친': 978,\n",
       " '복': 979,\n",
       " '시키': 980,\n",
       " '피하': 981,\n",
       " '장난': 982,\n",
       " '예뻐': 983,\n",
       " '이번': 984,\n",
       " '희망': 985,\n",
       " '청소': 986,\n",
       " '충전': 987,\n",
       " '가사': 988,\n",
       " '그리움': 989,\n",
       " '고통': 990,\n",
       " '상상': 991,\n",
       " '미래': 992,\n",
       " '못했': 993,\n",
       " '오랜만': 994,\n",
       " '오해': 995,\n",
       " '음악': 996,\n",
       " '설렘': 997,\n",
       " '전하': 998,\n",
       " '일까요': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index\n",
    "            else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word \n",
    "                    else '<unk>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7519\n",
      "7519\n",
      "[2006, 206, 2523, 101]\n",
      "[3, 264, 9, 132, 9, 39, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "enc_train = vectorize(que_corpus, word_to_index)\n",
    "dec_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(dec_train))\n",
    "print(enc_train[0])\n",
    "print(dec_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7443 76 7443 76\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(enc_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(dec_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(len(enc_train), len(enc_val), len(dec_train), len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6. 훈련\n",
    "- Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스크\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-head Attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Position-wise Feed Forward Network\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Layer\n",
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer 전체 모델 조립\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=30000,\n",
    "    tgt_vocab_size=30000,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed71763012744b81aeb53de7d35d8032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13be891a976348e5a5c4aab9048038e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f928de51644826953c0051c6c8275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e448b9dd4d49c3bff4c8d765032ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe3f4387d4a407a843a68f8a75d31a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd83044da3754d8280e2048a0deb4e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5803f0a26fd4d438a0c04f9c08d7b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b7c7ab9fff48d9b911244c1f20d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75755235aa8482da8440a048d71e89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4cbdfc35d4737ac631053ffe0d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d9c3190dc04c61a7ebd8e0d64ecf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e81aa0f4b2149eeba53b684fb4a2978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0132d3df97914b96aaaba9cf2526b6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd0c7352023453fa2a79e64ddc0cde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a66581824ce49799557d3a1978ec526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0425ee9c2bc14a9ba9be810771b8eaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed78cbf8f7ea4141a352a135f875e57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7476ee42c4e494da0951d8bb8c7f3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0402b305426e4776a92e50c70670a5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112895a5b91a44769dd5adab378709e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = mecab.morphs(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in pieces:\n",
    "        sen= get_encoded_sentence(sen, word_to_index)\n",
    "        tokens.append(sen)\n",
    "    \n",
    "    _input = keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                        value=word_to_index[\"<pad>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=20)\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: 지루하다, 놀러가고 싶어.\n",
      "답변: 이 에요 .\n",
      "원문: 오늘 일찍 일어났더니 피곤하다.\n",
      "답변: 만 남 봐요 .\n",
      "원문: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "답변: 도 몰라요 .\n",
      "원문: 집에 있는다는 소리야.\n",
      "답변: 을 먹 고 먹 고 먹 고 먹 으세요 .\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\",\n",
    "]\n",
    "\n",
    "for sentence in sample_sentences:\n",
    "    print(\"원문:\", sentence)\n",
    "    print(\"답변:\", translate(sentence, transformer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer를 이용한 프로젝트를 계속 진행중이다. Transformer가 자연어처리에서 어느정도의 입지인지 알 수 있었다. \n",
    "한편, 이번 프로젝트의 성능은 다른 프로젝트에 비해 성능이 좋지 못했다. 이유는 여러가지지만 데이터양에 따른 희소 문제가 하나의 이유일 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
